using CUDA on GPU 0...	
loading data files...	
cutting off end of data so that the batches/sequences divide evenly	
reshaping tensor...	
WARNING: less than 50 batches in the data in total? Looks like very small dataset. You probably want to use smaller batch_size and/or seq_length.	
data load done. Number of data batches in train: 41, val: 3, test: 0	
vocab size: 65	
creating an lstm with 2 layers	
setting forget gate biases to 1 in LSTM layer 1	
setting forget gate biases to 1 in LSTM layer 2	
number of parameters in the model: 240321	
cloning rnn	
cloning criterion	
1/2050 (epoch 0.024), train_loss = 4.16657931, grad/param norm = 5.4907e-01, time/batch = 0.4996s	
2/2050 (epoch 0.049), train_loss = 3.85286253, grad/param norm = 1.3898e+00, time/batch = 0.1106s	
3/2050 (epoch 0.073), train_loss = 3.46234061, grad/param norm = 1.0048e+00, time/batch = 0.1112s	
4/2050 (epoch 0.098), train_loss = 3.36914966, grad/param norm = 5.0181e-01, time/batch = 0.1118s	
5/2050 (epoch 0.122), train_loss = 3.35569435, grad/param norm = 5.5210e-01, time/batch = 0.1117s	
6/2050 (epoch 0.146), train_loss = 3.36236896, grad/param norm = 5.4106e-01, time/batch = 0.1117s	
7/2050 (epoch 0.171), train_loss = 3.32016224, grad/param norm = 4.2202e-01, time/batch = 0.1117s	
8/2050 (epoch 0.195), train_loss = 3.31643048, grad/param norm = 3.4165e-01, time/batch = 0.1117s	
9/2050 (epoch 0.220), train_loss = 3.31961571, grad/param norm = 3.0851e-01, time/batch = 0.1116s	
10/2050 (epoch 0.244), train_loss = 3.32732721, grad/param norm = 2.2877e-01, time/batch = 0.1116s	
11/2050 (epoch 0.268), train_loss = 3.32015136, grad/param norm = 1.8323e-01, time/batch = 0.1101s	
12/2050 (epoch 0.293), train_loss = 3.32066324, grad/param norm = 1.8243e-01, time/batch = 0.1097s	
13/2050 (epoch 0.317), train_loss = 3.30540425, grad/param norm = 2.0766e-01, time/batch = 0.1096s	
14/2050 (epoch 0.341), train_loss = 3.31152472, grad/param norm = 2.2610e-01, time/batch = 0.1094s	
15/2050 (epoch 0.366), train_loss = 3.32002753, grad/param norm = 2.2487e-01, time/batch = 0.1097s	
16/2050 (epoch 0.390), train_loss = 3.30914420, grad/param norm = 2.0611e-01, time/batch = 0.1097s	
17/2050 (epoch 0.415), train_loss = 3.32487364, grad/param norm = 2.0870e-01, time/batch = 0.1097s	
18/2050 (epoch 0.439), train_loss = 3.31206075, grad/param norm = 1.9562e-01, time/batch = 0.1098s	
19/2050 (epoch 0.463), train_loss = 3.32025084, grad/param norm = 1.9469e-01, time/batch = 0.1096s	
20/2050 (epoch 0.488), train_loss = 3.32057189, grad/param norm = 2.0742e-01, time/batch = 0.1116s	
21/2050 (epoch 0.512), train_loss = 3.30995674, grad/param norm = 2.1763e-01, time/batch = 0.1101s	
22/2050 (epoch 0.537), train_loss = 3.29670320, grad/param norm = 2.5507e-01, time/batch = 0.1097s	
23/2050 (epoch 0.561), train_loss = 3.29942563, grad/param norm = 3.0585e-01, time/batch = 0.1100s	
24/2050 (epoch 0.585), train_loss = 3.32244192, grad/param norm = 3.2780e-01, time/batch = 0.1095s	
25/2050 (epoch 0.610), train_loss = 3.31808012, grad/param norm = 2.8838e-01, time/batch = 0.1096s	
26/2050 (epoch 0.634), train_loss = 3.30768138, grad/param norm = 2.6609e-01, time/batch = 0.1098s	
27/2050 (epoch 0.659), train_loss = 3.31374407, grad/param norm = 2.4498e-01, time/batch = 0.1101s	
28/2050 (epoch 0.683), train_loss = 3.31898386, grad/param norm = 2.6120e-01, time/batch = 0.1097s	
29/2050 (epoch 0.707), train_loss = 3.32224466, grad/param norm = 2.1739e-01, time/batch = 0.1096s	
30/2050 (epoch 0.732), train_loss = 3.30619313, grad/param norm = 1.9800e-01, time/batch = 0.1096s	
31/2050 (epoch 0.756), train_loss = 3.29604987, grad/param norm = 2.2092e-01, time/batch = 0.1102s	
32/2050 (epoch 0.780), train_loss = 3.31069711, grad/param norm = 2.5492e-01, time/batch = 0.1097s	
33/2050 (epoch 0.805), train_loss = 3.30642703, grad/param norm = 2.7094e-01, time/batch = 0.1097s	
34/2050 (epoch 0.829), train_loss = 3.30519577, grad/param norm = 2.5401e-01, time/batch = 0.1095s	
35/2050 (epoch 0.854), train_loss = 3.31148170, grad/param norm = 3.0901e-01, time/batch = 0.1096s	
36/2050 (epoch 0.878), train_loss = 3.31449992, grad/param norm = 3.3520e-01, time/batch = 0.1097s	
37/2050 (epoch 0.902), train_loss = 3.30091029, grad/param norm = 3.2293e-01, time/batch = 0.1096s	
38/2050 (epoch 0.927), train_loss = 3.29829475, grad/param norm = 2.6358e-01, time/batch = 0.1100s	
39/2050 (epoch 0.951), train_loss = 3.30706268, grad/param norm = 2.8058e-01, time/batch = 0.1096s	
40/2050 (epoch 0.976), train_loss = 3.32260256, grad/param norm = 3.2159e-01, time/batch = 0.1096s	
41/2050 (epoch 1.000), train_loss = 3.31134652, grad/param norm = 3.2505e-01, time/batch = 0.1102s	
42/2050 (epoch 1.024), train_loss = 3.29114681, grad/param norm = 3.0387e-01, time/batch = 0.1097s	
43/2050 (epoch 1.049), train_loss = 3.32940306, grad/param norm = 2.5295e-01, time/batch = 0.1096s	
44/2050 (epoch 1.073), train_loss = 3.30569316, grad/param norm = 2.2778e-01, time/batch = 0.1095s	
45/2050 (epoch 1.098), train_loss = 3.37041131, grad/param norm = 9.7224e-01, time/batch = 0.1096s	
46/2050 (epoch 1.122), train_loss = 3.35380763, grad/param norm = 6.7016e-01, time/batch = 0.1097s	
47/2050 (epoch 1.146), train_loss = 3.32769723, grad/param norm = 2.8072e-01, time/batch = 0.1097s	
48/2050 (epoch 1.171), train_loss = 3.28659780, grad/param norm = 1.6377e-01, time/batch = 0.1103s	
49/2050 (epoch 1.195), train_loss = 3.28116803, grad/param norm = 1.8387e-01, time/batch = 0.1096s	
50/2050 (epoch 1.220), train_loss = 3.28200597, grad/param norm = 1.9661e-01, time/batch = 0.1098s	
51/2050 (epoch 1.244), train_loss = 3.28799007, grad/param norm = 2.9687e-01, time/batch = 0.1102s	
52/2050 (epoch 1.268), train_loss = 3.29336259, grad/param norm = 4.0281e-01, time/batch = 0.1096s	
53/2050 (epoch 1.293), train_loss = 3.29436271, grad/param norm = 3.8188e-01, time/batch = 0.1099s	
54/2050 (epoch 1.317), train_loss = 3.26637636, grad/param norm = 3.1579e-01, time/batch = 0.1095s	
55/2050 (epoch 1.341), train_loss = 3.25749903, grad/param norm = 2.7584e-01, time/batch = 0.1097s	
56/2050 (epoch 1.366), train_loss = 3.25884650, grad/param norm = 3.5967e-01, time/batch = 0.1097s	
57/2050 (epoch 1.390), train_loss = 3.25402262, grad/param norm = 5.2036e-01, time/batch = 0.1097s	
58/2050 (epoch 1.415), train_loss = 3.30758734, grad/param norm = 1.0009e+00, time/batch = 0.1097s	
59/2050 (epoch 1.439), train_loss = 3.30513792, grad/param norm = 6.9613e-01, time/batch = 0.1095s	
60/2050 (epoch 1.463), train_loss = 3.25573156, grad/param norm = 3.8780e-01, time/batch = 0.1097s	
61/2050 (epoch 1.488), train_loss = 3.23939330, grad/param norm = 3.0665e-01, time/batch = 0.1102s	
62/2050 (epoch 1.512), train_loss = 3.21776935, grad/param norm = 2.9264e-01, time/batch = 0.1097s	
63/2050 (epoch 1.537), train_loss = 3.19744132, grad/param norm = 3.2669e-01, time/batch = 0.1097s	
64/2050 (epoch 1.561), train_loss = 3.19100325, grad/param norm = 3.7589e-01, time/batch = 0.1096s	
65/2050 (epoch 1.585), train_loss = 3.20335879, grad/param norm = 5.4566e-01, time/batch = 0.1098s	
66/2050 (epoch 1.610), train_loss = 3.23396973, grad/param norm = 8.6101e-01, time/batch = 0.1097s	
67/2050 (epoch 1.634), train_loss = 3.25457890, grad/param norm = 1.2478e+00, time/batch = 0.1098s	
68/2050 (epoch 1.659), train_loss = 3.27485997, grad/param norm = 7.2828e-01, time/batch = 0.1098s	
69/2050 (epoch 1.683), train_loss = 3.20483028, grad/param norm = 2.2161e-01, time/batch = 0.1096s	
70/2050 (epoch 1.707), train_loss = 3.17673523, grad/param norm = 1.8739e-01, time/batch = 0.1097s	
71/2050 (epoch 1.732), train_loss = 3.16248274, grad/param norm = 1.6676e-01, time/batch = 0.1101s	
72/2050 (epoch 1.756), train_loss = 3.14721146, grad/param norm = 2.0355e-01, time/batch = 0.1097s	
73/2050 (epoch 1.780), train_loss = 3.15245679, grad/param norm = 2.7474e-01, time/batch = 0.1097s	
74/2050 (epoch 1.805), train_loss = 3.14624484, grad/param norm = 4.4027e-01, time/batch = 0.1095s	
75/2050 (epoch 1.829), train_loss = 3.14925980, grad/param norm = 7.3870e-01, time/batch = 0.1134s	
76/2050 (epoch 1.854), train_loss = 3.16264606, grad/param norm = 8.1302e-01, time/batch = 0.1098s	
77/2050 (epoch 1.878), train_loss = 3.14349813, grad/param norm = 6.1922e-01, time/batch = 0.1097s	
78/2050 (epoch 1.902), train_loss = 3.14270654, grad/param norm = 7.4333e-01, time/batch = 0.1098s	
79/2050 (epoch 1.927), train_loss = 3.15004160, grad/param norm = 8.8721e-01, time/batch = 0.1101s	
80/2050 (epoch 1.951), train_loss = 3.17409321, grad/param norm = 8.2715e-01, time/batch = 0.1097s	
81/2050 (epoch 1.976), train_loss = 3.12551699, grad/param norm = 3.8676e-01, time/batch = 0.1102s	
82/2050 (epoch 2.000), train_loss = 3.09758232, grad/param norm = 2.3169e-01, time/batch = 0.1096s	
83/2050 (epoch 2.024), train_loss = 3.09288255, grad/param norm = 2.5895e-01, time/batch = 0.1097s	
84/2050 (epoch 2.049), train_loss = 3.08642938, grad/param norm = 3.0728e-01, time/batch = 0.1095s	
85/2050 (epoch 2.073), train_loss = 3.07622759, grad/param norm = 5.5955e-01, time/batch = 0.1097s	
86/2050 (epoch 2.098), train_loss = 3.11371676, grad/param norm = 1.0934e+00, time/batch = 0.1097s	
87/2050 (epoch 2.122), train_loss = 3.14626698, grad/param norm = 1.0707e+00, time/batch = 0.1097s	
88/2050 (epoch 2.146), train_loss = 3.11843747, grad/param norm = 8.4731e-01, time/batch = 0.1097s	
89/2050 (epoch 2.171), train_loss = 3.05795654, grad/param norm = 4.5524e-01, time/batch = 0.1095s	
90/2050 (epoch 2.195), train_loss = 3.02941360, grad/param norm = 3.1603e-01, time/batch = 0.1096s	
91/2050 (epoch 2.220), train_loss = 3.02219271, grad/param norm = 3.4894e-01, time/batch = 0.1101s	
92/2050 (epoch 2.244), train_loss = 3.01152987, grad/param norm = 4.4705e-01, time/batch = 0.1097s	
93/2050 (epoch 2.268), train_loss = 3.01902008, grad/param norm = 5.4084e-01, time/batch = 0.1097s	
94/2050 (epoch 2.293), train_loss = 3.01125247, grad/param norm = 6.2229e-01, time/batch = 0.1098s	
95/2050 (epoch 2.317), train_loss = 3.00586319, grad/param norm = 8.3316e-01, time/batch = 0.1097s	
96/2050 (epoch 2.341), train_loss = 3.02628228, grad/param norm = 1.2000e+00, time/batch = 0.1097s	
97/2050 (epoch 2.366), train_loss = 3.05318070, grad/param norm = 1.3637e+00, time/batch = 0.1097s	
98/2050 (epoch 2.390), train_loss = 3.02790009, grad/param norm = 1.1118e+00, time/batch = 0.1097s	
99/2050 (epoch 2.415), train_loss = 3.02105683, grad/param norm = 1.0008e+00, time/batch = 0.1096s	
100/2050 (epoch 2.439), train_loss = 2.95848798, grad/param norm = 5.8004e-01, time/batch = 0.1096s	
101/2050 (epoch 2.463), train_loss = 2.93135102, grad/param norm = 4.0824e-01, time/batch = 0.1101s	
102/2050 (epoch 2.488), train_loss = 2.91993029, grad/param norm = 3.6644e-01, time/batch = 0.1096s	
103/2050 (epoch 2.512), train_loss = 2.90575395, grad/param norm = 3.4941e-01, time/batch = 0.1096s	
104/2050 (epoch 2.537), train_loss = 2.88731006, grad/param norm = 4.2806e-01, time/batch = 0.1095s	
105/2050 (epoch 2.561), train_loss = 2.88264119, grad/param norm = 6.7076e-01, time/batch = 0.1100s	
106/2050 (epoch 2.585), train_loss = 2.92407100, grad/param norm = 9.5006e-01, time/batch = 0.1098s	
107/2050 (epoch 2.610), train_loss = 2.91971653, grad/param norm = 9.6061e-01, time/batch = 0.1097s	
108/2050 (epoch 2.634), train_loss = 2.88759427, grad/param norm = 7.0820e-01, time/batch = 0.1098s	
109/2050 (epoch 2.659), train_loss = 2.85737741, grad/param norm = 4.6785e-01, time/batch = 0.1100s	
110/2050 (epoch 2.683), train_loss = 2.84677316, grad/param norm = 5.1507e-01, time/batch = 0.1097s	
111/2050 (epoch 2.707), train_loss = 2.85883707, grad/param norm = 8.1444e-01, time/batch = 0.1102s	
112/2050 (epoch 2.732), train_loss = 2.89039279, grad/param norm = 1.0058e+00, time/batch = 0.1096s	
113/2050 (epoch 2.756), train_loss = 2.87678098, grad/param norm = 9.3132e-01, time/batch = 0.1098s	
114/2050 (epoch 2.780), train_loss = 2.83731672, grad/param norm = 5.2610e-01, time/batch = 0.1095s	
115/2050 (epoch 2.805), train_loss = 2.79673793, grad/param norm = 3.2137e-01, time/batch = 0.1097s	
116/2050 (epoch 2.829), train_loss = 2.77767402, grad/param norm = 3.0546e-01, time/batch = 0.1098s	
117/2050 (epoch 2.854), train_loss = 2.77793107, grad/param norm = 4.6731e-01, time/batch = 0.1097s	
118/2050 (epoch 2.878), train_loss = 2.80234822, grad/param norm = 7.9131e-01, time/batch = 0.1097s	
119/2050 (epoch 2.902), train_loss = 2.82413877, grad/param norm = 1.1270e+00, time/batch = 0.1095s	
120/2050 (epoch 2.927), train_loss = 2.82257541, grad/param norm = 9.6255e-01, time/batch = 0.1101s	
121/2050 (epoch 2.951), train_loss = 2.78869884, grad/param norm = 5.8080e-01, time/batch = 0.1102s	
122/2050 (epoch 2.976), train_loss = 2.76184203, grad/param norm = 4.3896e-01, time/batch = 0.1096s	
123/2050 (epoch 3.000), train_loss = 2.74479182, grad/param norm = 4.9752e-01, time/batch = 0.1098s	
124/2050 (epoch 3.024), train_loss = 2.76211873, grad/param norm = 5.7215e-01, time/batch = 0.1096s	
125/2050 (epoch 3.049), train_loss = 2.76500282, grad/param norm = 6.9020e-01, time/batch = 0.1096s	
126/2050 (epoch 3.073), train_loss = 2.74740417, grad/param norm = 6.1552e-01, time/batch = 0.1098s	
127/2050 (epoch 3.098), train_loss = 2.72534141, grad/param norm = 4.9601e-01, time/batch = 0.1097s	
128/2050 (epoch 3.122), train_loss = 2.72728759, grad/param norm = 3.9688e-01, time/batch = 0.1097s	
129/2050 (epoch 3.146), train_loss = 2.70900411, grad/param norm = 3.9754e-01, time/batch = 0.1096s	
130/2050 (epoch 3.171), train_loss = 2.70644898, grad/param norm = 4.0839e-01, time/batch = 0.1099s	
131/2050 (epoch 3.195), train_loss = 2.69200476, grad/param norm = 6.0554e-01, time/batch = 0.1107s	
132/2050 (epoch 3.220), train_loss = 2.73212315, grad/param norm = 9.2392e-01, time/batch = 0.1097s	
133/2050 (epoch 3.244), train_loss = 2.75190564, grad/param norm = 1.1177e+00, time/batch = 0.1098s	
134/2050 (epoch 3.268), train_loss = 2.74613676, grad/param norm = 9.9333e-01, time/batch = 0.1096s	
135/2050 (epoch 3.293), train_loss = 2.72435987, grad/param norm = 9.7980e-01, time/batch = 0.1099s	
136/2050 (epoch 3.317), train_loss = 2.71821261, grad/param norm = 9.0591e-01, time/batch = 0.1098s	
137/2050 (epoch 3.341), train_loss = 2.70546437, grad/param norm = 8.4005e-01, time/batch = 0.1098s	
138/2050 (epoch 3.366), train_loss = 2.67603758, grad/param norm = 5.1327e-01, time/batch = 0.1098s	
139/2050 (epoch 3.390), train_loss = 2.65437832, grad/param norm = 4.3926e-01, time/batch = 0.1097s	
140/2050 (epoch 3.415), train_loss = 2.65205926, grad/param norm = 5.0193e-01, time/batch = 0.1097s	
141/2050 (epoch 3.439), train_loss = 2.66667727, grad/param norm = 6.6719e-01, time/batch = 0.1103s	
142/2050 (epoch 3.463), train_loss = 2.67385838, grad/param norm = 6.6834e-01, time/batch = 0.1097s	
143/2050 (epoch 3.488), train_loss = 2.65830301, grad/param norm = 5.0388e-01, time/batch = 0.1097s	
144/2050 (epoch 3.512), train_loss = 2.64001123, grad/param norm = 3.3972e-01, time/batch = 0.1096s	
145/2050 (epoch 3.537), train_loss = 2.61845686, grad/param norm = 2.6075e-01, time/batch = 0.1098s	
146/2050 (epoch 3.561), train_loss = 2.59688733, grad/param norm = 2.1825e-01, time/batch = 0.1102s	
147/2050 (epoch 3.585), train_loss = 2.60372071, grad/param norm = 2.5261e-01, time/batch = 0.1098s	
148/2050 (epoch 3.610), train_loss = 2.60684147, grad/param norm = 3.8489e-01, time/batch = 0.1098s	
149/2050 (epoch 3.634), train_loss = 2.61943910, grad/param norm = 9.0739e-01, time/batch = 0.1096s	
150/2050 (epoch 3.659), train_loss = 2.71473391, grad/param norm = 1.4517e+00, time/batch = 0.1110s	
151/2050 (epoch 3.683), train_loss = 2.73275260, grad/param norm = 1.4008e+00, time/batch = 0.1102s	
152/2050 (epoch 3.707), train_loss = 2.64795627, grad/param norm = 6.7341e-01, time/batch = 0.1097s	
153/2050 (epoch 3.732), train_loss = 2.60115382, grad/param norm = 4.2904e-01, time/batch = 0.1098s	
154/2050 (epoch 3.756), train_loss = 2.59244578, grad/param norm = 3.9780e-01, time/batch = 0.1096s	
155/2050 (epoch 3.780), train_loss = 2.59619238, grad/param norm = 4.0083e-01, time/batch = 0.1098s	
156/2050 (epoch 3.805), train_loss = 2.57686317, grad/param norm = 4.2362e-01, time/batch = 0.1098s	
157/2050 (epoch 3.829), train_loss = 2.57112239, grad/param norm = 4.4062e-01, time/batch = 0.1098s	
158/2050 (epoch 3.854), train_loss = 2.57280285, grad/param norm = 4.5630e-01, time/batch = 0.1098s	
159/2050 (epoch 3.878), train_loss = 2.57506376, grad/param norm = 4.6863e-01, time/batch = 0.1097s	
160/2050 (epoch 3.902), train_loss = 2.56312981, grad/param norm = 4.8993e-01, time/batch = 0.1097s	
161/2050 (epoch 3.927), train_loss = 2.55985845, grad/param norm = 4.0776e-01, time/batch = 0.1105s	
162/2050 (epoch 3.951), train_loss = 2.56720770, grad/param norm = 3.7762e-01, time/batch = 0.1098s	
163/2050 (epoch 3.976), train_loss = 2.55248653, grad/param norm = 4.1440e-01, time/batch = 0.1097s	
164/2050 (epoch 4.000), train_loss = 2.54229344, grad/param norm = 5.4915e-01, time/batch = 0.1095s	
165/2050 (epoch 4.024), train_loss = 2.58859393, grad/param norm = 8.7586e-01, time/batch = 0.1097s	
166/2050 (epoch 4.049), train_loss = 2.62468605, grad/param norm = 1.0940e+00, time/batch = 0.1098s	
167/2050 (epoch 4.073), train_loss = 2.59494538, grad/param norm = 8.9635e-01, time/batch = 0.1097s	
168/2050 (epoch 4.098), train_loss = 2.55802671, grad/param norm = 6.1507e-01, time/batch = 0.1098s	
169/2050 (epoch 4.122), train_loss = 2.56291499, grad/param norm = 7.7383e-01, time/batch = 0.1096s	
170/2050 (epoch 4.146), train_loss = 2.59152745, grad/param norm = 9.5740e-01, time/batch = 0.1097s	
171/2050 (epoch 4.171), train_loss = 2.57863495, grad/param norm = 8.5206e-01, time/batch = 0.1103s	
172/2050 (epoch 4.195), train_loss = 2.54247828, grad/param norm = 5.4464e-01, time/batch = 0.1098s	
173/2050 (epoch 4.220), train_loss = 2.52184991, grad/param norm = 3.2078e-01, time/batch = 0.1097s	
174/2050 (epoch 4.244), train_loss = 2.50023164, grad/param norm = 2.7048e-01, time/batch = 0.1095s	
175/2050 (epoch 4.268), train_loss = 2.50462279, grad/param norm = 2.6041e-01, time/batch = 0.1098s	
176/2050 (epoch 4.293), train_loss = 2.49926368, grad/param norm = 2.9020e-01, time/batch = 0.1099s	
177/2050 (epoch 4.317), train_loss = 2.49954939, grad/param norm = 3.2577e-01, time/batch = 0.1098s	
178/2050 (epoch 4.341), train_loss = 2.49292956, grad/param norm = 4.5285e-01, time/batch = 0.1098s	
179/2050 (epoch 4.366), train_loss = 2.51794659, grad/param norm = 5.5859e-01, time/batch = 0.1096s	
180/2050 (epoch 4.390), train_loss = 2.51216271, grad/param norm = 6.6597e-01, time/batch = 0.1097s	
181/2050 (epoch 4.415), train_loss = 2.50979954, grad/param norm = 6.5464e-01, time/batch = 0.1102s	
182/2050 (epoch 4.439), train_loss = 2.50684690, grad/param norm = 7.2071e-01, time/batch = 0.1097s	
183/2050 (epoch 4.463), train_loss = 2.51824852, grad/param norm = 7.8712e-01, time/batch = 0.1097s	
184/2050 (epoch 4.488), train_loss = 2.52098079, grad/param norm = 8.2384e-01, time/batch = 0.1095s	
185/2050 (epoch 4.512), train_loss = 2.52717506, grad/param norm = 6.5714e-01, time/batch = 0.1098s	
186/2050 (epoch 4.537), train_loss = 2.48461235, grad/param norm = 4.3317e-01, time/batch = 0.1098s	
187/2050 (epoch 4.561), train_loss = 2.45919665, grad/param norm = 3.6225e-01, time/batch = 0.1102s	
188/2050 (epoch 4.585), train_loss = 2.46770051, grad/param norm = 3.6369e-01, time/batch = 0.1097s	
189/2050 (epoch 4.610), train_loss = 2.47229237, grad/param norm = 4.4866e-01, time/batch = 0.1096s	
190/2050 (epoch 4.634), train_loss = 2.47150210, grad/param norm = 5.5841e-01, time/batch = 0.1098s	
191/2050 (epoch 4.659), train_loss = 2.48258706, grad/param norm = 6.2736e-01, time/batch = 0.1103s	
192/2050 (epoch 4.683), train_loss = 2.47305191, grad/param norm = 6.0989e-01, time/batch = 0.1097s	
193/2050 (epoch 4.707), train_loss = 2.47040530, grad/param norm = 5.4958e-01, time/batch = 0.1097s	
194/2050 (epoch 4.732), train_loss = 2.47502292, grad/param norm = 6.3033e-01, time/batch = 0.1096s	
195/2050 (epoch 4.756), train_loss = 2.48589550, grad/param norm = 6.2098e-01, time/batch = 0.1097s	
196/2050 (epoch 4.780), train_loss = 2.46994524, grad/param norm = 4.7338e-01, time/batch = 0.1099s	
197/2050 (epoch 4.805), train_loss = 2.43260474, grad/param norm = 3.5011e-01, time/batch = 0.1097s	
198/2050 (epoch 4.829), train_loss = 2.42267787, grad/param norm = 3.7554e-01, time/batch = 0.1098s	
199/2050 (epoch 4.854), train_loss = 2.43499562, grad/param norm = 6.3234e-01, time/batch = 0.1096s	
200/2050 (epoch 4.878), train_loss = 2.49595063, grad/param norm = 1.2413e+00, time/batch = 0.1098s	
201/2050 (epoch 4.902), train_loss = 2.52731468, grad/param norm = 9.3736e-01, time/batch = 0.1102s	
202/2050 (epoch 4.927), train_loss = 2.45955992, grad/param norm = 6.1842e-01, time/batch = 0.1098s	
203/2050 (epoch 4.951), train_loss = 2.46008703, grad/param norm = 4.0717e-01, time/batch = 0.1098s	
204/2050 (epoch 4.976), train_loss = 2.42796039, grad/param norm = 2.4236e-01, time/batch = 0.1096s	
205/2050 (epoch 5.000), train_loss = 2.41214507, grad/param norm = 2.4421e-01, time/batch = 0.1100s	
206/2050 (epoch 5.024), train_loss = 2.44248511, grad/param norm = 3.2295e-01, time/batch = 0.1098s	
207/2050 (epoch 5.049), train_loss = 2.42924929, grad/param norm = 4.2176e-01, time/batch = 0.1097s	
208/2050 (epoch 5.073), train_loss = 2.41423450, grad/param norm = 4.6563e-01, time/batch = 0.1098s	
209/2050 (epoch 5.098), train_loss = 2.41390085, grad/param norm = 4.5516e-01, time/batch = 0.1096s	
210/2050 (epoch 5.122), train_loss = 2.42269207, grad/param norm = 3.7099e-01, time/batch = 0.1097s	
211/2050 (epoch 5.146), train_loss = 2.39997619, grad/param norm = 3.2589e-01, time/batch = 0.1103s	
212/2050 (epoch 5.171), train_loss = 2.39792087, grad/param norm = 3.5457e-01, time/batch = 0.1097s	
213/2050 (epoch 5.195), train_loss = 2.39573637, grad/param norm = 4.2560e-01, time/batch = 0.1102s	
214/2050 (epoch 5.220), train_loss = 2.40763449, grad/param norm = 5.2022e-01, time/batch = 0.1095s	
215/2050 (epoch 5.244), train_loss = 2.40310171, grad/param norm = 5.8932e-01, time/batch = 0.1098s	
216/2050 (epoch 5.268), train_loss = 2.41818754, grad/param norm = 5.8978e-01, time/batch = 0.1097s	
217/2050 (epoch 5.293), train_loss = 2.41578336, grad/param norm = 6.0030e-01, time/batch = 0.1098s	
218/2050 (epoch 5.317), train_loss = 2.41950354, grad/param norm = 5.6281e-01, time/batch = 0.1099s	
219/2050 (epoch 5.341), train_loss = 2.39957231, grad/param norm = 6.1282e-01, time/batch = 0.1096s	
220/2050 (epoch 5.366), train_loss = 2.42002686, grad/param norm = 5.2051e-01, time/batch = 0.1098s	
221/2050 (epoch 5.390), train_loss = 2.38122666, grad/param norm = 3.3695e-01, time/batch = 0.1102s	
222/2050 (epoch 5.415), train_loss = 2.36457240, grad/param norm = 2.5134e-01, time/batch = 0.1097s	
223/2050 (epoch 5.439), train_loss = 2.36242000, grad/param norm = 2.3515e-01, time/batch = 0.1098s	
224/2050 (epoch 5.463), train_loss = 2.35888592, grad/param norm = 3.1256e-01, time/batch = 0.1106s	
225/2050 (epoch 5.488), train_loss = 2.37506041, grad/param norm = 5.4214e-01, time/batch = 0.1098s	
226/2050 (epoch 5.512), train_loss = 2.43231605, grad/param norm = 9.3017e-01, time/batch = 0.1098s	
227/2050 (epoch 5.537), train_loss = 2.47478920, grad/param norm = 9.0203e-01, time/batch = 0.1098s	
228/2050 (epoch 5.561), train_loss = 2.41201126, grad/param norm = 5.4460e-01, time/batch = 0.1103s	
229/2050 (epoch 5.585), train_loss = 2.37412450, grad/param norm = 2.8049e-01, time/batch = 0.1097s	
230/2050 (epoch 5.610), train_loss = 2.36217523, grad/param norm = 1.6457e-01, time/batch = 0.1097s	
231/2050 (epoch 5.634), train_loss = 2.34846008, grad/param norm = 1.7777e-01, time/batch = 0.1103s	
232/2050 (epoch 5.659), train_loss = 2.35154859, grad/param norm = 2.6787e-01, time/batch = 0.1097s	
233/2050 (epoch 5.683), train_loss = 2.35168891, grad/param norm = 4.2268e-01, time/batch = 0.1098s	
234/2050 (epoch 5.707), train_loss = 2.36921625, grad/param norm = 6.0007e-01, time/batch = 0.1096s	
235/2050 (epoch 5.732), train_loss = 2.38726895, grad/param norm = 5.5650e-01, time/batch = 0.1098s	
236/2050 (epoch 5.756), train_loss = 2.35791229, grad/param norm = 4.0003e-01, time/batch = 0.1099s	
237/2050 (epoch 5.780), train_loss = 2.35552147, grad/param norm = 3.4771e-01, time/batch = 0.1098s	
238/2050 (epoch 5.805), train_loss = 2.33261791, grad/param norm = 4.1881e-01, time/batch = 0.1098s	
239/2050 (epoch 5.829), train_loss = 2.34556670, grad/param norm = 5.7564e-01, time/batch = 0.1096s	
240/2050 (epoch 5.854), train_loss = 2.36109743, grad/param norm = 6.6590e-01, time/batch = 0.1098s	
241/2050 (epoch 5.878), train_loss = 2.36262877, grad/param norm = 5.6431e-01, time/batch = 0.1103s	
242/2050 (epoch 5.902), train_loss = 2.33970550, grad/param norm = 4.5064e-01, time/batch = 0.1096s	
243/2050 (epoch 5.927), train_loss = 2.32007280, grad/param norm = 3.4246e-01, time/batch = 0.1102s	
244/2050 (epoch 5.951), train_loss = 2.34023716, grad/param norm = 2.5972e-01, time/batch = 0.1095s	
245/2050 (epoch 5.976), train_loss = 2.32301634, grad/param norm = 2.2788e-01, time/batch = 0.1098s	
246/2050 (epoch 6.000), train_loss = 2.30984232, grad/param norm = 2.6418e-01, time/batch = 0.1098s	
247/2050 (epoch 6.024), train_loss = 2.34713837, grad/param norm = 3.3352e-01, time/batch = 0.1099s	
248/2050 (epoch 6.049), train_loss = 2.32461933, grad/param norm = 3.8106e-01, time/batch = 0.1097s	
249/2050 (epoch 6.073), train_loss = 2.30947923, grad/param norm = 4.1189e-01, time/batch = 0.1097s	
250/2050 (epoch 6.098), train_loss = 2.32232552, grad/param norm = 5.0215e-01, time/batch = 0.1098s	
251/2050 (epoch 6.122), train_loss = 2.34636079, grad/param norm = 4.8672e-01, time/batch = 0.1102s	
252/2050 (epoch 6.146), train_loss = 2.32107208, grad/param norm = 3.9386e-01, time/batch = 0.1097s	
253/2050 (epoch 6.171), train_loss = 2.31041418, grad/param norm = 3.3204e-01, time/batch = 0.1097s	
254/2050 (epoch 6.195), train_loss = 2.31019107, grad/param norm = 4.6803e-01, time/batch = 0.1100s	
255/2050 (epoch 6.220), train_loss = 2.34554266, grad/param norm = 8.3442e-01, time/batch = 0.1097s	
256/2050 (epoch 6.244), train_loss = 2.37821656, grad/param norm = 8.4841e-01, time/batch = 0.1098s	
257/2050 (epoch 6.268), train_loss = 2.35504306, grad/param norm = 5.4288e-01, time/batch = 0.1098s	
258/2050 (epoch 6.293), train_loss = 2.31146800, grad/param norm = 2.9511e-01, time/batch = 0.1101s	
259/2050 (epoch 6.317), train_loss = 2.30324930, grad/param norm = 2.3041e-01, time/batch = 0.1097s	
260/2050 (epoch 6.341), train_loss = 2.28374018, grad/param norm = 2.0089e-01, time/batch = 0.1097s	
261/2050 (epoch 6.366), train_loss = 2.29489822, grad/param norm = 2.2314e-01, time/batch = 0.1102s	
262/2050 (epoch 6.390), train_loss = 2.28727657, grad/param norm = 2.7010e-01, time/batch = 0.1096s	
263/2050 (epoch 6.415), train_loss = 2.28091324, grad/param norm = 3.0527e-01, time/batch = 0.1097s	
264/2050 (epoch 6.439), train_loss = 2.28482975, grad/param norm = 2.8801e-01, time/batch = 0.1096s	
265/2050 (epoch 6.463), train_loss = 2.27418600, grad/param norm = 2.6439e-01, time/batch = 0.1098s	
266/2050 (epoch 6.488), train_loss = 2.27401047, grad/param norm = 2.7273e-01, time/batch = 0.1097s	
267/2050 (epoch 6.512), train_loss = 2.28451241, grad/param norm = 3.0798e-01, time/batch = 0.1096s	
268/2050 (epoch 6.537), train_loss = 2.28459343, grad/param norm = 3.5811e-01, time/batch = 0.1097s	
269/2050 (epoch 6.561), train_loss = 2.27180126, grad/param norm = 3.9770e-01, time/batch = 0.1099s	
270/2050 (epoch 6.585), train_loss = 2.28920322, grad/param norm = 4.7941e-01, time/batch = 0.1096s	
271/2050 (epoch 6.610), train_loss = 2.30110136, grad/param norm = 5.6339e-01, time/batch = 0.1102s	
272/2050 (epoch 6.634), train_loss = 2.30291558, grad/param norm = 6.1732e-01, time/batch = 0.1098s	
273/2050 (epoch 6.659), train_loss = 2.30504321, grad/param norm = 5.4053e-01, time/batch = 0.1098s	
274/2050 (epoch 6.683), train_loss = 2.27111304, grad/param norm = 3.9884e-01, time/batch = 0.1095s	
275/2050 (epoch 6.707), train_loss = 2.25696329, grad/param norm = 2.7689e-01, time/batch = 0.1097s	
276/2050 (epoch 6.732), train_loss = 2.25663277, grad/param norm = 2.3826e-01, time/batch = 0.1098s	
277/2050 (epoch 6.756), train_loss = 2.25608563, grad/param norm = 2.2260e-01, time/batch = 0.1098s	
278/2050 (epoch 6.780), train_loss = 2.25583556, grad/param norm = 2.2699e-01, time/batch = 0.1098s	
279/2050 (epoch 6.805), train_loss = 2.23624140, grad/param norm = 2.8505e-01, time/batch = 0.1098s	
280/2050 (epoch 6.829), train_loss = 2.24928068, grad/param norm = 4.0136e-01, time/batch = 0.1098s	
281/2050 (epoch 6.854), train_loss = 2.26292778, grad/param norm = 4.4113e-01, time/batch = 0.1102s	
282/2050 (epoch 6.878), train_loss = 2.25705053, grad/param norm = 4.6091e-01, time/batch = 0.1097s	
283/2050 (epoch 6.902), train_loss = 2.26984281, grad/param norm = 5.1545e-01, time/batch = 0.1097s	
284/2050 (epoch 6.927), train_loss = 2.26572534, grad/param norm = 5.6564e-01, time/batch = 0.1097s	
285/2050 (epoch 6.951), train_loss = 2.32888576, grad/param norm = 8.2721e-01, time/batch = 0.1097s	
286/2050 (epoch 6.976), train_loss = 2.34851274, grad/param norm = 9.2801e-01, time/batch = 0.1099s	
287/2050 (epoch 7.000), train_loss = 2.29688821, grad/param norm = 5.8548e-01, time/batch = 0.1097s	
288/2050 (epoch 7.024), train_loss = 2.29101663, grad/param norm = 3.4332e-01, time/batch = 0.1099s	
289/2050 (epoch 7.049), train_loss = 2.24108393, grad/param norm = 2.7588e-01, time/batch = 0.1097s	
290/2050 (epoch 7.073), train_loss = 2.21326646, grad/param norm = 2.6967e-01, time/batch = 0.1097s	
291/2050 (epoch 7.098), train_loss = 2.21876395, grad/param norm = 2.7931e-01, time/batch = 0.1102s	
292/2050 (epoch 7.122), train_loss = 2.23191924, grad/param norm = 2.9839e-01, time/batch = 0.1097s	
293/2050 (epoch 7.146), train_loss = 2.22080312, grad/param norm = 3.2677e-01, time/batch = 0.1097s	
294/2050 (epoch 7.171), train_loss = 2.22560936, grad/param norm = 3.6355e-01, time/batch = 0.1096s	
295/2050 (epoch 7.195), train_loss = 2.22603557, grad/param norm = 3.9780e-01, time/batch = 0.1101s	
296/2050 (epoch 7.220), train_loss = 2.22452244, grad/param norm = 3.9000e-01, time/batch = 0.1097s	
297/2050 (epoch 7.244), train_loss = 2.20872993, grad/param norm = 3.4078e-01, time/batch = 0.1097s	
298/2050 (epoch 7.268), train_loss = 2.21955699, grad/param norm = 2.8461e-01, time/batch = 0.1098s	
299/2050 (epoch 7.293), train_loss = 2.21281085, grad/param norm = 2.7745e-01, time/batch = 0.1112s	
300/2050 (epoch 7.317), train_loss = 2.21917140, grad/param norm = 3.0792e-01, time/batch = 0.1097s	
301/2050 (epoch 7.341), train_loss = 2.21341820, grad/param norm = 4.0097e-01, time/batch = 0.1103s	
302/2050 (epoch 7.366), train_loss = 2.24292109, grad/param norm = 4.5626e-01, time/batch = 0.1097s	
303/2050 (epoch 7.390), train_loss = 2.22267775, grad/param norm = 4.4129e-01, time/batch = 0.1097s	
304/2050 (epoch 7.415), train_loss = 2.21748839, grad/param norm = 3.6484e-01, time/batch = 0.1095s	
305/2050 (epoch 7.439), train_loss = 2.20618807, grad/param norm = 2.7394e-01, time/batch = 0.1098s	
306/2050 (epoch 7.463), train_loss = 2.19190111, grad/param norm = 2.2159e-01, time/batch = 0.1098s	
307/2050 (epoch 7.488), train_loss = 2.18937152, grad/param norm = 2.1158e-01, time/batch = 0.1097s	
308/2050 (epoch 7.512), train_loss = 2.19955342, grad/param norm = 2.5574e-01, time/batch = 0.1099s	
309/2050 (epoch 7.537), train_loss = 2.20697308, grad/param norm = 3.5683e-01, time/batch = 0.1097s	
310/2050 (epoch 7.561), train_loss = 2.20422845, grad/param norm = 4.0028e-01, time/batch = 0.1101s	
311/2050 (epoch 7.585), train_loss = 2.20959188, grad/param norm = 3.9423e-01, time/batch = 0.1102s	
312/2050 (epoch 7.610), train_loss = 2.20770329, grad/param norm = 3.4516e-01, time/batch = 0.1097s	
313/2050 (epoch 7.634), train_loss = 2.19720339, grad/param norm = 3.5358e-01, time/batch = 0.1097s	
314/2050 (epoch 7.659), train_loss = 2.20494085, grad/param norm = 4.4533e-01, time/batch = 0.1095s	
315/2050 (epoch 7.683), train_loss = 2.21153011, grad/param norm = 4.7100e-01, time/batch = 0.1097s	
316/2050 (epoch 7.707), train_loss = 2.19540272, grad/param norm = 3.7473e-01, time/batch = 0.1098s	
317/2050 (epoch 7.732), train_loss = 2.19275647, grad/param norm = 2.9278e-01, time/batch = 0.1097s	
318/2050 (epoch 7.756), train_loss = 2.18594080, grad/param norm = 2.3716e-01, time/batch = 0.1098s	
319/2050 (epoch 7.780), train_loss = 2.18350651, grad/param norm = 2.1828e-01, time/batch = 0.1096s	
320/2050 (epoch 7.805), train_loss = 2.16067855, grad/param norm = 2.5737e-01, time/batch = 0.1097s	
321/2050 (epoch 7.829), train_loss = 2.17273138, grad/param norm = 3.0177e-01, time/batch = 0.1106s	
322/2050 (epoch 7.854), train_loss = 2.16425118, grad/param norm = 3.4938e-01, time/batch = 0.1098s	
323/2050 (epoch 7.878), train_loss = 2.18264181, grad/param norm = 4.2185e-01, time/batch = 0.1098s	
324/2050 (epoch 7.902), train_loss = 2.18716181, grad/param norm = 4.6181e-01, time/batch = 0.1096s	
325/2050 (epoch 7.927), train_loss = 2.17816755, grad/param norm = 5.0537e-01, time/batch = 0.1099s	
326/2050 (epoch 7.951), train_loss = 2.21277024, grad/param norm = 4.5266e-01, time/batch = 0.1098s	
327/2050 (epoch 7.976), train_loss = 2.18542109, grad/param norm = 3.5912e-01, time/batch = 0.1097s	
328/2050 (epoch 8.000), train_loss = 2.16226773, grad/param norm = 3.1179e-01, time/batch = 0.1098s	
329/2050 (epoch 8.024), train_loss = 2.21034840, grad/param norm = 2.8885e-01, time/batch = 0.1096s	
330/2050 (epoch 8.049), train_loss = 2.16413230, grad/param norm = 2.7400e-01, time/batch = 0.1097s	
331/2050 (epoch 8.073), train_loss = 2.13591067, grad/param norm = 2.5004e-01, time/batch = 0.1103s	
332/2050 (epoch 8.098), train_loss = 2.14450511, grad/param norm = 2.5016e-01, time/batch = 0.1097s	
333/2050 (epoch 8.122), train_loss = 2.15838900, grad/param norm = 2.6022e-01, time/batch = 0.1097s	
334/2050 (epoch 8.146), train_loss = 2.14755559, grad/param norm = 2.6834e-01, time/batch = 0.1096s	
335/2050 (epoch 8.171), train_loss = 2.15328953, grad/param norm = 2.6318e-01, time/batch = 0.1098s	
336/2050 (epoch 8.195), train_loss = 2.14501231, grad/param norm = 2.6187e-01, time/batch = 0.1102s	
337/2050 (epoch 8.220), train_loss = 2.14136844, grad/param norm = 2.6543e-01, time/batch = 0.1098s	
338/2050 (epoch 8.244), train_loss = 2.13386487, grad/param norm = 2.6779e-01, time/batch = 0.1098s	
339/2050 (epoch 8.268), train_loss = 2.15351936, grad/param norm = 2.7342e-01, time/batch = 0.1097s	
340/2050 (epoch 8.293), train_loss = 2.15321146, grad/param norm = 2.7168e-01, time/batch = 0.1101s	
341/2050 (epoch 8.317), train_loss = 2.15362299, grad/param norm = 2.6528e-01, time/batch = 0.1102s	
342/2050 (epoch 8.341), train_loss = 2.14224035, grad/param norm = 2.7999e-01, time/batch = 0.1097s	
343/2050 (epoch 8.366), train_loss = 2.14898646, grad/param norm = 3.7844e-01, time/batch = 0.1098s	
344/2050 (epoch 8.390), train_loss = 2.16715401, grad/param norm = 5.7795e-01, time/batch = 0.1095s	
345/2050 (epoch 8.415), train_loss = 2.18858723, grad/param norm = 5.6849e-01, time/batch = 0.1098s	
346/2050 (epoch 8.439), train_loss = 2.16669858, grad/param norm = 3.8462e-01, time/batch = 0.1099s	
347/2050 (epoch 8.463), train_loss = 2.13905509, grad/param norm = 2.6642e-01, time/batch = 0.1097s	
348/2050 (epoch 8.488), train_loss = 2.12523880, grad/param norm = 1.9340e-01, time/batch = 0.1097s	
349/2050 (epoch 8.512), train_loss = 2.12335555, grad/param norm = 1.6415e-01, time/batch = 0.1096s	
350/2050 (epoch 8.537), train_loss = 2.11949225, grad/param norm = 1.5167e-01, time/batch = 0.1097s	
351/2050 (epoch 8.561), train_loss = 2.10466484, grad/param norm = 1.5511e-01, time/batch = 0.1103s	
352/2050 (epoch 8.585), train_loss = 2.11730383, grad/param norm = 1.8402e-01, time/batch = 0.1097s	
353/2050 (epoch 8.610), train_loss = 2.13251398, grad/param norm = 2.4957e-01, time/batch = 0.1097s	
354/2050 (epoch 8.634), train_loss = 2.14219429, grad/param norm = 3.1726e-01, time/batch = 0.1096s	
355/2050 (epoch 8.659), train_loss = 2.13683797, grad/param norm = 3.5728e-01, time/batch = 0.1097s	
356/2050 (epoch 8.683), train_loss = 2.13308421, grad/param norm = 4.0626e-01, time/batch = 0.1097s	
357/2050 (epoch 8.707), train_loss = 2.12715377, grad/param norm = 4.6168e-01, time/batch = 0.1097s	
358/2050 (epoch 8.732), train_loss = 2.13947296, grad/param norm = 4.1078e-01, time/batch = 0.1098s	
359/2050 (epoch 8.756), train_loss = 2.13105255, grad/param norm = 3.2930e-01, time/batch = 0.1097s	
360/2050 (epoch 8.780), train_loss = 2.12435714, grad/param norm = 2.7265e-01, time/batch = 0.1097s	
361/2050 (epoch 8.805), train_loss = 2.09535199, grad/param norm = 2.3727e-01, time/batch = 0.1103s	
362/2050 (epoch 8.829), train_loss = 2.09596864, grad/param norm = 2.1532e-01, time/batch = 0.1101s	
363/2050 (epoch 8.854), train_loss = 2.08217408, grad/param norm = 2.1779e-01, time/batch = 0.1097s	
364/2050 (epoch 8.878), train_loss = 2.08881147, grad/param norm = 2.2039e-01, time/batch = 0.1095s	
365/2050 (epoch 8.902), train_loss = 2.09537234, grad/param norm = 2.2651e-01, time/batch = 0.1097s	
366/2050 (epoch 8.927), train_loss = 2.07810528, grad/param norm = 2.4186e-01, time/batch = 0.1102s	
367/2050 (epoch 8.951), train_loss = 2.11981872, grad/param norm = 2.5440e-01, time/batch = 0.1097s	
368/2050 (epoch 8.976), train_loss = 2.10941176, grad/param norm = 2.4617e-01, time/batch = 0.1099s	
369/2050 (epoch 9.000), train_loss = 2.08723600, grad/param norm = 2.2624e-01, time/batch = 0.1096s	
370/2050 (epoch 9.024), train_loss = 2.15846452, grad/param norm = 2.9109e-01, time/batch = 0.1097s	
371/2050 (epoch 9.049), train_loss = 2.12332992, grad/param norm = 4.5372e-01, time/batch = 0.1103s	
372/2050 (epoch 9.073), train_loss = 2.13435416, grad/param norm = 5.6959e-01, time/batch = 0.1096s	
373/2050 (epoch 9.098), train_loss = 2.13545994, grad/param norm = 4.5998e-01, time/batch = 0.1107s	
374/2050 (epoch 9.122), train_loss = 2.11937700, grad/param norm = 3.5181e-01, time/batch = 0.1096s	
375/2050 (epoch 9.146), train_loss = 2.09646406, grad/param norm = 3.7538e-01, time/batch = 0.1097s	
376/2050 (epoch 9.171), train_loss = 2.11087947, grad/param norm = 3.5749e-01, time/batch = 0.1098s	
377/2050 (epoch 9.195), train_loss = 2.08982778, grad/param norm = 2.7650e-01, time/batch = 0.1101s	
378/2050 (epoch 9.220), train_loss = 2.07558649, grad/param norm = 2.2800e-01, time/batch = 0.1097s	
379/2050 (epoch 9.244), train_loss = 2.06397401, grad/param norm = 2.1850e-01, time/batch = 0.1096s	
380/2050 (epoch 9.268), train_loss = 2.08445635, grad/param norm = 2.4063e-01, time/batch = 0.1097s	
381/2050 (epoch 9.293), train_loss = 2.08238560, grad/param norm = 2.5943e-01, time/batch = 0.1103s	
382/2050 (epoch 9.317), train_loss = 2.09125617, grad/param norm = 2.8827e-01, time/batch = 0.1098s	
383/2050 (epoch 9.341), train_loss = 2.08268976, grad/param norm = 2.8782e-01, time/batch = 0.1104s	
384/2050 (epoch 9.366), train_loss = 2.08421363, grad/param norm = 2.8383e-01, time/batch = 0.1095s	
385/2050 (epoch 9.390), train_loss = 2.07541574, grad/param norm = 2.8054e-01, time/batch = 0.1097s	
386/2050 (epoch 9.415), train_loss = 2.06910022, grad/param norm = 2.9649e-01, time/batch = 0.1098s	
387/2050 (epoch 9.439), train_loss = 2.07778198, grad/param norm = 2.7191e-01, time/batch = 0.1097s	
388/2050 (epoch 9.463), train_loss = 2.06079680, grad/param norm = 2.4921e-01, time/batch = 0.1097s	
389/2050 (epoch 9.488), train_loss = 2.06759195, grad/param norm = 2.5643e-01, time/batch = 0.1097s	
390/2050 (epoch 9.512), train_loss = 2.06663533, grad/param norm = 2.6670e-01, time/batch = 0.1097s	
391/2050 (epoch 9.537), train_loss = 2.07187257, grad/param norm = 2.8380e-01, time/batch = 0.1103s	
392/2050 (epoch 9.561), train_loss = 2.05860536, grad/param norm = 2.7522e-01, time/batch = 0.1100s	
393/2050 (epoch 9.585), train_loss = 2.06593491, grad/param norm = 2.8392e-01, time/batch = 0.1097s	
394/2050 (epoch 9.610), train_loss = 2.07733571, grad/param norm = 2.8175e-01, time/batch = 0.1096s	
395/2050 (epoch 9.634), train_loss = 2.06781884, grad/param norm = 2.5848e-01, time/batch = 0.1105s	
396/2050 (epoch 9.659), train_loss = 2.06468462, grad/param norm = 2.5915e-01, time/batch = 0.1098s	
397/2050 (epoch 9.683), train_loss = 2.05698922, grad/param norm = 2.5477e-01, time/batch = 0.1098s	
398/2050 (epoch 9.707), train_loss = 2.04321004, grad/param norm = 2.4979e-01, time/batch = 0.1098s	
399/2050 (epoch 9.732), train_loss = 2.05198755, grad/param norm = 2.2676e-01, time/batch = 0.1096s	
400/2050 (epoch 9.756), train_loss = 2.05792452, grad/param norm = 2.1795e-01, time/batch = 0.1097s	
401/2050 (epoch 9.780), train_loss = 2.05574431, grad/param norm = 2.0739e-01, time/batch = 0.1103s	
402/2050 (epoch 9.805), train_loss = 2.03048003, grad/param norm = 2.0476e-01, time/batch = 0.1097s	
403/2050 (epoch 9.829), train_loss = 2.03634669, grad/param norm = 1.8688e-01, time/batch = 0.1100s	
404/2050 (epoch 9.854), train_loss = 2.02141538, grad/param norm = 1.9363e-01, time/batch = 0.1096s	
405/2050 (epoch 9.878), train_loss = 2.03552994, grad/param norm = 2.5503e-01, time/batch = 0.1097s	
406/2050 (epoch 9.902), train_loss = 2.05871108, grad/param norm = 3.3687e-01, time/batch = 0.1098s	
407/2050 (epoch 9.927), train_loss = 2.04512813, grad/param norm = 4.0697e-01, time/batch = 0.1102s	
408/2050 (epoch 9.951), train_loss = 2.09600580, grad/param norm = 4.2347e-01, time/batch = 0.1098s	
409/2050 (epoch 9.976), train_loss = 2.07244911, grad/param norm = 3.4867e-01, time/batch = 0.1095s	
decayed learning rate by a factor 0.97 to 0.00194	
410/2050 (epoch 10.000), train_loss = 2.03271402, grad/param norm = 2.2165e-01, time/batch = 0.1097s	
411/2050 (epoch 10.024), train_loss = 2.09875592, grad/param norm = 1.7080e-01, time/batch = 0.1102s	
412/2050 (epoch 10.049), train_loss = 2.03054731, grad/param norm = 1.6320e-01, time/batch = 0.1097s	
413/2050 (epoch 10.073), train_loss = 2.00411450, grad/param norm = 1.9676e-01, time/batch = 0.1098s	
414/2050 (epoch 10.098), train_loss = 2.02275061, grad/param norm = 2.5963e-01, time/batch = 0.1096s	
415/2050 (epoch 10.122), train_loss = 2.04411776, grad/param norm = 2.9894e-01, time/batch = 0.1098s	
416/2050 (epoch 10.146), train_loss = 2.03166328, grad/param norm = 3.1861e-01, time/batch = 0.1098s	
417/2050 (epoch 10.171), train_loss = 2.04109537, grad/param norm = 2.8032e-01, time/batch = 0.1097s	
418/2050 (epoch 10.195), train_loss = 2.02379743, grad/param norm = 2.3614e-01, time/batch = 0.1099s	
419/2050 (epoch 10.220), train_loss = 2.01468028, grad/param norm = 2.2053e-01, time/batch = 0.1096s	
420/2050 (epoch 10.244), train_loss = 2.00033371, grad/param norm = 1.8976e-01, time/batch = 0.1098s	
421/2050 (epoch 10.268), train_loss = 2.01858810, grad/param norm = 1.7134e-01, time/batch = 0.1103s	
422/2050 (epoch 10.293), train_loss = 2.01652894, grad/param norm = 1.9202e-01, time/batch = 0.1098s	
423/2050 (epoch 10.317), train_loss = 2.03110099, grad/param norm = 2.0685e-01, time/batch = 0.1097s	
424/2050 (epoch 10.341), train_loss = 2.01691009, grad/param norm = 2.1673e-01, time/batch = 0.1096s	
425/2050 (epoch 10.366), train_loss = 2.02763057, grad/param norm = 2.2903e-01, time/batch = 0.1097s	
426/2050 (epoch 10.390), train_loss = 2.01433471, grad/param norm = 2.6550e-01, time/batch = 0.1098s	
427/2050 (epoch 10.415), train_loss = 2.02300926, grad/param norm = 2.8360e-01, time/batch = 0.1097s	
428/2050 (epoch 10.439), train_loss = 2.02686453, grad/param norm = 3.0716e-01, time/batch = 0.1099s	
429/2050 (epoch 10.463), train_loss = 2.02492412, grad/param norm = 2.8974e-01, time/batch = 0.1097s	
430/2050 (epoch 10.488), train_loss = 2.01083636, grad/param norm = 2.2882e-01, time/batch = 0.1097s	
431/2050 (epoch 10.512), train_loss = 2.00572635, grad/param norm = 1.8379e-01, time/batch = 0.1103s	
432/2050 (epoch 10.537), train_loss = 2.00229530, grad/param norm = 1.7064e-01, time/batch = 0.1097s	
433/2050 (epoch 10.561), train_loss = 1.99597850, grad/param norm = 2.2265e-01, time/batch = 0.1102s	
434/2050 (epoch 10.585), train_loss = 2.01195962, grad/param norm = 2.8210e-01, time/batch = 0.1095s	
435/2050 (epoch 10.610), train_loss = 2.02966870, grad/param norm = 3.2985e-01, time/batch = 0.1098s	
436/2050 (epoch 10.634), train_loss = 2.04140742, grad/param norm = 3.6229e-01, time/batch = 0.1097s	
437/2050 (epoch 10.659), train_loss = 2.02721436, grad/param norm = 3.3079e-01, time/batch = 0.1098s	
438/2050 (epoch 10.683), train_loss = 2.01383888, grad/param norm = 2.8242e-01, time/batch = 0.1098s	
439/2050 (epoch 10.707), train_loss = 1.98918079, grad/param norm = 2.2275e-01, time/batch = 0.1096s	
440/2050 (epoch 10.732), train_loss = 1.99102673, grad/param norm = 1.7142e-01, time/batch = 0.1098s	
441/2050 (epoch 10.756), train_loss = 1.99670889, grad/param norm = 1.4573e-01, time/batch = 0.1102s	
442/2050 (epoch 10.780), train_loss = 1.99152095, grad/param norm = 1.3824e-01, time/batch = 0.1096s	
443/2050 (epoch 10.805), train_loss = 1.97043095, grad/param norm = 1.6177e-01, time/batch = 0.1097s	
444/2050 (epoch 10.829), train_loss = 1.98020828, grad/param norm = 1.9087e-01, time/batch = 0.1098s	
445/2050 (epoch 10.854), train_loss = 1.97006016, grad/param norm = 2.3143e-01, time/batch = 0.1097s	
446/2050 (epoch 10.878), train_loss = 1.98556790, grad/param norm = 2.5221e-01, time/batch = 0.1098s	
447/2050 (epoch 10.902), train_loss = 1.99694605, grad/param norm = 2.7183e-01, time/batch = 0.1098s	
448/2050 (epoch 10.927), train_loss = 1.97849157, grad/param norm = 2.8890e-01, time/batch = 0.1115s	
449/2050 (epoch 10.951), train_loss = 2.01414802, grad/param norm = 2.5926e-01, time/batch = 0.1096s	
450/2050 (epoch 10.976), train_loss = 1.99191557, grad/param norm = 2.2416e-01, time/batch = 0.1097s	
decayed learning rate by a factor 0.97 to 0.0018818	
451/2050 (epoch 11.000), train_loss = 1.97224741, grad/param norm = 2.2942e-01, time/batch = 0.1103s	
452/2050 (epoch 11.024), train_loss = 2.06296528, grad/param norm = 2.8406e-01, time/batch = 0.1098s	
453/2050 (epoch 11.049), train_loss = 1.99502687, grad/param norm = 2.7928e-01, time/batch = 0.1098s	
454/2050 (epoch 11.073), train_loss = 1.95643546, grad/param norm = 2.2624e-01, time/batch = 0.1096s	
455/2050 (epoch 11.098), train_loss = 1.96322484, grad/param norm = 1.8756e-01, time/batch = 0.1097s	
456/2050 (epoch 11.122), train_loss = 1.97351849, grad/param norm = 1.7599e-01, time/batch = 0.1098s	
457/2050 (epoch 11.146), train_loss = 1.96032763, grad/param norm = 1.8313e-01, time/batch = 0.1099s	
458/2050 (epoch 11.171), train_loss = 1.97610526, grad/param norm = 1.9028e-01, time/batch = 0.1098s	
459/2050 (epoch 11.195), train_loss = 1.96933512, grad/param norm = 1.9606e-01, time/batch = 0.1101s	
460/2050 (epoch 11.220), train_loss = 1.96100542, grad/param norm = 1.9200e-01, time/batch = 0.1097s	
461/2050 (epoch 11.244), train_loss = 1.95110163, grad/param norm = 1.9343e-01, time/batch = 0.1101s	
462/2050 (epoch 11.268), train_loss = 1.97398943, grad/param norm = 1.9891e-01, time/batch = 0.1097s	
463/2050 (epoch 11.293), train_loss = 1.96986658, grad/param norm = 2.0659e-01, time/batch = 0.1097s	
464/2050 (epoch 11.317), train_loss = 1.98392740, grad/param norm = 2.1735e-01, time/batch = 0.1095s	
465/2050 (epoch 11.341), train_loss = 1.97246813, grad/param norm = 2.3031e-01, time/batch = 0.1098s	
466/2050 (epoch 11.366), train_loss = 1.97838843, grad/param norm = 2.4811e-01, time/batch = 0.1097s	
467/2050 (epoch 11.390), train_loss = 1.96950660, grad/param norm = 2.8976e-01, time/batch = 0.1099s	
468/2050 (epoch 11.415), train_loss = 1.97279773, grad/param norm = 3.1521e-01, time/batch = 0.1098s	
469/2050 (epoch 11.439), train_loss = 1.97961279, grad/param norm = 3.2630e-01, time/batch = 0.1096s	
470/2050 (epoch 11.463), train_loss = 1.97025901, grad/param norm = 2.5926e-01, time/batch = 0.1096s	
471/2050 (epoch 11.488), train_loss = 1.95787363, grad/param norm = 1.9601e-01, time/batch = 0.1102s	
472/2050 (epoch 11.512), train_loss = 1.95680747, grad/param norm = 1.6930e-01, time/batch = 0.1097s	
473/2050 (epoch 11.537), train_loss = 1.95659519, grad/param norm = 1.7471e-01, time/batch = 0.1097s	
474/2050 (epoch 11.561), train_loss = 1.95383543, grad/param norm = 2.0121e-01, time/batch = 0.1098s	
475/2050 (epoch 11.585), train_loss = 1.95919502, grad/param norm = 2.1407e-01, time/batch = 0.1098s	
476/2050 (epoch 11.610), train_loss = 1.96826585, grad/param norm = 2.3581e-01, time/batch = 0.1098s	
477/2050 (epoch 11.634), train_loss = 1.96688517, grad/param norm = 2.5445e-01, time/batch = 0.1097s	
478/2050 (epoch 11.659), train_loss = 1.95795073, grad/param norm = 2.6374e-01, time/batch = 0.1098s	
479/2050 (epoch 11.683), train_loss = 1.95787989, grad/param norm = 2.6599e-01, time/batch = 0.1096s	
480/2050 (epoch 11.707), train_loss = 1.94126203, grad/param norm = 2.6249e-01, time/batch = 0.1098s	
481/2050 (epoch 11.732), train_loss = 1.95153421, grad/param norm = 2.3877e-01, time/batch = 0.1102s	
482/2050 (epoch 11.756), train_loss = 1.95866283, grad/param norm = 2.1745e-01, time/batch = 0.1097s	
483/2050 (epoch 11.780), train_loss = 1.95097267, grad/param norm = 1.9480e-01, time/batch = 0.1097s	
484/2050 (epoch 11.805), train_loss = 1.92359356, grad/param norm = 1.7375e-01, time/batch = 0.1095s	
485/2050 (epoch 11.829), train_loss = 1.92384581, grad/param norm = 1.4506e-01, time/batch = 0.1102s	
486/2050 (epoch 11.854), train_loss = 1.90961742, grad/param norm = 1.3282e-01, time/batch = 0.1098s	
487/2050 (epoch 11.878), train_loss = 1.91564568, grad/param norm = 1.4663e-01, time/batch = 0.1097s	
488/2050 (epoch 11.902), train_loss = 1.93439615, grad/param norm = 1.6557e-01, time/batch = 0.1098s	
489/2050 (epoch 11.927), train_loss = 1.90981639, grad/param norm = 1.9946e-01, time/batch = 0.1100s	
490/2050 (epoch 11.951), train_loss = 1.96676098, grad/param norm = 2.4890e-01, time/batch = 0.1097s	
491/2050 (epoch 11.976), train_loss = 1.95475575, grad/param norm = 2.8239e-01, time/batch = 0.1102s	
decayed learning rate by a factor 0.97 to 0.001825346	
492/2050 (epoch 12.000), train_loss = 1.94381045, grad/param norm = 2.7235e-01, time/batch = 0.1097s	
493/2050 (epoch 12.024), train_loss = 2.02844156, grad/param norm = 2.5784e-01, time/batch = 0.1097s	
494/2050 (epoch 12.049), train_loss = 1.94475285, grad/param norm = 2.3956e-01, time/batch = 0.1096s	
495/2050 (epoch 12.073), train_loss = 1.91137646, grad/param norm = 2.3300e-01, time/batch = 0.1098s	
496/2050 (epoch 12.098), train_loss = 1.92653327, grad/param norm = 2.4289e-01, time/batch = 0.1098s	
497/2050 (epoch 12.122), train_loss = 1.93657902, grad/param norm = 2.1645e-01, time/batch = 0.1097s	
498/2050 (epoch 12.146), train_loss = 1.91446655, grad/param norm = 2.0868e-01, time/batch = 0.1098s	
499/2050 (epoch 12.171), train_loss = 1.93245394, grad/param norm = 2.0387e-01, time/batch = 0.1096s	
500/2050 (epoch 12.195), train_loss = 1.92032467, grad/param norm = 1.9153e-01, time/batch = 0.1100s	
501/2050 (epoch 12.220), train_loss = 1.91478489, grad/param norm = 1.7923e-01, time/batch = 0.1103s	
502/2050 (epoch 12.244), train_loss = 1.89875633, grad/param norm = 1.5973e-01, time/batch = 0.1097s	
503/2050 (epoch 12.268), train_loss = 1.92252958, grad/param norm = 1.6172e-01, time/batch = 0.1098s	
504/2050 (epoch 12.293), train_loss = 1.91611862, grad/param norm = 1.6823e-01, time/batch = 0.1095s	
505/2050 (epoch 12.317), train_loss = 1.93669798, grad/param norm = 1.7497e-01, time/batch = 0.1098s	
506/2050 (epoch 12.341), train_loss = 1.92094201, grad/param norm = 1.7954e-01, time/batch = 0.1098s	
507/2050 (epoch 12.366), train_loss = 1.92813865, grad/param norm = 1.8341e-01, time/batch = 0.1097s	
508/2050 (epoch 12.390), train_loss = 1.91267816, grad/param norm = 1.7910e-01, time/batch = 0.1098s	
509/2050 (epoch 12.415), train_loss = 1.91051180, grad/param norm = 1.7635e-01, time/batch = 0.1096s	
510/2050 (epoch 12.439), train_loss = 1.91606301, grad/param norm = 1.8353e-01, time/batch = 0.1097s	
511/2050 (epoch 12.463), train_loss = 1.90698086, grad/param norm = 1.7717e-01, time/batch = 0.1108s	
512/2050 (epoch 12.488), train_loss = 1.91146934, grad/param norm = 1.6202e-01, time/batch = 0.1097s	
513/2050 (epoch 12.512), train_loss = 1.90489961, grad/param norm = 1.5580e-01, time/batch = 0.1097s	
514/2050 (epoch 12.537), train_loss = 1.90823350, grad/param norm = 1.8100e-01, time/batch = 0.1096s	
515/2050 (epoch 12.561), train_loss = 1.90847648, grad/param norm = 2.4107e-01, time/batch = 0.1099s	
516/2050 (epoch 12.585), train_loss = 1.92815567, grad/param norm = 3.3028e-01, time/batch = 0.1098s	
517/2050 (epoch 12.610), train_loss = 1.95006619, grad/param norm = 3.1991e-01, time/batch = 0.1098s	
518/2050 (epoch 12.634), train_loss = 1.93163953, grad/param norm = 2.8190e-01, time/batch = 0.1099s	
519/2050 (epoch 12.659), train_loss = 1.91714956, grad/param norm = 2.4614e-01, time/batch = 0.1096s	
520/2050 (epoch 12.683), train_loss = 1.90190794, grad/param norm = 2.0287e-01, time/batch = 0.1097s	
521/2050 (epoch 12.707), train_loss = 1.88498087, grad/param norm = 1.7816e-01, time/batch = 0.1102s	
522/2050 (epoch 12.732), train_loss = 1.89656971, grad/param norm = 1.9084e-01, time/batch = 0.1097s	
523/2050 (epoch 12.756), train_loss = 1.91436853, grad/param norm = 2.0928e-01, time/batch = 0.1100s	
524/2050 (epoch 12.780), train_loss = 1.91140092, grad/param norm = 2.2639e-01, time/batch = 0.1095s	
525/2050 (epoch 12.805), train_loss = 1.89082281, grad/param norm = 2.1391e-01, time/batch = 0.1097s	
526/2050 (epoch 12.829), train_loss = 1.88497590, grad/param norm = 1.8613e-01, time/batch = 0.1101s	
527/2050 (epoch 12.854), train_loss = 1.87487943, grad/param norm = 1.8195e-01, time/batch = 0.1098s	
528/2050 (epoch 12.878), train_loss = 1.88264818, grad/param norm = 1.9069e-01, time/batch = 0.1098s	
529/2050 (epoch 12.902), train_loss = 1.90113902, grad/param norm = 2.0132e-01, time/batch = 0.1096s	
530/2050 (epoch 12.927), train_loss = 1.87489674, grad/param norm = 2.0211e-01, time/batch = 0.1099s	
531/2050 (epoch 12.951), train_loss = 1.92090614, grad/param norm = 1.9741e-01, time/batch = 0.1103s	
532/2050 (epoch 12.976), train_loss = 1.89771256, grad/param norm = 1.8450e-01, time/batch = 0.1097s	
decayed learning rate by a factor 0.97 to 0.00177058562	
533/2050 (epoch 13.000), train_loss = 1.87865202, grad/param norm = 1.5104e-01, time/batch = 0.1097s	
534/2050 (epoch 13.024), train_loss = 1.97553684, grad/param norm = 1.5048e-01, time/batch = 0.1096s	
535/2050 (epoch 13.049), train_loss = 1.88421207, grad/param norm = 1.4332e-01, time/batch = 0.1097s	
536/2050 (epoch 13.073), train_loss = 1.85253786, grad/param norm = 1.5767e-01, time/batch = 0.1097s	
537/2050 (epoch 13.098), train_loss = 1.87354165, grad/param norm = 1.9076e-01, time/batch = 0.1098s	
538/2050 (epoch 13.122), train_loss = 1.88771820, grad/param norm = 2.2720e-01, time/batch = 0.1097s	
539/2050 (epoch 13.146), train_loss = 1.87875523, grad/param norm = 2.3033e-01, time/batch = 0.1096s	
540/2050 (epoch 13.171), train_loss = 1.88734438, grad/param norm = 2.1538e-01, time/batch = 0.1100s	
541/2050 (epoch 13.195), train_loss = 1.87943720, grad/param norm = 1.8227e-01, time/batch = 0.1107s	
542/2050 (epoch 13.220), train_loss = 1.86576665, grad/param norm = 1.5327e-01, time/batch = 0.1098s	
543/2050 (epoch 13.244), train_loss = 1.85694513, grad/param norm = 1.5850e-01, time/batch = 0.1097s	
544/2050 (epoch 13.268), train_loss = 1.88090967, grad/param norm = 1.6086e-01, time/batch = 0.1095s	
545/2050 (epoch 13.293), train_loss = 1.87657792, grad/param norm = 1.5452e-01, time/batch = 0.1099s	
546/2050 (epoch 13.317), train_loss = 1.88999431, grad/param norm = 1.6162e-01, time/batch = 0.1098s	
547/2050 (epoch 13.341), train_loss = 1.87673629, grad/param norm = 1.6203e-01, time/batch = 0.1098s	
548/2050 (epoch 13.366), train_loss = 1.87946259, grad/param norm = 1.7113e-01, time/batch = 0.1098s	
549/2050 (epoch 13.390), train_loss = 1.87543339, grad/param norm = 2.0371e-01, time/batch = 0.1096s	
550/2050 (epoch 13.415), train_loss = 1.87451376, grad/param norm = 2.0040e-01, time/batch = 0.1097s	
551/2050 (epoch 13.439), train_loss = 1.87636412, grad/param norm = 1.9530e-01, time/batch = 0.1103s	
552/2050 (epoch 13.463), train_loss = 1.87315735, grad/param norm = 2.0022e-01, time/batch = 0.1099s	
553/2050 (epoch 13.488), train_loss = 1.88110949, grad/param norm = 2.0604e-01, time/batch = 0.1096s	
554/2050 (epoch 13.512), train_loss = 1.87525552, grad/param norm = 1.9742e-01, time/batch = 0.1096s	
555/2050 (epoch 13.537), train_loss = 1.87540864, grad/param norm = 2.2653e-01, time/batch = 0.1098s	
556/2050 (epoch 13.561), train_loss = 1.88251645, grad/param norm = 2.7825e-01, time/batch = 0.1100s	
557/2050 (epoch 13.585), train_loss = 1.88711928, grad/param norm = 3.1599e-01, time/batch = 0.1098s	
558/2050 (epoch 13.610), train_loss = 1.89788809, grad/param norm = 2.8336e-01, time/batch = 0.1098s	
559/2050 (epoch 13.634), train_loss = 1.87670573, grad/param norm = 2.1839e-01, time/batch = 0.1097s	
560/2050 (epoch 13.659), train_loss = 1.86384125, grad/param norm = 1.7280e-01, time/batch = 0.1097s	
561/2050 (epoch 13.683), train_loss = 1.85599741, grad/param norm = 1.4121e-01, time/batch = 0.1103s	
562/2050 (epoch 13.707), train_loss = 1.84063278, grad/param norm = 1.2880e-01, time/batch = 0.1097s	
563/2050 (epoch 13.732), train_loss = 1.85237982, grad/param norm = 1.2883e-01, time/batch = 0.1097s	
564/2050 (epoch 13.756), train_loss = 1.86553615, grad/param norm = 1.3674e-01, time/batch = 0.1096s	
565/2050 (epoch 13.780), train_loss = 1.86291129, grad/param norm = 1.5430e-01, time/batch = 0.1097s	
566/2050 (epoch 13.805), train_loss = 1.84243699, grad/param norm = 1.6950e-01, time/batch = 0.1098s	
567/2050 (epoch 13.829), train_loss = 1.84642164, grad/param norm = 1.6642e-01, time/batch = 0.1100s	
568/2050 (epoch 13.854), train_loss = 1.83228625, grad/param norm = 1.6188e-01, time/batch = 0.1098s	
569/2050 (epoch 13.878), train_loss = 1.83895986, grad/param norm = 1.6606e-01, time/batch = 0.1097s	
570/2050 (epoch 13.902), train_loss = 1.85414469, grad/param norm = 1.6045e-01, time/batch = 0.1098s	
571/2050 (epoch 13.927), train_loss = 1.82432278, grad/param norm = 1.6057e-01, time/batch = 0.1103s	
572/2050 (epoch 13.951), train_loss = 1.87143493, grad/param norm = 1.7839e-01, time/batch = 0.1096s	
573/2050 (epoch 13.976), train_loss = 1.84985985, grad/param norm = 1.9364e-01, time/batch = 0.1097s	
decayed learning rate by a factor 0.97 to 0.0017174680514	
574/2050 (epoch 14.000), train_loss = 1.84779263, grad/param norm = 2.3737e-01, time/batch = 0.1095s	
575/2050 (epoch 14.024), train_loss = 1.96539720, grad/param norm = 2.6259e-01, time/batch = 0.1097s	
576/2050 (epoch 14.049), train_loss = 1.86039625, grad/param norm = 2.2039e-01, time/batch = 0.1098s	
577/2050 (epoch 14.073), train_loss = 1.82292793, grad/param norm = 1.8422e-01, time/batch = 0.1097s	
578/2050 (epoch 14.098), train_loss = 1.83362146, grad/param norm = 1.5681e-01, time/batch = 0.1098s	
579/2050 (epoch 14.122), train_loss = 1.83850811, grad/param norm = 1.3601e-01, time/batch = 0.1095s	
580/2050 (epoch 14.146), train_loss = 1.82455849, grad/param norm = 1.3910e-01, time/batch = 0.1097s	
581/2050 (epoch 14.171), train_loss = 1.83976351, grad/param norm = 1.5415e-01, time/batch = 0.1102s	
582/2050 (epoch 14.195), train_loss = 1.83597757, grad/param norm = 1.4733e-01, time/batch = 0.1101s	
583/2050 (epoch 14.220), train_loss = 1.82465509, grad/param norm = 1.2625e-01, time/batch = 0.1097s	
584/2050 (epoch 14.244), train_loss = 1.81084177, grad/param norm = 1.3398e-01, time/batch = 0.1094s	
585/2050 (epoch 14.268), train_loss = 1.84175094, grad/param norm = 1.5795e-01, time/batch = 0.1098s	
586/2050 (epoch 14.293), train_loss = 1.84172344, grad/param norm = 2.1333e-01, time/batch = 0.1098s	
587/2050 (epoch 14.317), train_loss = 1.87882522, grad/param norm = 2.5141e-01, time/batch = 0.1097s	
588/2050 (epoch 14.341), train_loss = 1.85189632, grad/param norm = 2.2174e-01, time/batch = 0.1098s	
589/2050 (epoch 14.366), train_loss = 1.84977403, grad/param norm = 1.8857e-01, time/batch = 0.1096s	
590/2050 (epoch 14.390), train_loss = 1.83265554, grad/param norm = 1.6902e-01, time/batch = 0.1097s	
591/2050 (epoch 14.415), train_loss = 1.82787950, grad/param norm = 1.6666e-01, time/batch = 0.1103s	
592/2050 (epoch 14.439), train_loss = 1.83451842, grad/param norm = 1.7207e-01, time/batch = 0.1098s	
593/2050 (epoch 14.463), train_loss = 1.83405489, grad/param norm = 2.0501e-01, time/batch = 0.1098s	
594/2050 (epoch 14.488), train_loss = 1.85037652, grad/param norm = 2.1391e-01, time/batch = 0.1095s	
595/2050 (epoch 14.512), train_loss = 1.83771942, grad/param norm = 2.3747e-01, time/batch = 0.1097s	
596/2050 (epoch 14.537), train_loss = 1.84616052, grad/param norm = 2.4134e-01, time/batch = 0.1098s	
597/2050 (epoch 14.561), train_loss = 1.82975664, grad/param norm = 2.0474e-01, time/batch = 0.1116s	
598/2050 (epoch 14.585), train_loss = 1.82922157, grad/param norm = 1.8525e-01, time/batch = 0.1099s	
599/2050 (epoch 14.610), train_loss = 1.84120866, grad/param norm = 1.7677e-01, time/batch = 0.1096s	
600/2050 (epoch 14.634), train_loss = 1.83462413, grad/param norm = 1.7302e-01, time/batch = 0.1098s	
601/2050 (epoch 14.659), train_loss = 1.83105496, grad/param norm = 1.7097e-01, time/batch = 0.1103s	
602/2050 (epoch 14.683), train_loss = 1.82973632, grad/param norm = 1.8057e-01, time/batch = 0.1097s	
603/2050 (epoch 14.707), train_loss = 1.81510096, grad/param norm = 1.6932e-01, time/batch = 0.1097s	
604/2050 (epoch 14.732), train_loss = 1.82125250, grad/param norm = 1.3876e-01, time/batch = 0.1096s	
605/2050 (epoch 14.756), train_loss = 1.82937439, grad/param norm = 1.2259e-01, time/batch = 0.1096s	
606/2050 (epoch 14.780), train_loss = 1.82184189, grad/param norm = 1.1460e-01, time/batch = 0.1098s	
607/2050 (epoch 14.805), train_loss = 1.79870731, grad/param norm = 1.1958e-01, time/batch = 0.1098s	
608/2050 (epoch 14.829), train_loss = 1.80057878, grad/param norm = 1.3763e-01, time/batch = 0.1101s	
609/2050 (epoch 14.854), train_loss = 1.79526884, grad/param norm = 1.6405e-01, time/batch = 0.1096s	
610/2050 (epoch 14.878), train_loss = 1.80443778, grad/param norm = 2.0134e-01, time/batch = 0.1097s	
611/2050 (epoch 14.902), train_loss = 1.83373704, grad/param norm = 2.3613e-01, time/batch = 0.1103s	
612/2050 (epoch 14.927), train_loss = 1.80230672, grad/param norm = 2.2911e-01, time/batch = 0.1098s	
613/2050 (epoch 14.951), train_loss = 1.85016243, grad/param norm = 2.1000e-01, time/batch = 0.1097s	
614/2050 (epoch 14.976), train_loss = 1.81690753, grad/param norm = 1.7871e-01, time/batch = 0.1095s	
decayed learning rate by a factor 0.97 to 0.001665944009858	
615/2050 (epoch 15.000), train_loss = 1.80316863, grad/param norm = 1.3444e-01, time/batch = 0.1098s	
616/2050 (epoch 15.024), train_loss = 1.90960963, grad/param norm = 1.3924e-01, time/batch = 0.1098s	
617/2050 (epoch 15.049), train_loss = 1.81167439, grad/param norm = 1.5748e-01, time/batch = 0.1097s	
618/2050 (epoch 15.073), train_loss = 1.78178176, grad/param norm = 1.7539e-01, time/batch = 0.1098s	
619/2050 (epoch 15.098), train_loss = 1.79999022, grad/param norm = 1.5832e-01, time/batch = 0.1096s	
620/2050 (epoch 15.122), train_loss = 1.80181909, grad/param norm = 1.5079e-01, time/batch = 0.1097s	
621/2050 (epoch 15.146), train_loss = 1.79185218, grad/param norm = 1.5838e-01, time/batch = 0.1102s	
622/2050 (epoch 15.171), train_loss = 1.80601519, grad/param norm = 1.7150e-01, time/batch = 0.1097s	
623/2050 (epoch 15.195), train_loss = 1.80674257, grad/param norm = 1.7225e-01, time/batch = 0.1100s	
624/2050 (epoch 15.220), train_loss = 1.79616630, grad/param norm = 1.6227e-01, time/batch = 0.1095s	
625/2050 (epoch 15.244), train_loss = 1.78508140, grad/param norm = 1.6698e-01, time/batch = 0.1097s	
626/2050 (epoch 15.268), train_loss = 1.80962938, grad/param norm = 1.5857e-01, time/batch = 0.1098s	
627/2050 (epoch 15.293), train_loss = 1.80475268, grad/param norm = 1.5131e-01, time/batch = 0.1097s	
628/2050 (epoch 15.317), train_loss = 1.81918828, grad/param norm = 1.4282e-01, time/batch = 0.1098s	
629/2050 (epoch 15.341), train_loss = 1.80149009, grad/param norm = 1.3059e-01, time/batch = 0.1097s	
630/2050 (epoch 15.366), train_loss = 1.79873948, grad/param norm = 1.0852e-01, time/batch = 0.1098s	
631/2050 (epoch 15.390), train_loss = 1.78987865, grad/param norm = 1.0245e-01, time/batch = 0.1102s	
632/2050 (epoch 15.415), train_loss = 1.78363471, grad/param norm = 1.0049e-01, time/batch = 0.1097s	
633/2050 (epoch 15.439), train_loss = 1.78874677, grad/param norm = 9.7884e-02, time/batch = 0.1097s	
634/2050 (epoch 15.463), train_loss = 1.78673312, grad/param norm = 1.1002e-01, time/batch = 0.1099s	
635/2050 (epoch 15.488), train_loss = 1.80051377, grad/param norm = 1.4939e-01, time/batch = 0.1097s	
636/2050 (epoch 15.512), train_loss = 1.80765890, grad/param norm = 1.8254e-01, time/batch = 0.1098s	
637/2050 (epoch 15.537), train_loss = 1.80463820, grad/param norm = 1.8473e-01, time/batch = 0.1098s	
638/2050 (epoch 15.561), train_loss = 1.79923448, grad/param norm = 1.5596e-01, time/batch = 0.1098s	
639/2050 (epoch 15.585), train_loss = 1.78434864, grad/param norm = 1.3679e-01, time/batch = 0.1096s	
640/2050 (epoch 15.610), train_loss = 1.80315481, grad/param norm = 1.7354e-01, time/batch = 0.1097s	
641/2050 (epoch 15.634), train_loss = 1.80491197, grad/param norm = 2.2858e-01, time/batch = 0.1102s	
642/2050 (epoch 15.659), train_loss = 1.81264368, grad/param norm = 2.7421e-01, time/batch = 0.1097s	
643/2050 (epoch 15.683), train_loss = 1.81365792, grad/param norm = 2.6548e-01, time/batch = 0.1097s	
644/2050 (epoch 15.707), train_loss = 1.78953876, grad/param norm = 2.3586e-01, time/batch = 0.1095s	
645/2050 (epoch 15.732), train_loss = 1.80655865, grad/param norm = 2.4542e-01, time/batch = 0.1098s	
646/2050 (epoch 15.756), train_loss = 1.82255957, grad/param norm = 2.3246e-01, time/batch = 0.1097s	
647/2050 (epoch 15.780), train_loss = 1.80179841, grad/param norm = 1.6796e-01, time/batch = 0.1098s	
648/2050 (epoch 15.805), train_loss = 1.76898014, grad/param norm = 1.2777e-01, time/batch = 0.1098s	
649/2050 (epoch 15.829), train_loss = 1.76488662, grad/param norm = 9.9820e-02, time/batch = 0.1099s	
650/2050 (epoch 15.854), train_loss = 1.75262753, grad/param norm = 8.3215e-02, time/batch = 0.1097s	
651/2050 (epoch 15.878), train_loss = 1.75678155, grad/param norm = 7.0058e-02, time/batch = 0.1102s	
652/2050 (epoch 15.902), train_loss = 1.77438934, grad/param norm = 7.1613e-02, time/batch = 0.1097s	
653/2050 (epoch 15.927), train_loss = 1.74685087, grad/param norm = 9.5728e-02, time/batch = 0.1097s	
654/2050 (epoch 15.951), train_loss = 1.80203459, grad/param norm = 1.2309e-01, time/batch = 0.1096s	
655/2050 (epoch 15.976), train_loss = 1.77858280, grad/param norm = 1.7456e-01, time/batch = 0.1097s	
decayed learning rate by a factor 0.97 to 0.0016159656895623	
656/2050 (epoch 16.000), train_loss = 1.79272905, grad/param norm = 2.1681e-01, time/batch = 0.1098s	
657/2050 (epoch 16.024), train_loss = 1.90423044, grad/param norm = 2.1705e-01, time/batch = 0.1097s	
658/2050 (epoch 16.049), train_loss = 1.79130805, grad/param norm = 1.7489e-01, time/batch = 0.1098s	
659/2050 (epoch 16.073), train_loss = 1.74877226, grad/param norm = 1.5230e-01, time/batch = 0.1096s	
660/2050 (epoch 16.098), train_loss = 1.76664943, grad/param norm = 1.4028e-01, time/batch = 0.1097s	
661/2050 (epoch 16.122), train_loss = 1.76747124, grad/param norm = 1.3411e-01, time/batch = 0.1102s	
662/2050 (epoch 16.146), train_loss = 1.75626110, grad/param norm = 1.5009e-01, time/batch = 0.1098s	
663/2050 (epoch 16.171), train_loss = 1.77181594, grad/param norm = 1.7656e-01, time/batch = 0.1097s	
664/2050 (epoch 16.195), train_loss = 1.77440965, grad/param norm = 1.8705e-01, time/batch = 0.1099s	
665/2050 (epoch 16.220), train_loss = 1.76569192, grad/param norm = 1.7629e-01, time/batch = 0.1097s	
666/2050 (epoch 16.244), train_loss = 1.75023282, grad/param norm = 1.7000e-01, time/batch = 0.1098s	
667/2050 (epoch 16.268), train_loss = 1.77630189, grad/param norm = 1.6217e-01, time/batch = 0.1098s	
668/2050 (epoch 16.293), train_loss = 1.77075564, grad/param norm = 1.5772e-01, time/batch = 0.1097s	
669/2050 (epoch 16.317), train_loss = 1.79082692, grad/param norm = 1.5681e-01, time/batch = 0.1096s	
670/2050 (epoch 16.341), train_loss = 1.77234243, grad/param norm = 1.4604e-01, time/batch = 0.1097s	
671/2050 (epoch 16.366), train_loss = 1.77341762, grad/param norm = 1.4855e-01, time/batch = 0.1102s	
672/2050 (epoch 16.390), train_loss = 1.77173843, grad/param norm = 1.6464e-01, time/batch = 0.1104s	
673/2050 (epoch 16.415), train_loss = 1.76535990, grad/param norm = 1.7003e-01, time/batch = 0.1098s	
674/2050 (epoch 16.439), train_loss = 1.77462705, grad/param norm = 1.6725e-01, time/batch = 0.1096s	
675/2050 (epoch 16.463), train_loss = 1.76767261, grad/param norm = 1.6294e-01, time/batch = 0.1101s	
676/2050 (epoch 16.488), train_loss = 1.77769060, grad/param norm = 1.5323e-01, time/batch = 0.1098s	
677/2050 (epoch 16.512), train_loss = 1.76353560, grad/param norm = 1.4193e-01, time/batch = 0.1097s	
678/2050 (epoch 16.537), train_loss = 1.76755205, grad/param norm = 1.5640e-01, time/batch = 0.1097s	
679/2050 (epoch 16.561), train_loss = 1.76388358, grad/param norm = 1.7113e-01, time/batch = 0.1101s	
680/2050 (epoch 16.585), train_loss = 1.76702065, grad/param norm = 1.8550e-01, time/batch = 0.1097s	
681/2050 (epoch 16.610), train_loss = 1.77869686, grad/param norm = 2.0629e-01, time/batch = 0.1102s	
682/2050 (epoch 16.634), train_loss = 1.77306141, grad/param norm = 1.9728e-01, time/batch = 0.1097s	
683/2050 (epoch 16.659), train_loss = 1.76450125, grad/param norm = 1.7168e-01, time/batch = 0.1097s	
684/2050 (epoch 16.683), train_loss = 1.76125411, grad/param norm = 1.6825e-01, time/batch = 0.1096s	
685/2050 (epoch 16.707), train_loss = 1.74499905, grad/param norm = 1.5545e-01, time/batch = 0.1097s	
686/2050 (epoch 16.732), train_loss = 1.75567033, grad/param norm = 1.2214e-01, time/batch = 0.1098s	
687/2050 (epoch 16.756), train_loss = 1.76135624, grad/param norm = 1.0460e-01, time/batch = 0.1097s	
688/2050 (epoch 16.780), train_loss = 1.75614891, grad/param norm = 9.8579e-02, time/batch = 0.1098s	
689/2050 (epoch 16.805), train_loss = 1.73219428, grad/param norm = 1.0444e-01, time/batch = 0.1097s	
690/2050 (epoch 16.829), train_loss = 1.73713282, grad/param norm = 1.1587e-01, time/batch = 0.1101s	
691/2050 (epoch 16.854), train_loss = 1.73040497, grad/param norm = 1.4059e-01, time/batch = 0.1103s	
692/2050 (epoch 16.878), train_loss = 1.74358432, grad/param norm = 1.6051e-01, time/batch = 0.1097s	
693/2050 (epoch 16.902), train_loss = 1.76228423, grad/param norm = 1.7823e-01, time/batch = 0.1097s	
694/2050 (epoch 16.927), train_loss = 1.73812086, grad/param norm = 1.7835e-01, time/batch = 0.1095s	
695/2050 (epoch 16.951), train_loss = 1.78140066, grad/param norm = 1.6288e-01, time/batch = 0.1097s	
696/2050 (epoch 16.976), train_loss = 1.74941730, grad/param norm = 1.4241e-01, time/batch = 0.1098s	
decayed learning rate by a factor 0.97 to 0.0015674867188754	
697/2050 (epoch 17.000), train_loss = 1.74112112, grad/param norm = 1.1423e-01, time/batch = 0.1097s	
698/2050 (epoch 17.024), train_loss = 1.85787540, grad/param norm = 1.1483e-01, time/batch = 0.1097s	
699/2050 (epoch 17.049), train_loss = 1.74877366, grad/param norm = 1.3607e-01, time/batch = 0.1096s	
700/2050 (epoch 17.073), train_loss = 1.72225122, grad/param norm = 1.5098e-01, time/batch = 0.1097s	
701/2050 (epoch 17.098), train_loss = 1.74316634, grad/param norm = 1.4973e-01, time/batch = 0.1103s	
702/2050 (epoch 17.122), train_loss = 1.74190800, grad/param norm = 1.3498e-01, time/batch = 0.1097s	
703/2050 (epoch 17.146), train_loss = 1.73125871, grad/param norm = 1.2726e-01, time/batch = 0.1096s	
704/2050 (epoch 17.171), train_loss = 1.74102525, grad/param norm = 1.2393e-01, time/batch = 0.1096s	
705/2050 (epoch 17.195), train_loss = 1.73984487, grad/param norm = 1.1829e-01, time/batch = 0.1101s	
706/2050 (epoch 17.220), train_loss = 1.72920421, grad/param norm = 1.0709e-01, time/batch = 0.1098s	
707/2050 (epoch 17.244), train_loss = 1.71352792, grad/param norm = 1.0725e-01, time/batch = 0.1097s	
708/2050 (epoch 17.268), train_loss = 1.74185380, grad/param norm = 1.2651e-01, time/batch = 0.1098s	
709/2050 (epoch 17.293), train_loss = 1.74345255, grad/param norm = 1.7345e-01, time/batch = 0.1095s	
710/2050 (epoch 17.317), train_loss = 1.77676705, grad/param norm = 2.5277e-01, time/batch = 0.1098s	
711/2050 (epoch 17.341), train_loss = 1.77005094, grad/param norm = 2.6800e-01, time/batch = 0.1103s	
712/2050 (epoch 17.366), train_loss = 1.75846062, grad/param norm = 2.1556e-01, time/batch = 0.1096s	
713/2050 (epoch 17.390), train_loss = 1.74533048, grad/param norm = 1.7493e-01, time/batch = 0.1097s	
714/2050 (epoch 17.415), train_loss = 1.73185153, grad/param norm = 1.4908e-01, time/batch = 0.1095s	
715/2050 (epoch 17.439), train_loss = 1.73654086, grad/param norm = 1.2740e-01, time/batch = 0.1097s	
716/2050 (epoch 17.463), train_loss = 1.73117025, grad/param norm = 1.0799e-01, time/batch = 0.1101s	
717/2050 (epoch 17.488), train_loss = 1.73649645, grad/param norm = 9.9474e-02, time/batch = 0.1097s	
718/2050 (epoch 17.512), train_loss = 1.72751489, grad/param norm = 9.3360e-02, time/batch = 0.1099s	
719/2050 (epoch 17.537), train_loss = 1.72753276, grad/param norm = 9.6768e-02, time/batch = 0.1100s	
720/2050 (epoch 17.561), train_loss = 1.72844393, grad/param norm = 1.1867e-01, time/batch = 0.1102s	
721/2050 (epoch 17.585), train_loss = 1.73152225, grad/param norm = 1.4030e-01, time/batch = 0.1102s	
722/2050 (epoch 17.610), train_loss = 1.74892286, grad/param norm = 1.5171e-01, time/batch = 0.1097s	
723/2050 (epoch 17.634), train_loss = 1.74114918, grad/param norm = 1.5239e-01, time/batch = 0.1097s	
724/2050 (epoch 17.659), train_loss = 1.73605157, grad/param norm = 1.4899e-01, time/batch = 0.1095s	
725/2050 (epoch 17.683), train_loss = 1.73514472, grad/param norm = 1.5668e-01, time/batch = 0.1097s	
726/2050 (epoch 17.707), train_loss = 1.72051396, grad/param norm = 1.6949e-01, time/batch = 0.1097s	
727/2050 (epoch 17.732), train_loss = 1.73925125, grad/param norm = 1.6728e-01, time/batch = 0.1098s	
728/2050 (epoch 17.756), train_loss = 1.74590336, grad/param norm = 1.5885e-01, time/batch = 0.1097s	
729/2050 (epoch 17.780), train_loss = 1.73727090, grad/param norm = 1.3150e-01, time/batch = 0.1096s	
730/2050 (epoch 17.805), train_loss = 1.70899901, grad/param norm = 1.1362e-01, time/batch = 0.1097s	
731/2050 (epoch 17.829), train_loss = 1.71059873, grad/param norm = 1.1459e-01, time/batch = 0.1105s	
732/2050 (epoch 17.854), train_loss = 1.70340937, grad/param norm = 1.3206e-01, time/batch = 0.1097s	
733/2050 (epoch 17.878), train_loss = 1.71327991, grad/param norm = 1.4027e-01, time/batch = 0.1098s	
734/2050 (epoch 17.902), train_loss = 1.73152252, grad/param norm = 1.4755e-01, time/batch = 0.1096s	
735/2050 (epoch 17.927), train_loss = 1.70554734, grad/param norm = 1.5406e-01, time/batch = 0.1098s	
736/2050 (epoch 17.951), train_loss = 1.75338152, grad/param norm = 1.4768e-01, time/batch = 0.1098s	
737/2050 (epoch 17.976), train_loss = 1.71939257, grad/param norm = 1.2743e-01, time/batch = 0.1098s	
decayed learning rate by a factor 0.97 to 0.0015204621173091	
738/2050 (epoch 18.000), train_loss = 1.71579852, grad/param norm = 1.1341e-01, time/batch = 0.1098s	
739/2050 (epoch 18.024), train_loss = 1.83814530, grad/param norm = 1.1992e-01, time/batch = 0.1096s	
740/2050 (epoch 18.049), train_loss = 1.72140656, grad/param norm = 1.1985e-01, time/batch = 0.1097s	
741/2050 (epoch 18.073), train_loss = 1.68630967, grad/param norm = 1.1166e-01, time/batch = 0.1103s	
742/2050 (epoch 18.098), train_loss = 1.71070855, grad/param norm = 1.2299e-01, time/batch = 0.1100s	
743/2050 (epoch 18.122), train_loss = 1.71390972, grad/param norm = 1.5769e-01, time/batch = 0.1097s	
744/2050 (epoch 18.146), train_loss = 1.71517622, grad/param norm = 2.0881e-01, time/batch = 0.1096s	
745/2050 (epoch 18.171), train_loss = 1.73143502, grad/param norm = 2.4363e-01, time/batch = 0.1098s	
746/2050 (epoch 18.195), train_loss = 1.73354852, grad/param norm = 2.1984e-01, time/batch = 0.1104s	
747/2050 (epoch 18.220), train_loss = 1.71039364, grad/param norm = 1.5860e-01, time/batch = 0.1098s	
748/2050 (epoch 18.244), train_loss = 1.69265647, grad/param norm = 1.3781e-01, time/batch = 0.1098s	
749/2050 (epoch 18.268), train_loss = 1.71971352, grad/param norm = 1.3496e-01, time/batch = 0.1096s	
750/2050 (epoch 18.293), train_loss = 1.71740847, grad/param norm = 1.3532e-01, time/batch = 0.1098s	
751/2050 (epoch 18.317), train_loss = 1.73958980, grad/param norm = 1.5545e-01, time/batch = 0.1102s	
752/2050 (epoch 18.341), train_loss = 1.72626161, grad/param norm = 1.6057e-01, time/batch = 0.1097s	
753/2050 (epoch 18.366), train_loss = 1.72283972, grad/param norm = 1.5258e-01, time/batch = 0.1097s	
754/2050 (epoch 18.390), train_loss = 1.71877006, grad/param norm = 1.3899e-01, time/batch = 0.1095s	
755/2050 (epoch 18.415), train_loss = 1.70204031, grad/param norm = 1.2422e-01, time/batch = 0.1098s	
756/2050 (epoch 18.439), train_loss = 1.71002000, grad/param norm = 1.1002e-01, time/batch = 0.1098s	
757/2050 (epoch 18.463), train_loss = 1.70322311, grad/param norm = 1.0131e-01, time/batch = 0.1100s	
758/2050 (epoch 18.488), train_loss = 1.71206767, grad/param norm = 1.0450e-01, time/batch = 0.1097s	
759/2050 (epoch 18.512), train_loss = 1.70362378, grad/param norm = 1.2053e-01, time/batch = 0.1096s	
760/2050 (epoch 18.537), train_loss = 1.70718750, grad/param norm = 1.2634e-01, time/batch = 0.1097s	
761/2050 (epoch 18.561), train_loss = 1.70393283, grad/param norm = 1.2830e-01, time/batch = 0.1102s	
762/2050 (epoch 18.585), train_loss = 1.70404950, grad/param norm = 1.4084e-01, time/batch = 0.1097s	
763/2050 (epoch 18.610), train_loss = 1.72265617, grad/param norm = 1.6103e-01, time/batch = 0.1097s	
764/2050 (epoch 18.634), train_loss = 1.71739985, grad/param norm = 1.6972e-01, time/batch = 0.1095s	
765/2050 (epoch 18.659), train_loss = 1.71976311, grad/param norm = 1.7917e-01, time/batch = 0.1099s	
766/2050 (epoch 18.683), train_loss = 1.71945297, grad/param norm = 1.8596e-01, time/batch = 0.1099s	
767/2050 (epoch 18.707), train_loss = 1.69906209, grad/param norm = 1.5898e-01, time/batch = 0.1097s	
768/2050 (epoch 18.732), train_loss = 1.70757370, grad/param norm = 1.1757e-01, time/batch = 0.1098s	
769/2050 (epoch 18.756), train_loss = 1.70994281, grad/param norm = 9.3887e-02, time/batch = 0.1097s	
770/2050 (epoch 18.780), train_loss = 1.70558754, grad/param norm = 8.7505e-02, time/batch = 0.1098s	
771/2050 (epoch 18.805), train_loss = 1.67964309, grad/param norm = 9.1259e-02, time/batch = 0.1102s	
772/2050 (epoch 18.829), train_loss = 1.68284256, grad/param norm = 9.3816e-02, time/batch = 0.1101s	
773/2050 (epoch 18.854), train_loss = 1.67362426, grad/param norm = 9.2717e-02, time/batch = 0.1097s	
774/2050 (epoch 18.878), train_loss = 1.68230108, grad/param norm = 9.6430e-02, time/batch = 0.1096s	
775/2050 (epoch 18.902), train_loss = 1.69925742, grad/param norm = 1.0999e-01, time/batch = 0.1098s	
776/2050 (epoch 18.927), train_loss = 1.67260701, grad/param norm = 1.2910e-01, time/batch = 0.1098s	
777/2050 (epoch 18.951), train_loss = 1.72817043, grad/param norm = 1.5774e-01, time/batch = 0.1097s	
778/2050 (epoch 18.976), train_loss = 1.70552815, grad/param norm = 1.9884e-01, time/batch = 0.1098s	
decayed learning rate by a factor 0.97 to 0.0014748482537899	
779/2050 (epoch 19.000), train_loss = 1.70792703, grad/param norm = 1.8554e-01, time/batch = 0.1096s	
780/2050 (epoch 19.024), train_loss = 1.82585289, grad/param norm = 1.6566e-01, time/batch = 0.1098s	
781/2050 (epoch 19.049), train_loss = 1.70103014, grad/param norm = 1.4543e-01, time/batch = 0.1102s	
782/2050 (epoch 19.073), train_loss = 1.66490886, grad/param norm = 1.2706e-01, time/batch = 0.1097s	
783/2050 (epoch 19.098), train_loss = 1.68888335, grad/param norm = 1.2165e-01, time/batch = 0.1099s	
784/2050 (epoch 19.122), train_loss = 1.68658253, grad/param norm = 1.1826e-01, time/batch = 0.1095s	
785/2050 (epoch 19.146), train_loss = 1.68229891, grad/param norm = 1.3076e-01, time/batch = 0.1097s	
786/2050 (epoch 19.171), train_loss = 1.69412868, grad/param norm = 1.4957e-01, time/batch = 0.1098s	
787/2050 (epoch 19.195), train_loss = 1.69839526, grad/param norm = 1.5494e-01, time/batch = 0.1098s	
788/2050 (epoch 19.220), train_loss = 1.68336838, grad/param norm = 1.4263e-01, time/batch = 0.1098s	
789/2050 (epoch 19.244), train_loss = 1.67055837, grad/param norm = 1.5225e-01, time/batch = 0.1096s	
790/2050 (epoch 19.268), train_loss = 1.69747104, grad/param norm = 1.4788e-01, time/batch = 0.1097s	
791/2050 (epoch 19.293), train_loss = 1.69560394, grad/param norm = 1.4718e-01, time/batch = 0.1103s	
792/2050 (epoch 19.317), train_loss = 1.71788982, grad/param norm = 1.6380e-01, time/batch = 0.1097s	
793/2050 (epoch 19.341), train_loss = 1.70084429, grad/param norm = 1.5267e-01, time/batch = 0.1098s	
794/2050 (epoch 19.366), train_loss = 1.69534528, grad/param norm = 1.2638e-01, time/batch = 0.1096s	
795/2050 (epoch 19.390), train_loss = 1.69072674, grad/param norm = 1.1984e-01, time/batch = 0.1097s	
796/2050 (epoch 19.415), train_loss = 1.67887438, grad/param norm = 1.1948e-01, time/batch = 0.1098s	
797/2050 (epoch 19.439), train_loss = 1.69085140, grad/param norm = 1.3223e-01, time/batch = 0.1098s	
798/2050 (epoch 19.463), train_loss = 1.69278465, grad/param norm = 1.4448e-01, time/batch = 0.1102s	
799/2050 (epoch 19.488), train_loss = 1.69659000, grad/param norm = 1.4016e-01, time/batch = 0.1097s	
800/2050 (epoch 19.512), train_loss = 1.68652827, grad/param norm = 1.1999e-01, time/batch = 0.1097s	
801/2050 (epoch 19.537), train_loss = 1.68332394, grad/param norm = 1.0466e-01, time/batch = 0.1102s	
802/2050 (epoch 19.561), train_loss = 1.68340169, grad/param norm = 1.1609e-01, time/batch = 0.1097s	
803/2050 (epoch 19.585), train_loss = 1.68070435, grad/param norm = 1.2419e-01, time/batch = 0.1097s	
804/2050 (epoch 19.610), train_loss = 1.69297693, grad/param norm = 1.1265e-01, time/batch = 0.1096s	
805/2050 (epoch 19.634), train_loss = 1.68413831, grad/param norm = 1.0523e-01, time/batch = 0.1097s	
806/2050 (epoch 19.659), train_loss = 1.68402764, grad/param norm = 1.1350e-01, time/batch = 0.1098s	
807/2050 (epoch 19.683), train_loss = 1.68560950, grad/param norm = 1.3536e-01, time/batch = 0.1097s	
808/2050 (epoch 19.707), train_loss = 1.67259503, grad/param norm = 1.3980e-01, time/batch = 0.1098s	
809/2050 (epoch 19.732), train_loss = 1.68694109, grad/param norm = 1.2881e-01, time/batch = 0.1096s	
810/2050 (epoch 19.756), train_loss = 1.69357461, grad/param norm = 1.2241e-01, time/batch = 0.1098s	
811/2050 (epoch 19.780), train_loss = 1.69250633, grad/param norm = 1.5421e-01, time/batch = 0.1102s	
812/2050 (epoch 19.805), train_loss = 1.67600023, grad/param norm = 1.7678e-01, time/batch = 0.1097s	
813/2050 (epoch 19.829), train_loss = 1.67761414, grad/param norm = 1.8509e-01, time/batch = 0.1099s	
814/2050 (epoch 19.854), train_loss = 1.67331482, grad/param norm = 1.7867e-01, time/batch = 0.1095s	
815/2050 (epoch 19.878), train_loss = 1.67446401, grad/param norm = 1.5549e-01, time/batch = 0.1098s	
816/2050 (epoch 19.902), train_loss = 1.68690497, grad/param norm = 1.4275e-01, time/batch = 0.1098s	
817/2050 (epoch 19.927), train_loss = 1.65291261, grad/param norm = 1.2446e-01, time/batch = 0.1098s	
818/2050 (epoch 19.951), train_loss = 1.70012738, grad/param norm = 1.1910e-01, time/batch = 0.1098s	
819/2050 (epoch 19.976), train_loss = 1.66812123, grad/param norm = 1.1858e-01, time/batch = 0.1097s	
decayed learning rate by a factor 0.97 to 0.0014306028061762	
820/2050 (epoch 20.000), train_loss = 1.66964073, grad/param norm = 1.0585e-01, time/batch = 0.1098s	
821/2050 (epoch 20.024), train_loss = 1.79616885, grad/param norm = 1.0446e-01, time/batch = 0.1103s	
822/2050 (epoch 20.049), train_loss = 1.67325442, grad/param norm = 1.0492e-01, time/batch = 0.1096s	
823/2050 (epoch 20.073), train_loss = 1.63895791, grad/param norm = 9.8225e-02, time/batch = 0.1098s	
824/2050 (epoch 20.098), train_loss = 1.66378659, grad/param norm = 9.7012e-02, time/batch = 0.1100s	
825/2050 (epoch 20.122), train_loss = 1.66168114, grad/param norm = 9.4109e-02, time/batch = 0.1098s	
826/2050 (epoch 20.146), train_loss = 1.65555638, grad/param norm = 1.0669e-01, time/batch = 0.1098s	
827/2050 (epoch 20.171), train_loss = 1.66843456, grad/param norm = 1.2920e-01, time/batch = 0.1097s	
828/2050 (epoch 20.195), train_loss = 1.67257298, grad/param norm = 1.4785e-01, time/batch = 0.1100s	
829/2050 (epoch 20.220), train_loss = 1.66247094, grad/param norm = 1.4915e-01, time/batch = 0.1096s	
830/2050 (epoch 20.244), train_loss = 1.65032922, grad/param norm = 1.6030e-01, time/batch = 0.1098s	
831/2050 (epoch 20.268), train_loss = 1.67962725, grad/param norm = 1.6397e-01, time/batch = 0.1103s	
832/2050 (epoch 20.293), train_loss = 1.67823694, grad/param norm = 1.6122e-01, time/batch = 0.1097s	
833/2050 (epoch 20.317), train_loss = 1.69728852, grad/param norm = 1.6106e-01, time/batch = 0.1098s	
834/2050 (epoch 20.341), train_loss = 1.67659974, grad/param norm = 1.4028e-01, time/batch = 0.1095s	
835/2050 (epoch 20.366), train_loss = 1.67067962, grad/param norm = 1.1034e-01, time/batch = 0.1097s	
836/2050 (epoch 20.390), train_loss = 1.66565085, grad/param norm = 1.0508e-01, time/batch = 0.1098s	
837/2050 (epoch 20.415), train_loss = 1.65555607, grad/param norm = 1.0647e-01, time/batch = 0.1097s	
838/2050 (epoch 20.439), train_loss = 1.66575422, grad/param norm = 1.0297e-01, time/batch = 0.1099s	
839/2050 (epoch 20.463), train_loss = 1.66405046, grad/param norm = 1.0394e-01, time/batch = 0.1102s	
840/2050 (epoch 20.488), train_loss = 1.67120733, grad/param norm = 1.1398e-01, time/batch = 0.1097s	
841/2050 (epoch 20.512), train_loss = 1.66495250, grad/param norm = 1.1963e-01, time/batch = 0.1103s	
842/2050 (epoch 20.537), train_loss = 1.66456821, grad/param norm = 1.1575e-01, time/batch = 0.1104s	
843/2050 (epoch 20.561), train_loss = 1.66472323, grad/param norm = 1.2099e-01, time/batch = 0.1098s	
844/2050 (epoch 20.585), train_loss = 1.66156123, grad/param norm = 1.3305e-01, time/batch = 0.1096s	
845/2050 (epoch 20.610), train_loss = 1.68025342, grad/param norm = 1.3880e-01, time/batch = 0.1098s	
846/2050 (epoch 20.634), train_loss = 1.67248538, grad/param norm = 1.4275e-01, time/batch = 0.1098s	
847/2050 (epoch 20.659), train_loss = 1.67279699, grad/param norm = 1.3765e-01, time/batch = 0.1098s	
848/2050 (epoch 20.683), train_loss = 1.66526521, grad/param norm = 1.2245e-01, time/batch = 0.1098s	
849/2050 (epoch 20.707), train_loss = 1.64599992, grad/param norm = 1.0307e-01, time/batch = 0.1096s	
850/2050 (epoch 20.732), train_loss = 1.66014157, grad/param norm = 9.0958e-02, time/batch = 0.1098s	
851/2050 (epoch 20.756), train_loss = 1.66692426, grad/param norm = 8.6440e-02, time/batch = 0.1103s	
852/2050 (epoch 20.780), train_loss = 1.66451570, grad/param norm = 9.4264e-02, time/batch = 0.1097s	
853/2050 (epoch 20.805), train_loss = 1.64128643, grad/param norm = 1.1379e-01, time/batch = 0.1098s	
854/2050 (epoch 20.829), train_loss = 1.64699326, grad/param norm = 1.4415e-01, time/batch = 0.1100s	
855/2050 (epoch 20.854), train_loss = 1.64583618, grad/param norm = 1.5588e-01, time/batch = 0.1097s	
856/2050 (epoch 20.878), train_loss = 1.65442961, grad/param norm = 1.6919e-01, time/batch = 0.1098s	
857/2050 (epoch 20.902), train_loss = 1.67580060, grad/param norm = 1.6974e-01, time/batch = 0.1098s	
858/2050 (epoch 20.927), train_loss = 1.63794757, grad/param norm = 1.4881e-01, time/batch = 0.1098s	
859/2050 (epoch 20.951), train_loss = 1.68744113, grad/param norm = 1.4034e-01, time/batch = 0.1096s	
860/2050 (epoch 20.976), train_loss = 1.65137317, grad/param norm = 1.4127e-01, time/batch = 0.1097s	
decayed learning rate by a factor 0.97 to 0.0013876847219909	
861/2050 (epoch 21.000), train_loss = 1.65819221, grad/param norm = 1.4191e-01, time/batch = 0.1102s	
862/2050 (epoch 21.024), train_loss = 1.78789925, grad/param norm = 1.3814e-01, time/batch = 0.1096s	
863/2050 (epoch 21.049), train_loss = 1.65730153, grad/param norm = 1.1624e-01, time/batch = 0.1097s	
864/2050 (epoch 21.073), train_loss = 1.61880914, grad/param norm = 8.9209e-02, time/batch = 0.1095s	
865/2050 (epoch 21.098), train_loss = 1.64146372, grad/param norm = 7.9390e-02, time/batch = 0.1102s	
866/2050 (epoch 21.122), train_loss = 1.63774842, grad/param norm = 6.5071e-02, time/batch = 0.1099s	
867/2050 (epoch 21.146), train_loss = 1.62983596, grad/param norm = 6.4469e-02, time/batch = 0.1098s	
868/2050 (epoch 21.171), train_loss = 1.64208362, grad/param norm = 7.5796e-02, time/batch = 0.1098s	
869/2050 (epoch 21.195), train_loss = 1.64406013, grad/param norm = 9.0325e-02, time/batch = 0.1101s	
870/2050 (epoch 21.220), train_loss = 1.63777206, grad/param norm = 9.9443e-02, time/batch = 0.1097s	
871/2050 (epoch 21.244), train_loss = 1.62463732, grad/param norm = 1.1281e-01, time/batch = 0.1102s	
872/2050 (epoch 21.268), train_loss = 1.65763741, grad/param norm = 1.3034e-01, time/batch = 0.1097s	
873/2050 (epoch 21.293), train_loss = 1.65652709, grad/param norm = 1.3565e-01, time/batch = 0.1097s	
874/2050 (epoch 21.317), train_loss = 1.67685996, grad/param norm = 1.3844e-01, time/batch = 0.1096s	
875/2050 (epoch 21.341), train_loss = 1.66006753, grad/param norm = 1.5342e-01, time/batch = 0.1100s	
876/2050 (epoch 21.366), train_loss = 1.66517723, grad/param norm = 1.6846e-01, time/batch = 0.1098s	
877/2050 (epoch 21.390), train_loss = 1.65962815, grad/param norm = 1.5789e-01, time/batch = 0.1098s	
878/2050 (epoch 21.415), train_loss = 1.64370281, grad/param norm = 1.3177e-01, time/batch = 0.1098s	
879/2050 (epoch 21.439), train_loss = 1.65287457, grad/param norm = 1.2064e-01, time/batch = 0.1096s	
880/2050 (epoch 21.463), train_loss = 1.64595397, grad/param norm = 1.1182e-01, time/batch = 0.1102s	
881/2050 (epoch 21.488), train_loss = 1.65233035, grad/param norm = 1.0999e-01, time/batch = 0.1102s	
882/2050 (epoch 21.512), train_loss = 1.64118005, grad/param norm = 1.2347e-01, time/batch = 0.1097s	
883/2050 (epoch 21.537), train_loss = 1.64881667, grad/param norm = 1.4035e-01, time/batch = 0.1098s	
884/2050 (epoch 21.561), train_loss = 1.64774051, grad/param norm = 1.6325e-01, time/batch = 0.1096s	
885/2050 (epoch 21.585), train_loss = 1.65097168, grad/param norm = 1.7332e-01, time/batch = 0.1098s	
886/2050 (epoch 21.610), train_loss = 1.66333029, grad/param norm = 1.7566e-01, time/batch = 0.1098s	
887/2050 (epoch 21.634), train_loss = 1.65471678, grad/param norm = 1.5302e-01, time/batch = 0.1098s	
888/2050 (epoch 21.659), train_loss = 1.64694158, grad/param norm = 1.2810e-01, time/batch = 0.1098s	
889/2050 (epoch 21.683), train_loss = 1.64427866, grad/param norm = 1.1809e-01, time/batch = 0.1096s	
890/2050 (epoch 21.707), train_loss = 1.62606568, grad/param norm = 1.0938e-01, time/batch = 0.1097s	
891/2050 (epoch 21.732), train_loss = 1.64392850, grad/param norm = 1.0120e-01, time/batch = 0.1107s	
892/2050 (epoch 21.756), train_loss = 1.64999873, grad/param norm = 1.0047e-01, time/batch = 0.1097s	
893/2050 (epoch 21.780), train_loss = 1.64826376, grad/param norm = 1.0052e-01, time/batch = 0.1097s	
894/2050 (epoch 21.805), train_loss = 1.62260539, grad/param norm = 1.0072e-01, time/batch = 0.1095s	
895/2050 (epoch 21.829), train_loss = 1.62654445, grad/param norm = 1.0385e-01, time/batch = 0.1103s	
896/2050 (epoch 21.854), train_loss = 1.62135870, grad/param norm = 1.0575e-01, time/batch = 0.1099s	
897/2050 (epoch 21.878), train_loss = 1.62766067, grad/param norm = 9.8463e-02, time/batch = 0.1097s	
898/2050 (epoch 21.902), train_loss = 1.64186532, grad/param norm = 9.3451e-02, time/batch = 0.1098s	
899/2050 (epoch 21.927), train_loss = 1.61182159, grad/param norm = 9.4964e-02, time/batch = 0.1096s	
900/2050 (epoch 21.951), train_loss = 1.66297712, grad/param norm = 1.0227e-01, time/batch = 0.1097s	
901/2050 (epoch 21.976), train_loss = 1.62798346, grad/param norm = 1.1058e-01, time/batch = 0.1102s	
decayed learning rate by a factor 0.97 to 0.0013460541803311	
902/2050 (epoch 22.000), train_loss = 1.63989771, grad/param norm = 1.2276e-01, time/batch = 0.1097s	
903/2050 (epoch 22.024), train_loss = 1.77225674, grad/param norm = 1.2143e-01, time/batch = 0.1097s	
904/2050 (epoch 22.049), train_loss = 1.63798977, grad/param norm = 9.8742e-02, time/batch = 0.1095s	
905/2050 (epoch 22.073), train_loss = 1.59859558, grad/param norm = 8.7867e-02, time/batch = 0.1098s	
906/2050 (epoch 22.098), train_loss = 1.62646818, grad/param norm = 1.0449e-01, time/batch = 0.1101s	
907/2050 (epoch 22.122), train_loss = 1.62634180, grad/param norm = 1.0596e-01, time/batch = 0.1097s	
908/2050 (epoch 22.146), train_loss = 1.61952880, grad/param norm = 1.0822e-01, time/batch = 0.1098s	
909/2050 (epoch 22.171), train_loss = 1.63139561, grad/param norm = 1.2367e-01, time/batch = 0.1096s	
910/2050 (epoch 22.195), train_loss = 1.63473867, grad/param norm = 1.4145e-01, time/batch = 0.1100s	
911/2050 (epoch 22.220), train_loss = 1.62738601, grad/param norm = 1.4701e-01, time/batch = 0.1103s	
912/2050 (epoch 22.244), train_loss = 1.61280938, grad/param norm = 1.5027e-01, time/batch = 0.1096s	
913/2050 (epoch 22.268), train_loss = 1.64103805, grad/param norm = 1.5317e-01, time/batch = 0.1098s	
914/2050 (epoch 22.293), train_loss = 1.64110702, grad/param norm = 1.4981e-01, time/batch = 0.1096s	
915/2050 (epoch 22.317), train_loss = 1.65835874, grad/param norm = 1.4634e-01, time/batch = 0.1098s	
916/2050 (epoch 22.341), train_loss = 1.63820470, grad/param norm = 1.2637e-01, time/batch = 0.1097s	
917/2050 (epoch 22.366), train_loss = 1.63435869, grad/param norm = 1.0894e-01, time/batch = 0.1099s	
918/2050 (epoch 22.390), train_loss = 1.63307396, grad/param norm = 1.1060e-01, time/batch = 0.1097s	
919/2050 (epoch 22.415), train_loss = 1.62296928, grad/param norm = 1.2739e-01, time/batch = 0.1097s	
920/2050 (epoch 22.439), train_loss = 1.63780752, grad/param norm = 1.2764e-01, time/batch = 0.1097s	
921/2050 (epoch 22.463), train_loss = 1.63070754, grad/param norm = 1.1722e-01, time/batch = 0.1107s	
922/2050 (epoch 22.488), train_loss = 1.63777272, grad/param norm = 1.1997e-01, time/batch = 0.1097s	
923/2050 (epoch 22.512), train_loss = 1.62759058, grad/param norm = 1.3029e-01, time/batch = 0.1097s	
924/2050 (epoch 22.537), train_loss = 1.63014974, grad/param norm = 1.2032e-01, time/batch = 0.1095s	
925/2050 (epoch 22.561), train_loss = 1.62415232, grad/param norm = 1.0468e-01, time/batch = 0.1098s	
926/2050 (epoch 22.585), train_loss = 1.62146850, grad/param norm = 1.0591e-01, time/batch = 0.1098s	
927/2050 (epoch 22.610), train_loss = 1.63795265, grad/param norm = 1.0988e-01, time/batch = 0.1099s	
928/2050 (epoch 22.634), train_loss = 1.63077783, grad/param norm = 1.1250e-01, time/batch = 0.1098s	
929/2050 (epoch 22.659), train_loss = 1.63020748, grad/param norm = 1.2020e-01, time/batch = 0.1097s	
930/2050 (epoch 22.683), train_loss = 1.63018214, grad/param norm = 1.2236e-01, time/batch = 0.1097s	
931/2050 (epoch 22.707), train_loss = 1.61188275, grad/param norm = 1.0788e-01, time/batch = 0.1102s	
932/2050 (epoch 22.732), train_loss = 1.62709491, grad/param norm = 8.7134e-02, time/batch = 0.1101s	
933/2050 (epoch 22.756), train_loss = 1.63163168, grad/param norm = 7.9554e-02, time/batch = 0.1098s	
934/2050 (epoch 22.780), train_loss = 1.63099938, grad/param norm = 8.8230e-02, time/batch = 0.1096s	
935/2050 (epoch 22.805), train_loss = 1.60510098, grad/param norm = 9.7510e-02, time/batch = 0.1098s	
936/2050 (epoch 22.829), train_loss = 1.60926584, grad/param norm = 1.0104e-01, time/batch = 0.1099s	
937/2050 (epoch 22.854), train_loss = 1.60396671, grad/param norm = 1.0596e-01, time/batch = 0.1098s	
938/2050 (epoch 22.878), train_loss = 1.61415341, grad/param norm = 1.0766e-01, time/batch = 0.1098s	
939/2050 (epoch 22.902), train_loss = 1.62809578, grad/param norm = 1.0552e-01, time/batch = 0.1096s	
940/2050 (epoch 22.927), train_loss = 1.59761304, grad/param norm = 1.0709e-01, time/batch = 0.1098s	
941/2050 (epoch 22.951), train_loss = 1.64739505, grad/param norm = 1.2929e-01, time/batch = 0.1102s	
942/2050 (epoch 22.976), train_loss = 1.61904229, grad/param norm = 1.4190e-01, time/batch = 0.1097s	
decayed learning rate by a factor 0.97 to 0.0013056725549212	
943/2050 (epoch 23.000), train_loss = 1.62445258, grad/param norm = 1.3140e-01, time/batch = 0.1098s	
944/2050 (epoch 23.024), train_loss = 1.75860688, grad/param norm = 1.3684e-01, time/batch = 0.1095s	
945/2050 (epoch 23.049), train_loss = 1.62744260, grad/param norm = 1.3342e-01, time/batch = 0.1097s	
946/2050 (epoch 23.073), train_loss = 1.59007495, grad/param norm = 1.3222e-01, time/batch = 0.1097s	
947/2050 (epoch 23.098), train_loss = 1.61848858, grad/param norm = 1.4006e-01, time/batch = 0.1102s	
948/2050 (epoch 23.122), train_loss = 1.61311910, grad/param norm = 1.5355e-01, time/batch = 0.1098s	
949/2050 (epoch 23.146), train_loss = 1.61227583, grad/param norm = 1.6406e-01, time/batch = 0.1096s	
950/2050 (epoch 23.171), train_loss = 1.62006210, grad/param norm = 1.5879e-01, time/batch = 0.1098s	
951/2050 (epoch 23.195), train_loss = 1.61713030, grad/param norm = 1.3291e-01, time/batch = 0.1103s	
952/2050 (epoch 23.220), train_loss = 1.60215611, grad/param norm = 8.8634e-02, time/batch = 0.1097s	
953/2050 (epoch 23.244), train_loss = 1.58402621, grad/param norm = 7.7226e-02, time/batch = 0.1097s	
954/2050 (epoch 23.268), train_loss = 1.61540888, grad/param norm = 8.1570e-02, time/batch = 0.1096s	
955/2050 (epoch 23.293), train_loss = 1.61540649, grad/param norm = 8.6098e-02, time/batch = 0.1097s	
956/2050 (epoch 23.317), train_loss = 1.63448889, grad/param norm = 8.5517e-02, time/batch = 0.1098s	
957/2050 (epoch 23.341), train_loss = 1.61614880, grad/param norm = 8.3875e-02, time/batch = 0.1098s	
958/2050 (epoch 23.366), train_loss = 1.61685354, grad/param norm = 8.2290e-02, time/batch = 0.1097s	
959/2050 (epoch 23.390), train_loss = 1.61354331, grad/param norm = 8.7139e-02, time/batch = 0.1096s	
960/2050 (epoch 23.415), train_loss = 1.60147059, grad/param norm = 8.8439e-02, time/batch = 0.1098s	
961/2050 (epoch 23.439), train_loss = 1.61445718, grad/param norm = 9.1351e-02, time/batch = 0.1103s	
962/2050 (epoch 23.463), train_loss = 1.61163692, grad/param norm = 1.0273e-01, time/batch = 0.1098s	
963/2050 (epoch 23.488), train_loss = 1.61732539, grad/param norm = 1.0711e-01, time/batch = 0.1097s	
964/2050 (epoch 23.512), train_loss = 1.60686567, grad/param norm = 1.0262e-01, time/batch = 0.1095s	
965/2050 (epoch 23.537), train_loss = 1.60990616, grad/param norm = 1.0037e-01, time/batch = 0.1098s	
966/2050 (epoch 23.561), train_loss = 1.60868239, grad/param norm = 1.0761e-01, time/batch = 0.1098s	
967/2050 (epoch 23.585), train_loss = 1.60905305, grad/param norm = 1.2382e-01, time/batch = 0.1098s	
968/2050 (epoch 23.610), train_loss = 1.62566095, grad/param norm = 1.3206e-01, time/batch = 0.1098s	
969/2050 (epoch 23.634), train_loss = 1.62215961, grad/param norm = 1.4613e-01, time/batch = 0.1097s	
970/2050 (epoch 23.659), train_loss = 1.62219591, grad/param norm = 1.3732e-01, time/batch = 0.1107s	
971/2050 (epoch 23.683), train_loss = 1.61599287, grad/param norm = 1.1663e-01, time/batch = 0.1103s	
972/2050 (epoch 23.707), train_loss = 1.59561629, grad/param norm = 9.8776e-02, time/batch = 0.1097s	
973/2050 (epoch 23.732), train_loss = 1.61026264, grad/param norm = 9.2733e-02, time/batch = 0.1102s	
974/2050 (epoch 23.756), train_loss = 1.61756525, grad/param norm = 9.3630e-02, time/batch = 0.1096s	
975/2050 (epoch 23.780), train_loss = 1.61834545, grad/param norm = 1.0683e-01, time/batch = 0.1098s	
976/2050 (epoch 23.805), train_loss = 1.59051151, grad/param norm = 1.0074e-01, time/batch = 0.1098s	
977/2050 (epoch 23.829), train_loss = 1.59119490, grad/param norm = 8.9251e-02, time/batch = 0.1101s	
978/2050 (epoch 23.854), train_loss = 1.58437231, grad/param norm = 8.9770e-02, time/batch = 0.1098s	
979/2050 (epoch 23.878), train_loss = 1.59632976, grad/param norm = 9.9314e-02, time/batch = 0.1097s	
980/2050 (epoch 23.902), train_loss = 1.61215532, grad/param norm = 1.1508e-01, time/batch = 0.1098s	
981/2050 (epoch 23.927), train_loss = 1.58721629, grad/param norm = 1.3972e-01, time/batch = 0.1102s	
982/2050 (epoch 23.951), train_loss = 1.63918101, grad/param norm = 1.6985e-01, time/batch = 0.1097s	
983/2050 (epoch 23.976), train_loss = 1.61046106, grad/param norm = 1.8066e-01, time/batch = 0.1098s	
decayed learning rate by a factor 0.97 to 0.0012665023782736	
984/2050 (epoch 24.000), train_loss = 1.61318309, grad/param norm = 1.4796e-01, time/batch = 0.1095s	
985/2050 (epoch 24.024), train_loss = 1.74213200, grad/param norm = 1.2086e-01, time/batch = 0.1097s	
986/2050 (epoch 24.049), train_loss = 1.60431290, grad/param norm = 9.9512e-02, time/batch = 0.1097s	
987/2050 (epoch 24.073), train_loss = 1.56628668, grad/param norm = 8.3248e-02, time/batch = 0.1098s	
988/2050 (epoch 24.098), train_loss = 1.59232495, grad/param norm = 8.3154e-02, time/batch = 0.1102s	
989/2050 (epoch 24.122), train_loss = 1.58918827, grad/param norm = 8.4559e-02, time/batch = 0.1097s	
990/2050 (epoch 24.146), train_loss = 1.58667133, grad/param norm = 9.8749e-02, time/batch = 0.1097s	
991/2050 (epoch 24.171), train_loss = 1.59839048, grad/param norm = 1.1547e-01, time/batch = 0.1103s	
992/2050 (epoch 24.195), train_loss = 1.59917689, grad/param norm = 1.1720e-01, time/batch = 0.1098s	
993/2050 (epoch 24.220), train_loss = 1.58932059, grad/param norm = 9.9859e-02, time/batch = 0.1097s	
994/2050 (epoch 24.244), train_loss = 1.57188510, grad/param norm = 9.6501e-02, time/batch = 0.1096s	
995/2050 (epoch 24.268), train_loss = 1.60399935, grad/param norm = 1.0224e-01, time/batch = 0.1098s	
996/2050 (epoch 24.293), train_loss = 1.60547403, grad/param norm = 1.1064e-01, time/batch = 0.1099s	
997/2050 (epoch 24.317), train_loss = 1.62580863, grad/param norm = 1.2421e-01, time/batch = 0.1098s	
998/2050 (epoch 24.341), train_loss = 1.60816196, grad/param norm = 1.2109e-01, time/batch = 0.1097s	
999/2050 (epoch 24.366), train_loss = 1.60600308, grad/param norm = 1.0938e-01, time/batch = 0.1096s	
evaluating loss over split index 2	
1/3...	
2/3...	
3/3...	
saving checkpoint to cv/lm_lstm_epoch24.39_1.6528.t7	
1000/2050 (epoch 24.390), train_loss = 1.60371543, grad/param norm = 1.0451e-01, time/batch = 0.1097s	
1001/2050 (epoch 24.415), train_loss = 1.73525440, grad/param norm = 1.0062e-01, time/batch = 0.1106s	
1002/2050 (epoch 24.439), train_loss = 1.60237192, grad/param norm = 8.3308e-02, time/batch = 0.1097s	
1003/2050 (epoch 24.463), train_loss = 1.59424346, grad/param norm = 8.6824e-02, time/batch = 0.1098s	
1004/2050 (epoch 24.488), train_loss = 1.60215137, grad/param norm = 1.0069e-01, time/batch = 0.1098s	
1005/2050 (epoch 24.512), train_loss = 1.59297128, grad/param norm = 1.1594e-01, time/batch = 0.1100s	
1006/2050 (epoch 24.537), train_loss = 1.60057907, grad/param norm = 1.1893e-01, time/batch = 0.1097s	
1007/2050 (epoch 24.561), train_loss = 1.59615126, grad/param norm = 1.1594e-01, time/batch = 0.1098s	
1008/2050 (epoch 24.585), train_loss = 1.59437156, grad/param norm = 1.1264e-01, time/batch = 0.1099s	
1009/2050 (epoch 24.610), train_loss = 1.60697752, grad/param norm = 1.0626e-01, time/batch = 0.1102s	
1010/2050 (epoch 24.634), train_loss = 1.59810888, grad/param norm = 1.0472e-01, time/batch = 0.1099s	
1011/2050 (epoch 24.659), train_loss = 1.59775068, grad/param norm = 1.1252e-01, time/batch = 0.1103s	
1012/2050 (epoch 24.683), train_loss = 1.60193985, grad/param norm = 1.2919e-01, time/batch = 0.1097s	
1013/2050 (epoch 24.707), train_loss = 1.58507385, grad/param norm = 1.2443e-01, time/batch = 0.1097s	
1014/2050 (epoch 24.732), train_loss = 1.59944100, grad/param norm = 1.1034e-01, time/batch = 0.1096s	
1015/2050 (epoch 24.756), train_loss = 1.60405798, grad/param norm = 1.0504e-01, time/batch = 0.1098s	
1016/2050 (epoch 24.780), train_loss = 1.60608702, grad/param norm = 1.1494e-01, time/batch = 0.1098s	
1017/2050 (epoch 24.805), train_loss = 1.57932645, grad/param norm = 1.2807e-01, time/batch = 0.1098s	
1018/2050 (epoch 24.829), train_loss = 1.58505896, grad/param norm = 1.2817e-01, time/batch = 0.1097s	
1019/2050 (epoch 24.854), train_loss = 1.57586974, grad/param norm = 1.2813e-01, time/batch = 0.1096s	
1020/2050 (epoch 24.878), train_loss = 1.58945089, grad/param norm = 1.2345e-01, time/batch = 0.1101s	
1021/2050 (epoch 24.902), train_loss = 1.59716004, grad/param norm = 1.0555e-01, time/batch = 0.1102s	
1022/2050 (epoch 24.927), train_loss = 1.57007198, grad/param norm = 1.0434e-01, time/batch = 0.1097s	
1023/2050 (epoch 24.951), train_loss = 1.61416977, grad/param norm = 1.0825e-01, time/batch = 0.1097s	
1024/2050 (epoch 24.976), train_loss = 1.58201084, grad/param norm = 1.0625e-01, time/batch = 0.1096s	
decayed learning rate by a factor 0.97 to 0.0012285073069254	
1025/2050 (epoch 25.000), train_loss = 1.58931909, grad/param norm = 1.0696e-01, time/batch = 0.1098s	
1026/2050 (epoch 25.024), train_loss = 1.72852743, grad/param norm = 1.1031e-01, time/batch = 0.1098s	
1027/2050 (epoch 25.049), train_loss = 1.58931811, grad/param norm = 1.0522e-01, time/batch = 0.1097s	
1028/2050 (epoch 25.073), train_loss = 1.55407960, grad/param norm = 9.3063e-02, time/batch = 0.1098s	
1029/2050 (epoch 25.098), train_loss = 1.57894519, grad/param norm = 8.8562e-02, time/batch = 0.1096s	
1030/2050 (epoch 25.122), train_loss = 1.57748265, grad/param norm = 8.6807e-02, time/batch = 0.1097s	
1031/2050 (epoch 25.146), train_loss = 1.57446388, grad/param norm = 9.9964e-02, time/batch = 0.1107s	
1032/2050 (epoch 25.171), train_loss = 1.58495586, grad/param norm = 1.0772e-01, time/batch = 0.1097s	
1033/2050 (epoch 25.195), train_loss = 1.58369472, grad/param norm = 1.0296e-01, time/batch = 0.1097s	
1034/2050 (epoch 25.220), train_loss = 1.57493692, grad/param norm = 8.8845e-02, time/batch = 0.1095s	
1035/2050 (epoch 25.244), train_loss = 1.55727857, grad/param norm = 8.4577e-02, time/batch = 0.1099s	
1036/2050 (epoch 25.268), train_loss = 1.58839214, grad/param norm = 8.9191e-02, time/batch = 0.1097s	
1037/2050 (epoch 25.293), train_loss = 1.58992683, grad/param norm = 9.9144e-02, time/batch = 0.1097s	
1038/2050 (epoch 25.317), train_loss = 1.61055701, grad/param norm = 1.1568e-01, time/batch = 0.1098s	
1039/2050 (epoch 25.341), train_loss = 1.59304092, grad/param norm = 1.1554e-01, time/batch = 0.1097s	
1040/2050 (epoch 25.366), train_loss = 1.59195907, grad/param norm = 1.1227e-01, time/batch = 0.1098s	
1041/2050 (epoch 25.390), train_loss = 1.59182792, grad/param norm = 1.1867e-01, time/batch = 0.1103s	
1042/2050 (epoch 25.415), train_loss = 1.58012879, grad/param norm = 1.3232e-01, time/batch = 0.1096s	
1043/2050 (epoch 25.439), train_loss = 1.59478189, grad/param norm = 1.2122e-01, time/batch = 0.1106s	
1044/2050 (epoch 25.463), train_loss = 1.58480725, grad/param norm = 1.0178e-01, time/batch = 0.1096s	
1045/2050 (epoch 25.488), train_loss = 1.58998288, grad/param norm = 9.5920e-02, time/batch = 0.1097s	
1046/2050 (epoch 25.512), train_loss = 1.57772410, grad/param norm = 1.0266e-01, time/batch = 0.1102s	
1047/2050 (epoch 25.537), train_loss = 1.58374605, grad/param norm = 1.0817e-01, time/batch = 0.1098s	
1048/2050 (epoch 25.561), train_loss = 1.58103637, grad/param norm = 1.1456e-01, time/batch = 0.1097s	
1049/2050 (epoch 25.585), train_loss = 1.58026565, grad/param norm = 1.1710e-01, time/batch = 0.1096s	
1050/2050 (epoch 25.610), train_loss = 1.59477667, grad/param norm = 1.1286e-01, time/batch = 0.1100s	
1051/2050 (epoch 25.634), train_loss = 1.58430850, grad/param norm = 1.0608e-01, time/batch = 0.1103s	
1052/2050 (epoch 25.659), train_loss = 1.58242545, grad/param norm = 1.0062e-01, time/batch = 0.1099s	
1053/2050 (epoch 25.683), train_loss = 1.58295306, grad/param norm = 1.0373e-01, time/batch = 0.1104s	
1054/2050 (epoch 25.707), train_loss = 1.56680481, grad/param norm = 1.0745e-01, time/batch = 0.1095s	
1055/2050 (epoch 25.732), train_loss = 1.58382088, grad/param norm = 1.0055e-01, time/batch = 0.1097s	
1056/2050 (epoch 25.756), train_loss = 1.58916983, grad/param norm = 9.7530e-02, time/batch = 0.1098s	
1057/2050 (epoch 25.780), train_loss = 1.59209116, grad/param norm = 1.0494e-01, time/batch = 0.1097s	
1058/2050 (epoch 25.805), train_loss = 1.56415812, grad/param norm = 1.1108e-01, time/batch = 0.1097s	
1059/2050 (epoch 25.829), train_loss = 1.56692626, grad/param norm = 1.0437e-01, time/batch = 0.1097s	
1060/2050 (epoch 25.854), train_loss = 1.55832154, grad/param norm = 9.8387e-02, time/batch = 0.1097s	
1061/2050 (epoch 25.878), train_loss = 1.57116540, grad/param norm = 9.8532e-02, time/batch = 0.1108s	
1062/2050 (epoch 25.902), train_loss = 1.58193736, grad/param norm = 8.6127e-02, time/batch = 0.1097s	
1063/2050 (epoch 25.927), train_loss = 1.55284143, grad/param norm = 7.8964e-02, time/batch = 0.1097s	
1064/2050 (epoch 25.951), train_loss = 1.59751727, grad/param norm = 8.6254e-02, time/batch = 0.1096s	
1065/2050 (epoch 25.976), train_loss = 1.56693243, grad/param norm = 9.9111e-02, time/batch = 0.1097s	
decayed learning rate by a factor 0.97 to 0.0011916520877176	
1066/2050 (epoch 26.000), train_loss = 1.57906764, grad/param norm = 1.1134e-01, time/batch = 0.1098s	
1067/2050 (epoch 26.024), train_loss = 1.72019030, grad/param norm = 1.1963e-01, time/batch = 0.1097s	
1068/2050 (epoch 26.049), train_loss = 1.58018610, grad/param norm = 1.1912e-01, time/batch = 0.1098s	
1069/2050 (epoch 26.073), train_loss = 1.54185815, grad/param norm = 9.9390e-02, time/batch = 0.1096s	
1070/2050 (epoch 26.098), train_loss = 1.56554174, grad/param norm = 8.9106e-02, time/batch = 0.1097s	
1071/2050 (epoch 26.122), train_loss = 1.56189508, grad/param norm = 8.1863e-02, time/batch = 0.1103s	
1072/2050 (epoch 26.146), train_loss = 1.55916058, grad/param norm = 8.6298e-02, time/batch = 0.1101s	
1073/2050 (epoch 26.171), train_loss = 1.57075456, grad/param norm = 9.1705e-02, time/batch = 0.1097s	
1074/2050 (epoch 26.195), train_loss = 1.57011941, grad/param norm = 9.2644e-02, time/batch = 0.1096s	
1075/2050 (epoch 26.220), train_loss = 1.56506102, grad/param norm = 1.0049e-01, time/batch = 0.1097s	
1076/2050 (epoch 26.244), train_loss = 1.54739001, grad/param norm = 1.0356e-01, time/batch = 0.1101s	
1077/2050 (epoch 26.268), train_loss = 1.57737569, grad/param norm = 9.8748e-02, time/batch = 0.1098s	
1078/2050 (epoch 26.293), train_loss = 1.57880789, grad/param norm = 1.0895e-01, time/batch = 0.1097s	
1079/2050 (epoch 26.317), train_loss = 1.59912808, grad/param norm = 1.1875e-01, time/batch = 0.1096s	
1080/2050 (epoch 26.341), train_loss = 1.58232527, grad/param norm = 1.2357e-01, time/batch = 0.1098s	
1081/2050 (epoch 26.366), train_loss = 1.58519858, grad/param norm = 1.2979e-01, time/batch = 0.1102s	
1082/2050 (epoch 26.390), train_loss = 1.58006547, grad/param norm = 1.2642e-01, time/batch = 0.1097s	
1083/2050 (epoch 26.415), train_loss = 1.56262486, grad/param norm = 1.0989e-01, time/batch = 0.1101s	
1084/2050 (epoch 26.439), train_loss = 1.57504025, grad/param norm = 1.0369e-01, time/batch = 0.1096s	
1085/2050 (epoch 26.463), train_loss = 1.56997416, grad/param norm = 1.0347e-01, time/batch = 0.1097s	
1086/2050 (epoch 26.488), train_loss = 1.57351648, grad/param norm = 1.1142e-01, time/batch = 0.1098s	
1087/2050 (epoch 26.512), train_loss = 1.56985064, grad/param norm = 1.2361e-01, time/batch = 0.1098s	
1088/2050 (epoch 26.537), train_loss = 1.57404413, grad/param norm = 1.2317e-01, time/batch = 0.1098s	
1089/2050 (epoch 26.561), train_loss = 1.57136123, grad/param norm = 1.1579e-01, time/batch = 0.1097s	
1090/2050 (epoch 26.585), train_loss = 1.56694665, grad/param norm = 1.0751e-01, time/batch = 0.1097s	
1091/2050 (epoch 26.610), train_loss = 1.57961118, grad/param norm = 9.4400e-02, time/batch = 0.1103s	
1092/2050 (epoch 26.634), train_loss = 1.56950077, grad/param norm = 8.5919e-02, time/batch = 0.1096s	
1093/2050 (epoch 26.659), train_loss = 1.56681310, grad/param norm = 7.8757e-02, time/batch = 0.1097s	
1094/2050 (epoch 26.683), train_loss = 1.56672176, grad/param norm = 7.8416e-02, time/batch = 0.1095s	
1095/2050 (epoch 26.707), train_loss = 1.55061353, grad/param norm = 8.4806e-02, time/batch = 0.1097s	
1096/2050 (epoch 26.732), train_loss = 1.56807667, grad/param norm = 8.5369e-02, time/batch = 0.1097s	
1097/2050 (epoch 26.756), train_loss = 1.57563102, grad/param norm = 9.2383e-02, time/batch = 0.1100s	
1098/2050 (epoch 26.780), train_loss = 1.57866428, grad/param norm = 1.0145e-01, time/batch = 0.1097s	
1099/2050 (epoch 26.805), train_loss = 1.55119873, grad/param norm = 1.0819e-01, time/batch = 0.1096s	
1100/2050 (epoch 26.829), train_loss = 1.55492272, grad/param norm = 1.0784e-01, time/batch = 0.1097s	
1101/2050 (epoch 26.854), train_loss = 1.54698776, grad/param norm = 1.0496e-01, time/batch = 0.1102s	
1102/2050 (epoch 26.878), train_loss = 1.55977012, grad/param norm = 1.0177e-01, time/batch = 0.1097s	
1103/2050 (epoch 26.902), train_loss = 1.57084120, grad/param norm = 8.9665e-02, time/batch = 0.1097s	
1104/2050 (epoch 26.927), train_loss = 1.54177024, grad/param norm = 7.9299e-02, time/batch = 0.1096s	
1105/2050 (epoch 26.951), train_loss = 1.58602592, grad/param norm = 7.8357e-02, time/batch = 0.1097s	
1106/2050 (epoch 26.976), train_loss = 1.55294703, grad/param norm = 8.5209e-02, time/batch = 0.1099s	
decayed learning rate by a factor 0.97 to 0.0011559025250861	
1107/2050 (epoch 27.000), train_loss = 1.56475005, grad/param norm = 9.1946e-02, time/batch = 0.1098s	
1108/2050 (epoch 27.024), train_loss = 1.70921934, grad/param norm = 1.0116e-01, time/batch = 0.1097s	
1109/2050 (epoch 27.049), train_loss = 1.56340300, grad/param norm = 9.3111e-02, time/batch = 0.1096s	
1110/2050 (epoch 27.073), train_loss = 1.52553803, grad/param norm = 7.5273e-02, time/batch = 0.1097s	
1111/2050 (epoch 27.098), train_loss = 1.55108575, grad/param norm = 7.1377e-02, time/batch = 0.1102s	
1112/2050 (epoch 27.122), train_loss = 1.54841944, grad/param norm = 6.7829e-02, time/batch = 0.1097s	
1113/2050 (epoch 27.146), train_loss = 1.54546289, grad/param norm = 7.3743e-02, time/batch = 0.1100s	
1114/2050 (epoch 27.171), train_loss = 1.55606149, grad/param norm = 7.7176e-02, time/batch = 0.1095s	
1115/2050 (epoch 27.195), train_loss = 1.55480860, grad/param norm = 7.3699e-02, time/batch = 0.1097s	
1116/2050 (epoch 27.220), train_loss = 1.54827727, grad/param norm = 7.1379e-02, time/batch = 0.1097s	
1117/2050 (epoch 27.244), train_loss = 1.53278154, grad/param norm = 9.3638e-02, time/batch = 0.1099s	
1118/2050 (epoch 27.268), train_loss = 1.57174245, grad/param norm = 1.4848e-01, time/batch = 0.1102s	
1119/2050 (epoch 27.293), train_loss = 1.58595062, grad/param norm = 1.7874e-01, time/batch = 0.1096s	
1120/2050 (epoch 27.317), train_loss = 1.59758024, grad/param norm = 1.7455e-01, time/batch = 0.1097s	
1121/2050 (epoch 27.341), train_loss = 1.57631046, grad/param norm = 1.3869e-01, time/batch = 0.1103s	
1122/2050 (epoch 27.366), train_loss = 1.56643106, grad/param norm = 9.2357e-02, time/batch = 0.1097s	
1123/2050 (epoch 27.390), train_loss = 1.56224255, grad/param norm = 7.6826e-02, time/batch = 0.1098s	
1124/2050 (epoch 27.415), train_loss = 1.54501866, grad/param norm = 7.6478e-02, time/batch = 0.1095s	
1125/2050 (epoch 27.439), train_loss = 1.56018134, grad/param norm = 7.5964e-02, time/batch = 0.1098s	
1126/2050 (epoch 27.463), train_loss = 1.55547162, grad/param norm = 8.1658e-02, time/batch = 0.1098s	
1127/2050 (epoch 27.488), train_loss = 1.56164409, grad/param norm = 9.8891e-02, time/batch = 0.1098s	
1128/2050 (epoch 27.512), train_loss = 1.55613195, grad/param norm = 1.1118e-01, time/batch = 0.1102s	
1129/2050 (epoch 27.537), train_loss = 1.55993132, grad/param norm = 1.1151e-01, time/batch = 0.1096s	
1130/2050 (epoch 27.561), train_loss = 1.55798176, grad/param norm = 1.0923e-01, time/batch = 0.1097s	
1131/2050 (epoch 27.585), train_loss = 1.55525764, grad/param norm = 1.0186e-01, time/batch = 0.1102s	
1132/2050 (epoch 27.610), train_loss = 1.56732473, grad/param norm = 9.1037e-02, time/batch = 0.1098s	
1133/2050 (epoch 27.634), train_loss = 1.55672714, grad/param norm = 7.6414e-02, time/batch = 0.1097s	
1134/2050 (epoch 27.659), train_loss = 1.55385840, grad/param norm = 7.2694e-02, time/batch = 0.1095s	
1135/2050 (epoch 27.683), train_loss = 1.55545163, grad/param norm = 7.9394e-02, time/batch = 0.1097s	
1136/2050 (epoch 27.707), train_loss = 1.53928468, grad/param norm = 8.7019e-02, time/batch = 0.1098s	
1137/2050 (epoch 27.732), train_loss = 1.55669991, grad/param norm = 7.4874e-02, time/batch = 0.1097s	
1138/2050 (epoch 27.756), train_loss = 1.56195914, grad/param norm = 7.1808e-02, time/batch = 0.1097s	
1139/2050 (epoch 27.780), train_loss = 1.56328775, grad/param norm = 6.8049e-02, time/batch = 0.1096s	
1140/2050 (epoch 27.805), train_loss = 1.53261410, grad/param norm = 6.1249e-02, time/batch = 0.1097s	
1141/2050 (epoch 27.829), train_loss = 1.53583920, grad/param norm = 5.8079e-02, time/batch = 0.1102s	
1142/2050 (epoch 27.854), train_loss = 1.52904798, grad/param norm = 6.3998e-02, time/batch = 0.1097s	
1143/2050 (epoch 27.878), train_loss = 1.54428432, grad/param norm = 7.3055e-02, time/batch = 0.1102s	
1144/2050 (epoch 27.902), train_loss = 1.55811914, grad/param norm = 8.6142e-02, time/batch = 0.1095s	
1145/2050 (epoch 27.927), train_loss = 1.53498068, grad/param norm = 1.0491e-01, time/batch = 0.1097s	
1146/2050 (epoch 27.951), train_loss = 1.58116803, grad/param norm = 1.2773e-01, time/batch = 0.1098s	
1147/2050 (epoch 27.976), train_loss = 1.55325160, grad/param norm = 1.3214e-01, time/batch = 0.1098s	
decayed learning rate by a factor 0.97 to 0.0011212254493335	
1148/2050 (epoch 28.000), train_loss = 1.55746327, grad/param norm = 1.1604e-01, time/batch = 0.1097s	
1149/2050 (epoch 28.024), train_loss = 1.69989642, grad/param norm = 1.0568e-01, time/batch = 0.1098s	
1150/2050 (epoch 28.049), train_loss = 1.55446615, grad/param norm = 1.0057e-01, time/batch = 0.1097s	
1151/2050 (epoch 28.073), train_loss = 1.51770998, grad/param norm = 9.0813e-02, time/batch = 0.1103s	
1152/2050 (epoch 28.098), train_loss = 1.54253663, grad/param norm = 8.2260e-02, time/batch = 0.1097s	
1153/2050 (epoch 28.122), train_loss = 1.53893769, grad/param norm = 8.2992e-02, time/batch = 0.1097s	
1154/2050 (epoch 28.146), train_loss = 1.53844501, grad/param norm = 1.0309e-01, time/batch = 0.1098s	
1155/2050 (epoch 28.171), train_loss = 1.55127396, grad/param norm = 1.2929e-01, time/batch = 0.1097s	
1156/2050 (epoch 28.195), train_loss = 1.55327090, grad/param norm = 1.3283e-01, time/batch = 0.1098s	
1157/2050 (epoch 28.220), train_loss = 1.54297517, grad/param norm = 1.1654e-01, time/batch = 0.1098s	
1158/2050 (epoch 28.244), train_loss = 1.52447936, grad/param norm = 1.0891e-01, time/batch = 0.1102s	
1159/2050 (epoch 28.268), train_loss = 1.55494782, grad/param norm = 1.0196e-01, time/batch = 0.1097s	
1160/2050 (epoch 28.293), train_loss = 1.55518902, grad/param norm = 1.0012e-01, time/batch = 0.1097s	
1161/2050 (epoch 28.317), train_loss = 1.57137488, grad/param norm = 9.6165e-02, time/batch = 0.1103s	
1162/2050 (epoch 28.341), train_loss = 1.55444890, grad/param norm = 9.5988e-02, time/batch = 0.1097s	
1163/2050 (epoch 28.366), train_loss = 1.55522254, grad/param norm = 8.8856e-02, time/batch = 0.1097s	
1164/2050 (epoch 28.390), train_loss = 1.55072301, grad/param norm = 7.8696e-02, time/batch = 0.1096s	
1165/2050 (epoch 28.415), train_loss = 1.53476540, grad/param norm = 7.0347e-02, time/batch = 0.1097s	
1166/2050 (epoch 28.439), train_loss = 1.54867829, grad/param norm = 6.3018e-02, time/batch = 0.1099s	
1167/2050 (epoch 28.463), train_loss = 1.54312646, grad/param norm = 6.7604e-02, time/batch = 0.1098s	
1168/2050 (epoch 28.488), train_loss = 1.54837760, grad/param norm = 8.0614e-02, time/batch = 0.1097s	
1169/2050 (epoch 28.512), train_loss = 1.54061220, grad/param norm = 8.8451e-02, time/batch = 0.1101s	
1170/2050 (epoch 28.537), train_loss = 1.54697631, grad/param norm = 9.0423e-02, time/batch = 0.1097s	
1171/2050 (epoch 28.561), train_loss = 1.54348580, grad/param norm = 9.7333e-02, time/batch = 0.1102s	
1172/2050 (epoch 28.585), train_loss = 1.54457976, grad/param norm = 1.0027e-01, time/batch = 0.1097s	
1173/2050 (epoch 28.610), train_loss = 1.55755124, grad/param norm = 1.0272e-01, time/batch = 0.1097s	
1174/2050 (epoch 28.634), train_loss = 1.55119976, grad/param norm = 1.0077e-01, time/batch = 0.1095s	
1175/2050 (epoch 28.659), train_loss = 1.54722072, grad/param norm = 1.0516e-01, time/batch = 0.1096s	
1176/2050 (epoch 28.683), train_loss = 1.55019213, grad/param norm = 1.0985e-01, time/batch = 0.1097s	
1177/2050 (epoch 28.707), train_loss = 1.53204567, grad/param norm = 1.1280e-01, time/batch = 0.1097s	
1178/2050 (epoch 28.732), train_loss = 1.55108119, grad/param norm = 1.0925e-01, time/batch = 0.1097s	
1179/2050 (epoch 28.756), train_loss = 1.55685357, grad/param norm = 1.1751e-01, time/batch = 0.1096s	
1180/2050 (epoch 28.780), train_loss = 1.55879681, grad/param norm = 1.1026e-01, time/batch = 0.1097s	
1181/2050 (epoch 28.805), train_loss = 1.52666875, grad/param norm = 9.7041e-02, time/batch = 0.1102s	
1182/2050 (epoch 28.829), train_loss = 1.53039590, grad/param norm = 9.4852e-02, time/batch = 0.1096s	
1183/2050 (epoch 28.854), train_loss = 1.52550945, grad/param norm = 1.0068e-01, time/batch = 0.1097s	
1184/2050 (epoch 28.878), train_loss = 1.53859677, grad/param norm = 9.5790e-02, time/batch = 0.1098s	
1185/2050 (epoch 28.902), train_loss = 1.55074315, grad/param norm = 9.0409e-02, time/batch = 0.1097s	
1186/2050 (epoch 28.927), train_loss = 1.52398322, grad/param norm = 9.7345e-02, time/batch = 0.1098s	
1187/2050 (epoch 28.951), train_loss = 1.56711144, grad/param norm = 1.0548e-01, time/batch = 0.1102s	
1188/2050 (epoch 28.976), train_loss = 1.53418148, grad/param norm = 1.0187e-01, time/batch = 0.1098s	
decayed learning rate by a factor 0.97 to 0.0010875886858535	
1189/2050 (epoch 29.000), train_loss = 1.54246040, grad/param norm = 9.3103e-02, time/batch = 0.1097s	
1190/2050 (epoch 29.024), train_loss = 1.68774714, grad/param norm = 9.0760e-02, time/batch = 0.1097s	
1191/2050 (epoch 29.049), train_loss = 1.54200725, grad/param norm = 8.6133e-02, time/batch = 0.1105s	
1192/2050 (epoch 29.073), train_loss = 1.50542094, grad/param norm = 8.8485e-02, time/batch = 0.1108s	
1193/2050 (epoch 29.098), train_loss = 1.53447041, grad/param norm = 9.8630e-02, time/batch = 0.1098s	
1194/2050 (epoch 29.122), train_loss = 1.53043010, grad/param norm = 1.0417e-01, time/batch = 0.1096s	
1195/2050 (epoch 29.146), train_loss = 1.52944240, grad/param norm = 1.1189e-01, time/batch = 0.1100s	
1196/2050 (epoch 29.171), train_loss = 1.53909337, grad/param norm = 1.1838e-01, time/batch = 0.1098s	
1197/2050 (epoch 29.195), train_loss = 1.53709633, grad/param norm = 1.0602e-01, time/batch = 0.1098s	
1198/2050 (epoch 29.220), train_loss = 1.52816735, grad/param norm = 8.3468e-02, time/batch = 0.1097s	
1199/2050 (epoch 29.244), train_loss = 1.50870990, grad/param norm = 7.6750e-02, time/batch = 0.1101s	
1200/2050 (epoch 29.268), train_loss = 1.54125788, grad/param norm = 7.5217e-02, time/batch = 0.1096s	
1201/2050 (epoch 29.293), train_loss = 1.54129972, grad/param norm = 7.4261e-02, time/batch = 0.1103s	
1202/2050 (epoch 29.317), train_loss = 1.55780314, grad/param norm = 7.1163e-02, time/batch = 0.1097s	
1203/2050 (epoch 29.341), train_loss = 1.54112728, grad/param norm = 6.8573e-02, time/batch = 0.1097s	
1204/2050 (epoch 29.366), train_loss = 1.54245236, grad/param norm = 6.8765e-02, time/batch = 0.1095s	
1205/2050 (epoch 29.390), train_loss = 1.54071457, grad/param norm = 7.3127e-02, time/batch = 0.1097s	
1206/2050 (epoch 29.415), train_loss = 1.52376611, grad/param norm = 7.6570e-02, time/batch = 0.1097s	
1207/2050 (epoch 29.439), train_loss = 1.54215910, grad/param norm = 8.9058e-02, time/batch = 0.1098s	
1208/2050 (epoch 29.463), train_loss = 1.53733277, grad/param norm = 9.6462e-02, time/batch = 0.1098s	
1209/2050 (epoch 29.488), train_loss = 1.54114621, grad/param norm = 8.6082e-02, time/batch = 0.1099s	
1210/2050 (epoch 29.512), train_loss = 1.53025110, grad/param norm = 8.6551e-02, time/batch = 0.1103s	
1211/2050 (epoch 29.537), train_loss = 1.53738570, grad/param norm = 8.9056e-02, time/batch = 0.1102s	
1212/2050 (epoch 29.561), train_loss = 1.53330031, grad/param norm = 9.8940e-02, time/batch = 0.1097s	
1213/2050 (epoch 29.585), train_loss = 1.53475213, grad/param norm = 1.0016e-01, time/batch = 0.1097s	
1214/2050 (epoch 29.610), train_loss = 1.54509753, grad/param norm = 9.2972e-02, time/batch = 0.1095s	
1215/2050 (epoch 29.634), train_loss = 1.53739027, grad/param norm = 1.0245e-01, time/batch = 0.1097s	
1216/2050 (epoch 29.659), train_loss = 1.53790756, grad/param norm = 1.0684e-01, time/batch = 0.1097s	
1217/2050 (epoch 29.683), train_loss = 1.53874600, grad/param norm = 1.0257e-01, time/batch = 0.1097s	
1218/2050 (epoch 29.707), train_loss = 1.52110663, grad/param norm = 9.7714e-02, time/batch = 0.1097s	
1219/2050 (epoch 29.732), train_loss = 1.53655398, grad/param norm = 8.9601e-02, time/batch = 0.1096s	
1220/2050 (epoch 29.756), train_loss = 1.54347005, grad/param norm = 8.9021e-02, time/batch = 0.1098s	
1221/2050 (epoch 29.780), train_loss = 1.54572247, grad/param norm = 8.9784e-02, time/batch = 0.1105s	
1222/2050 (epoch 29.805), train_loss = 1.51569228, grad/param norm = 8.8082e-02, time/batch = 0.1096s	
1223/2050 (epoch 29.829), train_loss = 1.51878934, grad/param norm = 8.9953e-02, time/batch = 0.1097s	
1224/2050 (epoch 29.854), train_loss = 1.51289135, grad/param norm = 9.1618e-02, time/batch = 0.1095s	
1225/2050 (epoch 29.878), train_loss = 1.52647466, grad/param norm = 8.4095e-02, time/batch = 0.1102s	
1226/2050 (epoch 29.902), train_loss = 1.53842693, grad/param norm = 7.5206e-02, time/batch = 0.1097s	
1227/2050 (epoch 29.927), train_loss = 1.51178418, grad/param norm = 8.3900e-02, time/batch = 0.1097s	
1228/2050 (epoch 29.951), train_loss = 1.55577702, grad/param norm = 1.0092e-01, time/batch = 0.1100s	
1229/2050 (epoch 29.976), train_loss = 1.52550114, grad/param norm = 1.1160e-01, time/batch = 0.1096s	
decayed learning rate by a factor 0.97 to 0.0010549610252779	
1230/2050 (epoch 30.000), train_loss = 1.53631635, grad/param norm = 1.1367e-01, time/batch = 0.1096s	
1231/2050 (epoch 30.024), train_loss = 1.68325439, grad/param norm = 1.1093e-01, time/batch = 0.1103s	
1232/2050 (epoch 30.049), train_loss = 1.53556087, grad/param norm = 1.0899e-01, time/batch = 0.1101s	
1233/2050 (epoch 30.073), train_loss = 1.49637437, grad/param norm = 9.1705e-02, time/batch = 0.1097s	
1234/2050 (epoch 30.098), train_loss = 1.52076200, grad/param norm = 7.7905e-02, time/batch = 0.1096s	
1235/2050 (epoch 30.122), train_loss = 1.51701282, grad/param norm = 7.3219e-02, time/batch = 0.1097s	
1236/2050 (epoch 30.146), train_loss = 1.51590436, grad/param norm = 8.3251e-02, time/batch = 0.1101s	
1237/2050 (epoch 30.171), train_loss = 1.52563969, grad/param norm = 8.3047e-02, time/batch = 0.1097s	
1238/2050 (epoch 30.195), train_loss = 1.52306237, grad/param norm = 7.2670e-02, time/batch = 0.1098s	
1239/2050 (epoch 30.220), train_loss = 1.51657947, grad/param norm = 6.2296e-02, time/batch = 0.1095s	
1240/2050 (epoch 30.244), train_loss = 1.49718384, grad/param norm = 5.9663e-02, time/batch = 0.1098s	
1241/2050 (epoch 30.268), train_loss = 1.52956308, grad/param norm = 5.6451e-02, time/batch = 0.1102s	
1242/2050 (epoch 30.293), train_loss = 1.52993015, grad/param norm = 5.7278e-02, time/batch = 0.1096s	
1243/2050 (epoch 30.317), train_loss = 1.54718420, grad/param norm = 6.2350e-02, time/batch = 0.1097s	
1244/2050 (epoch 30.341), train_loss = 1.53316840, grad/param norm = 8.5169e-02, time/batch = 0.1095s	
1245/2050 (epoch 30.366), train_loss = 1.54117822, grad/param norm = 1.1958e-01, time/batch = 0.1097s	
1246/2050 (epoch 30.390), train_loss = 1.54339721, grad/param norm = 1.5541e-01, time/batch = 0.1098s	
1247/2050 (epoch 30.415), train_loss = 1.52913181, grad/param norm = 1.5471e-01, time/batch = 0.1096s	
1248/2050 (epoch 30.439), train_loss = 1.53916450, grad/param norm = 1.3407e-01, time/batch = 0.1097s	
1249/2050 (epoch 30.463), train_loss = 1.52957091, grad/param norm = 1.1066e-01, time/batch = 0.1096s	
1250/2050 (epoch 30.488), train_loss = 1.52739407, grad/param norm = 8.8377e-02, time/batch = 0.1097s	
1251/2050 (epoch 30.512), train_loss = 1.51919960, grad/param norm = 8.0205e-02, time/batch = 0.1102s	
1252/2050 (epoch 30.537), train_loss = 1.52365841, grad/param norm = 7.2003e-02, time/batch = 0.1097s	
1253/2050 (epoch 30.561), train_loss = 1.51924333, grad/param norm = 7.3723e-02, time/batch = 0.1097s	
1254/2050 (epoch 30.585), train_loss = 1.52035628, grad/param norm = 7.9227e-02, time/batch = 0.1095s	
1255/2050 (epoch 30.610), train_loss = 1.53434977, grad/param norm = 7.6299e-02, time/batch = 0.1097s	
1256/2050 (epoch 30.634), train_loss = 1.52583635, grad/param norm = 8.1396e-02, time/batch = 0.1098s	
1257/2050 (epoch 30.659), train_loss = 1.52523197, grad/param norm = 8.5148e-02, time/batch = 0.1097s	
1258/2050 (epoch 30.683), train_loss = 1.52701280, grad/param norm = 8.6168e-02, time/batch = 0.1097s	
1259/2050 (epoch 30.707), train_loss = 1.50936424, grad/param norm = 8.0339e-02, time/batch = 0.1096s	
1260/2050 (epoch 30.732), train_loss = 1.52448687, grad/param norm = 7.3108e-02, time/batch = 0.1097s	
1261/2050 (epoch 30.756), train_loss = 1.53230084, grad/param norm = 6.9568e-02, time/batch = 0.1101s	
1262/2050 (epoch 30.780), train_loss = 1.53452096, grad/param norm = 6.8082e-02, time/batch = 0.1098s	
1263/2050 (epoch 30.805), train_loss = 1.50562614, grad/param norm = 7.5171e-02, time/batch = 0.1097s	
1264/2050 (epoch 30.829), train_loss = 1.50995905, grad/param norm = 9.0162e-02, time/batch = 0.1095s	
1265/2050 (epoch 30.854), train_loss = 1.50649610, grad/param norm = 9.6799e-02, time/batch = 0.1096s	
1266/2050 (epoch 30.878), train_loss = 1.52060705, grad/param norm = 9.9034e-02, time/batch = 0.1101s	
1267/2050 (epoch 30.902), train_loss = 1.53678618, grad/param norm = 1.1450e-01, time/batch = 0.1107s	
1268/2050 (epoch 30.927), train_loss = 1.51037827, grad/param norm = 1.2323e-01, time/batch = 0.1100s	
1269/2050 (epoch 30.951), train_loss = 1.55055691, grad/param norm = 1.2260e-01, time/batch = 0.1104s	
1270/2050 (epoch 30.976), train_loss = 1.51659081, grad/param norm = 1.0857e-01, time/batch = 0.1098s	
decayed learning rate by a factor 0.97 to 0.0010233121945196	
1271/2050 (epoch 31.000), train_loss = 1.52203557, grad/param norm = 8.3033e-02, time/batch = 0.1102s	
1272/2050 (epoch 31.024), train_loss = 1.66914721, grad/param norm = 7.2873e-02, time/batch = 0.1097s	
1273/2050 (epoch 31.049), train_loss = 1.51977706, grad/param norm = 7.1263e-02, time/batch = 0.1101s	
1274/2050 (epoch 31.073), train_loss = 1.48306014, grad/param norm = 6.8954e-02, time/batch = 0.1095s	
1275/2050 (epoch 31.098), train_loss = 1.51065270, grad/param norm = 6.9680e-02, time/batch = 0.1098s	
1276/2050 (epoch 31.122), train_loss = 1.50675404, grad/param norm = 7.2887e-02, time/batch = 0.1098s	
1277/2050 (epoch 31.146), train_loss = 1.50648335, grad/param norm = 8.7941e-02, time/batch = 0.1101s	
1278/2050 (epoch 31.171), train_loss = 1.51749942, grad/param norm = 9.9615e-02, time/batch = 0.1098s	
1279/2050 (epoch 31.195), train_loss = 1.51665136, grad/param norm = 9.5927e-02, time/batch = 0.1097s	
1280/2050 (epoch 31.220), train_loss = 1.50903304, grad/param norm = 8.1929e-02, time/batch = 0.1098s	
1281/2050 (epoch 31.244), train_loss = 1.49076268, grad/param norm = 8.5329e-02, time/batch = 0.1103s	
1282/2050 (epoch 31.268), train_loss = 1.52568457, grad/param norm = 9.7350e-02, time/batch = 0.1096s	
1283/2050 (epoch 31.293), train_loss = 1.52835217, grad/param norm = 1.0747e-01, time/batch = 0.1098s	
1284/2050 (epoch 31.317), train_loss = 1.54457084, grad/param norm = 1.1674e-01, time/batch = 0.1095s	
1285/2050 (epoch 31.341), train_loss = 1.52926737, grad/param norm = 1.1107e-01, time/batch = 0.1097s	
1286/2050 (epoch 31.366), train_loss = 1.52650424, grad/param norm = 9.2698e-02, time/batch = 0.1097s	
1287/2050 (epoch 31.390), train_loss = 1.52386861, grad/param norm = 8.0453e-02, time/batch = 0.1101s	
1288/2050 (epoch 31.415), train_loss = 1.50536373, grad/param norm = 8.1529e-02, time/batch = 0.1100s	
1289/2050 (epoch 31.439), train_loss = 1.52138267, grad/param norm = 7.7111e-02, time/batch = 0.1096s	
1290/2050 (epoch 31.463), train_loss = 1.51541745, grad/param norm = 7.7764e-02, time/batch = 0.1097s	
1291/2050 (epoch 31.488), train_loss = 1.52018305, grad/param norm = 8.7301e-02, time/batch = 0.1102s	
1292/2050 (epoch 31.512), train_loss = 1.51088426, grad/param norm = 9.0169e-02, time/batch = 0.1097s	
1293/2050 (epoch 31.537), train_loss = 1.51675286, grad/param norm = 8.8040e-02, time/batch = 0.1097s	
1294/2050 (epoch 31.561), train_loss = 1.51227727, grad/param norm = 9.0617e-02, time/batch = 0.1095s	
1295/2050 (epoch 31.585), train_loss = 1.51347254, grad/param norm = 9.4034e-02, time/batch = 0.1097s	
1296/2050 (epoch 31.610), train_loss = 1.52645656, grad/param norm = 8.5937e-02, time/batch = 0.1098s	
1297/2050 (epoch 31.634), train_loss = 1.51655837, grad/param norm = 8.3104e-02, time/batch = 0.1098s	
1298/2050 (epoch 31.659), train_loss = 1.51524771, grad/param norm = 8.1233e-02, time/batch = 0.1098s	
1299/2050 (epoch 31.683), train_loss = 1.51702006, grad/param norm = 7.9630e-02, time/batch = 0.1097s	
1300/2050 (epoch 31.707), train_loss = 1.49879412, grad/param norm = 7.5349e-02, time/batch = 0.1097s	
1301/2050 (epoch 31.732), train_loss = 1.51451174, grad/param norm = 6.3844e-02, time/batch = 0.1102s	
1302/2050 (epoch 31.756), train_loss = 1.52157788, grad/param norm = 6.0214e-02, time/batch = 0.1097s	
1303/2050 (epoch 31.780), train_loss = 1.52440628, grad/param norm = 5.9252e-02, time/batch = 0.1100s	
1304/2050 (epoch 31.805), train_loss = 1.49397593, grad/param norm = 5.9060e-02, time/batch = 0.1095s	
1305/2050 (epoch 31.829), train_loss = 1.49621857, grad/param norm = 5.7251e-02, time/batch = 0.1097s	
1306/2050 (epoch 31.854), train_loss = 1.49054233, grad/param norm = 6.5984e-02, time/batch = 0.1098s	
1307/2050 (epoch 31.878), train_loss = 1.50823651, grad/param norm = 7.9495e-02, time/batch = 0.1101s	
1308/2050 (epoch 31.902), train_loss = 1.52313820, grad/param norm = 8.8329e-02, time/batch = 0.1098s	
1309/2050 (epoch 31.927), train_loss = 1.49679905, grad/param norm = 9.0467e-02, time/batch = 0.1096s	
1310/2050 (epoch 31.951), train_loss = 1.53563848, grad/param norm = 9.1601e-02, time/batch = 0.1102s	
1311/2050 (epoch 31.976), train_loss = 1.50482000, grad/param norm = 9.1651e-02, time/batch = 0.1102s	
decayed learning rate by a factor 0.97 to 0.00099261282868397	
1312/2050 (epoch 32.000), train_loss = 1.51456764, grad/param norm = 8.4610e-02, time/batch = 0.1097s	
1313/2050 (epoch 32.024), train_loss = 1.66586409, grad/param norm = 9.1533e-02, time/batch = 0.1098s	
1314/2050 (epoch 32.049), train_loss = 1.51467961, grad/param norm = 9.0659e-02, time/batch = 0.1100s	
1315/2050 (epoch 32.073), train_loss = 1.47504817, grad/param norm = 7.3259e-02, time/batch = 0.1097s	
1316/2050 (epoch 32.098), train_loss = 1.50236103, grad/param norm = 6.6366e-02, time/batch = 0.1098s	
1317/2050 (epoch 32.122), train_loss = 1.49706221, grad/param norm = 6.1248e-02, time/batch = 0.1097s	
1318/2050 (epoch 32.146), train_loss = 1.49519869, grad/param norm = 6.2776e-02, time/batch = 0.1102s	
1319/2050 (epoch 32.171), train_loss = 1.50679387, grad/param norm = 7.8295e-02, time/batch = 0.1095s	
1320/2050 (epoch 32.195), train_loss = 1.50754796, grad/param norm = 9.0773e-02, time/batch = 0.1097s	
1321/2050 (epoch 32.220), train_loss = 1.50429701, grad/param norm = 9.8496e-02, time/batch = 0.1102s	
1322/2050 (epoch 32.244), train_loss = 1.48765165, grad/param norm = 1.0937e-01, time/batch = 0.1097s	
1323/2050 (epoch 32.268), train_loss = 1.52232571, grad/param norm = 1.1508e-01, time/batch = 0.1097s	
1324/2050 (epoch 32.293), train_loss = 1.52246661, grad/param norm = 1.1019e-01, time/batch = 0.1094s	
1325/2050 (epoch 32.317), train_loss = 1.53483186, grad/param norm = 9.5209e-02, time/batch = 0.1097s	
1326/2050 (epoch 32.341), train_loss = 1.51631401, grad/param norm = 7.8275e-02, time/batch = 0.1098s	
1327/2050 (epoch 32.366), train_loss = 1.51514977, grad/param norm = 6.0043e-02, time/batch = 0.1097s	
1328/2050 (epoch 32.390), train_loss = 1.51191427, grad/param norm = 4.9846e-02, time/batch = 0.1098s	
1329/2050 (epoch 32.415), train_loss = 1.49396977, grad/param norm = 5.1414e-02, time/batch = 0.1095s	
1330/2050 (epoch 32.439), train_loss = 1.50986818, grad/param norm = 5.1571e-02, time/batch = 0.1097s	
1331/2050 (epoch 32.463), train_loss = 1.50391484, grad/param norm = 5.6335e-02, time/batch = 0.1102s	
1332/2050 (epoch 32.488), train_loss = 1.50778414, grad/param norm = 7.3946e-02, time/batch = 0.1097s	
1333/2050 (epoch 32.512), train_loss = 1.50295353, grad/param norm = 8.7757e-02, time/batch = 0.1097s	
1334/2050 (epoch 32.537), train_loss = 1.50802526, grad/param norm = 8.2596e-02, time/batch = 0.1095s	
1335/2050 (epoch 32.561), train_loss = 1.50202034, grad/param norm = 7.4250e-02, time/batch = 0.1096s	
1336/2050 (epoch 32.585), train_loss = 1.50170683, grad/param norm = 6.9566e-02, time/batch = 0.1097s	
1337/2050 (epoch 32.610), train_loss = 1.51529655, grad/param norm = 6.8245e-02, time/batch = 0.1097s	
1338/2050 (epoch 32.634), train_loss = 1.50701718, grad/param norm = 7.4557e-02, time/batch = 0.1097s	
1339/2050 (epoch 32.659), train_loss = 1.50716080, grad/param norm = 9.3455e-02, time/batch = 0.1095s	
1340/2050 (epoch 32.683), train_loss = 1.51389439, grad/param norm = 1.2612e-01, time/batch = 0.1096s	
1341/2050 (epoch 32.707), train_loss = 1.50223046, grad/param norm = 1.4400e-01, time/batch = 0.1117s	
1342/2050 (epoch 32.732), train_loss = 1.51905958, grad/param norm = 1.4426e-01, time/batch = 0.1098s	
1343/2050 (epoch 32.756), train_loss = 1.52438980, grad/param norm = 1.2582e-01, time/batch = 0.1099s	
1344/2050 (epoch 32.780), train_loss = 1.52248419, grad/param norm = 1.1016e-01, time/batch = 0.1099s	
1345/2050 (epoch 32.805), train_loss = 1.49214557, grad/param norm = 1.0227e-01, time/batch = 0.1097s	
1346/2050 (epoch 32.829), train_loss = 1.49174854, grad/param norm = 9.1710e-02, time/batch = 0.1097s	
1347/2050 (epoch 32.854), train_loss = 1.48516315, grad/param norm = 8.5333e-02, time/batch = 0.1097s	
1348/2050 (epoch 32.878), train_loss = 1.49958925, grad/param norm = 7.9716e-02, time/batch = 0.1098s	
1349/2050 (epoch 32.902), train_loss = 1.51371563, grad/param norm = 7.5325e-02, time/batch = 0.1096s	
1350/2050 (epoch 32.927), train_loss = 1.48397150, grad/param norm = 6.5669e-02, time/batch = 0.1098s	
1351/2050 (epoch 32.951), train_loss = 1.52367912, grad/param norm = 6.4553e-02, time/batch = 0.1102s	
1352/2050 (epoch 32.976), train_loss = 1.49092101, grad/param norm = 7.3979e-02, time/batch = 0.1097s	
decayed learning rate by a factor 0.97 to 0.00096283444382345	
1353/2050 (epoch 33.000), train_loss = 1.50620554, grad/param norm = 9.3015e-02, time/batch = 0.1097s	
1354/2050 (epoch 33.024), train_loss = 1.66154166, grad/param norm = 1.0050e-01, time/batch = 0.1096s	
1355/2050 (epoch 33.049), train_loss = 1.50775575, grad/param norm = 8.7200e-02, time/batch = 0.1100s	
1356/2050 (epoch 33.073), train_loss = 1.46697705, grad/param norm = 7.0455e-02, time/batch = 0.1098s	
1357/2050 (epoch 33.098), train_loss = 1.49460986, grad/param norm = 6.5523e-02, time/batch = 0.1098s	
1358/2050 (epoch 33.122), train_loss = 1.48808801, grad/param norm = 5.9679e-02, time/batch = 0.1097s	
1359/2050 (epoch 33.146), train_loss = 1.48720309, grad/param norm = 6.2849e-02, time/batch = 0.1099s	
1360/2050 (epoch 33.171), train_loss = 1.49720185, grad/param norm = 7.1190e-02, time/batch = 0.1096s	
1361/2050 (epoch 33.195), train_loss = 1.49614211, grad/param norm = 6.7642e-02, time/batch = 0.1102s	
1362/2050 (epoch 33.220), train_loss = 1.49062452, grad/param norm = 6.1333e-02, time/batch = 0.1097s	
1363/2050 (epoch 33.244), train_loss = 1.47145422, grad/param norm = 6.2461e-02, time/batch = 0.1097s	
1364/2050 (epoch 33.268), train_loss = 1.50517329, grad/param norm = 6.5857e-02, time/batch = 0.1095s	
1365/2050 (epoch 33.293), train_loss = 1.50635382, grad/param norm = 7.3108e-02, time/batch = 0.1097s	
1366/2050 (epoch 33.317), train_loss = 1.52351061, grad/param norm = 8.7387e-02, time/batch = 0.1098s	
1367/2050 (epoch 33.341), train_loss = 1.50959067, grad/param norm = 9.5792e-02, time/batch = 0.1098s	
1368/2050 (epoch 33.366), train_loss = 1.51066687, grad/param norm = 9.5606e-02, time/batch = 0.1099s	
1369/2050 (epoch 33.390), train_loss = 1.51004008, grad/param norm = 9.2282e-02, time/batch = 0.1097s	
1370/2050 (epoch 33.415), train_loss = 1.49136634, grad/param norm = 9.6424e-02, time/batch = 0.1097s	
1371/2050 (epoch 33.439), train_loss = 1.50738590, grad/param norm = 9.0610e-02, time/batch = 0.1101s	
1372/2050 (epoch 33.463), train_loss = 1.50008719, grad/param norm = 8.9520e-02, time/batch = 0.1097s	
1373/2050 (epoch 33.488), train_loss = 1.50481085, grad/param norm = 9.4728e-02, time/batch = 0.1098s	
1374/2050 (epoch 33.512), train_loss = 1.49542012, grad/param norm = 9.4853e-02, time/batch = 0.1096s	
1375/2050 (epoch 33.537), train_loss = 1.50109001, grad/param norm = 8.7571e-02, time/batch = 0.1097s	
1376/2050 (epoch 33.561), train_loss = 1.49397971, grad/param norm = 7.9433e-02, time/batch = 0.1097s	
1377/2050 (epoch 33.585), train_loss = 1.49511115, grad/param norm = 7.8073e-02, time/batch = 0.1096s	
1378/2050 (epoch 33.610), train_loss = 1.50732569, grad/param norm = 7.6524e-02, time/batch = 0.1097s	
1379/2050 (epoch 33.634), train_loss = 1.49882729, grad/param norm = 7.3336e-02, time/batch = 0.1096s	
1380/2050 (epoch 33.659), train_loss = 1.49626393, grad/param norm = 7.3406e-02, time/batch = 0.1097s	
1381/2050 (epoch 33.683), train_loss = 1.49990056, grad/param norm = 8.2268e-02, time/batch = 0.1107s	
1382/2050 (epoch 33.707), train_loss = 1.48438959, grad/param norm = 1.0148e-01, time/batch = 0.1096s	
1383/2050 (epoch 33.732), train_loss = 1.50503552, grad/param norm = 1.2220e-01, time/batch = 0.1097s	
1384/2050 (epoch 33.756), train_loss = 1.51548094, grad/param norm = 1.2235e-01, time/batch = 0.1096s	
1385/2050 (epoch 33.780), train_loss = 1.51619415, grad/param norm = 1.0565e-01, time/batch = 0.1100s	
1386/2050 (epoch 33.805), train_loss = 1.48263832, grad/param norm = 9.2986e-02, time/batch = 0.1098s	
1387/2050 (epoch 33.829), train_loss = 1.48368644, grad/param norm = 8.4715e-02, time/batch = 0.1097s	
1388/2050 (epoch 33.854), train_loss = 1.47599268, grad/param norm = 8.4574e-02, time/batch = 0.1107s	
1389/2050 (epoch 33.878), train_loss = 1.49248320, grad/param norm = 7.9143e-02, time/batch = 0.1098s	
1390/2050 (epoch 33.902), train_loss = 1.50385461, grad/param norm = 6.5222e-02, time/batch = 0.1097s	
1391/2050 (epoch 33.927), train_loss = 1.47663745, grad/param norm = 6.4509e-02, time/batch = 0.1102s	
1392/2050 (epoch 33.951), train_loss = 1.51552017, grad/param norm = 7.2981e-02, time/batch = 0.1096s	
1393/2050 (epoch 33.976), train_loss = 1.48446592, grad/param norm = 7.7893e-02, time/batch = 0.1097s	
decayed learning rate by a factor 0.97 to 0.00093394941050874	
1394/2050 (epoch 34.000), train_loss = 1.49754689, grad/param norm = 8.4245e-02, time/batch = 0.1095s	
1395/2050 (epoch 34.024), train_loss = 1.65211681, grad/param norm = 8.7291e-02, time/batch = 0.1097s	
1396/2050 (epoch 34.049), train_loss = 1.49766820, grad/param norm = 8.3327e-02, time/batch = 0.1101s	
1397/2050 (epoch 34.073), train_loss = 1.45830210, grad/param norm = 6.7299e-02, time/batch = 0.1097s	
1398/2050 (epoch 34.098), train_loss = 1.48617631, grad/param norm = 6.1875e-02, time/batch = 0.1097s	
1399/2050 (epoch 34.122), train_loss = 1.48070160, grad/param norm = 6.0667e-02, time/batch = 0.1096s	
1400/2050 (epoch 34.146), train_loss = 1.48084010, grad/param norm = 7.1137e-02, time/batch = 0.1098s	
1401/2050 (epoch 34.171), train_loss = 1.49131492, grad/param norm = 8.1319e-02, time/batch = 0.1102s	
1402/2050 (epoch 34.195), train_loss = 1.49065788, grad/param norm = 8.0175e-02, time/batch = 0.1097s	
1403/2050 (epoch 34.220), train_loss = 1.48599828, grad/param norm = 7.9498e-02, time/batch = 0.1098s	
1404/2050 (epoch 34.244), train_loss = 1.46706944, grad/param norm = 7.9229e-02, time/batch = 0.1095s	
1405/2050 (epoch 34.268), train_loss = 1.49898577, grad/param norm = 7.5129e-02, time/batch = 0.1097s	
1406/2050 (epoch 34.293), train_loss = 1.49942667, grad/param norm = 7.6535e-02, time/batch = 0.1098s	
1407/2050 (epoch 34.317), train_loss = 1.51572457, grad/param norm = 7.9875e-02, time/batch = 0.1097s	
1408/2050 (epoch 34.341), train_loss = 1.50053747, grad/param norm = 8.1599e-02, time/batch = 0.1097s	
1409/2050 (epoch 34.366), train_loss = 1.50242244, grad/param norm = 8.1981e-02, time/batch = 0.1095s	
1410/2050 (epoch 34.390), train_loss = 1.50115185, grad/param norm = 8.4338e-02, time/batch = 0.1097s	
1411/2050 (epoch 34.415), train_loss = 1.48297040, grad/param norm = 8.9214e-02, time/batch = 0.1105s	
1412/2050 (epoch 34.439), train_loss = 1.50149073, grad/param norm = 9.6762e-02, time/batch = 0.1097s	
1413/2050 (epoch 34.463), train_loss = 1.49535776, grad/param norm = 9.8086e-02, time/batch = 0.1097s	
1414/2050 (epoch 34.488), train_loss = 1.49574439, grad/param norm = 8.9645e-02, time/batch = 0.1096s	
1415/2050 (epoch 34.512), train_loss = 1.48650406, grad/param norm = 7.8021e-02, time/batch = 0.1098s	
1416/2050 (epoch 34.537), train_loss = 1.49063039, grad/param norm = 6.9211e-02, time/batch = 0.1109s	
1417/2050 (epoch 34.561), train_loss = 1.48579842, grad/param norm = 7.6912e-02, time/batch = 0.1099s	
1418/2050 (epoch 34.585), train_loss = 1.48851541, grad/param norm = 8.1997e-02, time/batch = 0.1099s	
1419/2050 (epoch 34.610), train_loss = 1.49946725, grad/param norm = 7.7864e-02, time/batch = 0.1097s	
1420/2050 (epoch 34.634), train_loss = 1.49131668, grad/param norm = 7.0041e-02, time/batch = 0.1097s	
1421/2050 (epoch 34.659), train_loss = 1.48762557, grad/param norm = 6.7829e-02, time/batch = 0.1103s	
1422/2050 (epoch 34.683), train_loss = 1.49084350, grad/param norm = 6.7500e-02, time/batch = 0.1101s	
1423/2050 (epoch 34.707), train_loss = 1.47308042, grad/param norm = 7.3935e-02, time/batch = 0.1097s	
1424/2050 (epoch 34.732), train_loss = 1.49213547, grad/param norm = 7.7432e-02, time/batch = 0.1095s	
1425/2050 (epoch 34.756), train_loss = 1.50031651, grad/param norm = 9.4866e-02, time/batch = 0.1098s	
1426/2050 (epoch 34.780), train_loss = 1.50545251, grad/param norm = 9.7098e-02, time/batch = 0.1102s	
1427/2050 (epoch 34.805), train_loss = 1.47311891, grad/param norm = 8.8338e-02, time/batch = 0.1097s	
1428/2050 (epoch 34.829), train_loss = 1.47637935, grad/param norm = 8.4779e-02, time/batch = 0.1097s	
1429/2050 (epoch 34.854), train_loss = 1.46918521, grad/param norm = 8.8866e-02, time/batch = 0.1095s	
1430/2050 (epoch 34.878), train_loss = 1.48644553, grad/param norm = 8.4559e-02, time/batch = 0.1102s	
1431/2050 (epoch 34.902), train_loss = 1.49836442, grad/param norm = 7.2357e-02, time/batch = 0.1102s	
1432/2050 (epoch 34.927), train_loss = 1.47106817, grad/param norm = 7.3634e-02, time/batch = 0.1097s	
1433/2050 (epoch 34.951), train_loss = 1.51047773, grad/param norm = 8.2115e-02, time/batch = 0.1097s	
1434/2050 (epoch 34.976), train_loss = 1.47943584, grad/param norm = 9.4086e-02, time/batch = 0.1096s	
decayed learning rate by a factor 0.97 to 0.00090593092819348	
1435/2050 (epoch 35.000), train_loss = 1.49477464, grad/param norm = 1.1382e-01, time/batch = 0.1097s	
1436/2050 (epoch 35.024), train_loss = 1.64779124, grad/param norm = 9.4597e-02, time/batch = 0.1097s	
1437/2050 (epoch 35.049), train_loss = 1.49079864, grad/param norm = 7.0943e-02, time/batch = 0.1102s	
1438/2050 (epoch 35.073), train_loss = 1.45068720, grad/param norm = 6.1882e-02, time/batch = 0.1097s	
1439/2050 (epoch 35.098), train_loss = 1.47976453, grad/param norm = 6.7049e-02, time/batch = 0.1096s	
1440/2050 (epoch 35.122), train_loss = 1.47401450, grad/param norm = 7.0824e-02, time/batch = 0.1098s	
1441/2050 (epoch 35.146), train_loss = 1.47487730, grad/param norm = 8.3152e-02, time/batch = 0.1102s	
1442/2050 (epoch 35.171), train_loss = 1.48483958, grad/param norm = 9.1105e-02, time/batch = 0.1096s	
1443/2050 (epoch 35.195), train_loss = 1.48336084, grad/param norm = 8.1542e-02, time/batch = 0.1097s	
1444/2050 (epoch 35.220), train_loss = 1.47668943, grad/param norm = 6.7234e-02, time/batch = 0.1096s	
1445/2050 (epoch 35.244), train_loss = 1.45694757, grad/param norm = 6.3535e-02, time/batch = 0.1097s	
1446/2050 (epoch 35.268), train_loss = 1.49010926, grad/param norm = 6.1521e-02, time/batch = 0.1098s	
1447/2050 (epoch 35.293), train_loss = 1.48980419, grad/param norm = 6.1996e-02, time/batch = 0.1096s	
1448/2050 (epoch 35.317), train_loss = 1.50612309, grad/param norm = 6.4062e-02, time/batch = 0.1098s	
1449/2050 (epoch 35.341), train_loss = 1.49191174, grad/param norm = 7.7948e-02, time/batch = 0.1095s	
1450/2050 (epoch 35.366), train_loss = 1.49625005, grad/param norm = 8.6895e-02, time/batch = 0.1097s	
1451/2050 (epoch 35.390), train_loss = 1.49376424, grad/param norm = 8.8135e-02, time/batch = 0.1103s	
1452/2050 (epoch 35.415), train_loss = 1.47649910, grad/param norm = 8.3049e-02, time/batch = 0.1101s	
1453/2050 (epoch 35.439), train_loss = 1.49189873, grad/param norm = 7.9916e-02, time/batch = 0.1098s	
1454/2050 (epoch 35.463), train_loss = 1.48468896, grad/param norm = 7.8109e-02, time/batch = 0.1095s	
1455/2050 (epoch 35.488), train_loss = 1.48456360, grad/param norm = 7.4038e-02, time/batch = 0.1097s	
1456/2050 (epoch 35.512), train_loss = 1.47653209, grad/param norm = 6.6559e-02, time/batch = 0.1097s	
1457/2050 (epoch 35.537), train_loss = 1.48240492, grad/param norm = 6.1974e-02, time/batch = 0.1097s	
1458/2050 (epoch 35.561), train_loss = 1.47696446, grad/param norm = 6.3547e-02, time/batch = 0.1097s	
1459/2050 (epoch 35.585), train_loss = 1.47913451, grad/param norm = 6.7947e-02, time/batch = 0.1097s	
1460/2050 (epoch 35.610), train_loss = 1.49161042, grad/param norm = 6.9064e-02, time/batch = 0.1097s	
1461/2050 (epoch 35.634), train_loss = 1.48364355, grad/param norm = 6.6806e-02, time/batch = 0.1102s	
1462/2050 (epoch 35.659), train_loss = 1.48154749, grad/param norm = 7.1691e-02, time/batch = 0.1097s	
1463/2050 (epoch 35.683), train_loss = 1.48629637, grad/param norm = 8.1772e-02, time/batch = 0.1100s	
1464/2050 (epoch 35.707), train_loss = 1.46795398, grad/param norm = 8.6984e-02, time/batch = 0.1096s	
1465/2050 (epoch 35.732), train_loss = 1.48649669, grad/param norm = 8.2262e-02, time/batch = 0.1097s	
1466/2050 (epoch 35.756), train_loss = 1.49370004, grad/param norm = 9.2505e-02, time/batch = 0.1097s	
1467/2050 (epoch 35.780), train_loss = 1.49836964, grad/param norm = 9.1057e-02, time/batch = 0.1101s	
1468/2050 (epoch 35.805), train_loss = 1.46593514, grad/param norm = 8.4109e-02, time/batch = 0.1097s	
1469/2050 (epoch 35.829), train_loss = 1.46974398, grad/param norm = 8.3430e-02, time/batch = 0.1096s	
1470/2050 (epoch 35.854), train_loss = 1.46220135, grad/param norm = 9.1589e-02, time/batch = 0.1098s	
1471/2050 (epoch 35.878), train_loss = 1.48095415, grad/param norm = 9.2106e-02, time/batch = 0.1102s	
1472/2050 (epoch 35.902), train_loss = 1.49276876, grad/param norm = 8.2098e-02, time/batch = 0.1096s	
1473/2050 (epoch 35.927), train_loss = 1.46459956, grad/param norm = 7.6811e-02, time/batch = 0.1097s	
1474/2050 (epoch 35.951), train_loss = 1.50272106, grad/param norm = 7.6139e-02, time/batch = 0.1095s	
1475/2050 (epoch 35.976), train_loss = 1.47020196, grad/param norm = 8.2767e-02, time/batch = 0.1097s	
decayed learning rate by a factor 0.97 to 0.00087875300034768	
1476/2050 (epoch 36.000), train_loss = 1.48751523, grad/param norm = 9.9855e-02, time/batch = 0.1097s	
1477/2050 (epoch 36.024), train_loss = 1.64503702, grad/param norm = 9.8448e-02, time/batch = 0.1097s	
1478/2050 (epoch 36.049), train_loss = 1.48506061, grad/param norm = 8.7706e-02, time/batch = 0.1101s	
1479/2050 (epoch 36.073), train_loss = 1.44580126, grad/param norm = 8.2878e-02, time/batch = 0.1096s	
1480/2050 (epoch 36.098), train_loss = 1.47452401, grad/param norm = 8.1836e-02, time/batch = 0.1096s	
1481/2050 (epoch 36.122), train_loss = 1.46825995, grad/param norm = 6.6883e-02, time/batch = 0.1102s	
1482/2050 (epoch 36.146), train_loss = 1.46648105, grad/param norm = 6.0427e-02, time/batch = 0.1097s	
1483/2050 (epoch 36.171), train_loss = 1.47585671, grad/param norm = 6.6580e-02, time/batch = 0.1096s	
1484/2050 (epoch 36.195), train_loss = 1.47509281, grad/param norm = 6.7869e-02, time/batch = 0.1095s	
1485/2050 (epoch 36.220), train_loss = 1.46969496, grad/param norm = 6.5150e-02, time/batch = 0.1098s	
1486/2050 (epoch 36.244), train_loss = 1.45123483, grad/param norm = 7.1689e-02, time/batch = 0.1097s	
1487/2050 (epoch 36.268), train_loss = 1.48620836, grad/param norm = 7.3141e-02, time/batch = 0.1098s	
1488/2050 (epoch 36.293), train_loss = 1.48618825, grad/param norm = 7.5086e-02, time/batch = 0.1097s	
1489/2050 (epoch 36.317), train_loss = 1.50101209, grad/param norm = 6.9903e-02, time/batch = 0.1096s	
1490/2050 (epoch 36.341), train_loss = 1.48551125, grad/param norm = 7.1005e-02, time/batch = 0.1113s	
1491/2050 (epoch 36.366), train_loss = 1.48793062, grad/param norm = 7.4526e-02, time/batch = 0.1105s	
1492/2050 (epoch 36.390), train_loss = 1.48649617, grad/param norm = 8.6727e-02, time/batch = 0.1098s	
1493/2050 (epoch 36.415), train_loss = 1.46941913, grad/param norm = 9.4171e-02, time/batch = 0.1100s	
1494/2050 (epoch 36.439), train_loss = 1.48552638, grad/param norm = 9.6747e-02, time/batch = 0.1095s	
1495/2050 (epoch 36.463), train_loss = 1.48036096, grad/param norm = 9.7709e-02, time/batch = 0.1097s	
1496/2050 (epoch 36.488), train_loss = 1.47766718, grad/param norm = 8.4913e-02, time/batch = 0.1097s	
1497/2050 (epoch 36.512), train_loss = 1.47006847, grad/param norm = 7.0572e-02, time/batch = 0.1097s	
1498/2050 (epoch 36.537), train_loss = 1.47531934, grad/param norm = 6.0882e-02, time/batch = 0.1098s	
1499/2050 (epoch 36.561), train_loss = 1.46958364, grad/param norm = 6.2879e-02, time/batch = 0.1097s	
1500/2050 (epoch 36.585), train_loss = 1.47275606, grad/param norm = 8.0775e-02, time/batch = 0.1097s	
1501/2050 (epoch 36.610), train_loss = 1.48771933, grad/param norm = 9.7010e-02, time/batch = 0.1102s	
1502/2050 (epoch 36.634), train_loss = 1.48100769, grad/param norm = 9.3240e-02, time/batch = 0.1097s	
1503/2050 (epoch 36.659), train_loss = 1.47682168, grad/param norm = 8.8031e-02, time/batch = 0.1097s	
1504/2050 (epoch 36.683), train_loss = 1.47934220, grad/param norm = 8.0124e-02, time/batch = 0.1099s	
1505/2050 (epoch 36.707), train_loss = 1.45941577, grad/param norm = 6.4994e-02, time/batch = 0.1097s	
1506/2050 (epoch 36.732), train_loss = 1.47656027, grad/param norm = 6.0981e-02, time/batch = 0.1097s	
1507/2050 (epoch 36.756), train_loss = 1.48402054, grad/param norm = 6.7395e-02, time/batch = 0.1099s	
1508/2050 (epoch 36.780), train_loss = 1.48850230, grad/param norm = 6.5472e-02, time/batch = 0.1098s	
1509/2050 (epoch 36.805), train_loss = 1.45756875, grad/param norm = 6.2675e-02, time/batch = 0.1096s	
1510/2050 (epoch 36.829), train_loss = 1.45986817, grad/param norm = 5.8848e-02, time/batch = 0.1097s	
1511/2050 (epoch 36.854), train_loss = 1.45207208, grad/param norm = 6.1375e-02, time/batch = 0.1102s	
1512/2050 (epoch 36.878), train_loss = 1.47036657, grad/param norm = 6.2048e-02, time/batch = 0.1097s	
1513/2050 (epoch 36.902), train_loss = 1.48465142, grad/param norm = 6.6697e-02, time/batch = 0.1097s	
1514/2050 (epoch 36.927), train_loss = 1.45831881, grad/param norm = 7.7956e-02, time/batch = 0.1095s	
1515/2050 (epoch 36.951), train_loss = 1.49854404, grad/param norm = 1.0350e-01, time/batch = 0.1097s	
1516/2050 (epoch 36.976), train_loss = 1.47090298, grad/param norm = 1.1494e-01, time/batch = 0.1097s	
decayed learning rate by a factor 0.97 to 0.00085239041033725	
1517/2050 (epoch 37.000), train_loss = 1.48043929, grad/param norm = 1.0021e-01, time/batch = 0.1097s	
1518/2050 (epoch 37.024), train_loss = 1.63523247, grad/param norm = 8.8035e-02, time/batch = 0.1098s	
1519/2050 (epoch 37.049), train_loss = 1.47836065, grad/param norm = 8.0276e-02, time/batch = 0.1101s	
1520/2050 (epoch 37.073), train_loss = 1.43850976, grad/param norm = 7.4631e-02, time/batch = 0.1098s	
1521/2050 (epoch 37.098), train_loss = 1.46878588, grad/param norm = 7.8768e-02, time/batch = 0.1102s	
1522/2050 (epoch 37.122), train_loss = 1.46140544, grad/param norm = 8.3754e-02, time/batch = 0.1097s	
1523/2050 (epoch 37.146), train_loss = 1.46303904, grad/param norm = 9.1246e-02, time/batch = 0.1097s	
1524/2050 (epoch 37.171), train_loss = 1.47196100, grad/param norm = 9.7076e-02, time/batch = 0.1095s	
1525/2050 (epoch 37.195), train_loss = 1.47022728, grad/param norm = 8.5596e-02, time/batch = 0.1097s	
1526/2050 (epoch 37.220), train_loss = 1.46321670, grad/param norm = 6.8210e-02, time/batch = 0.1097s	
1527/2050 (epoch 37.244), train_loss = 1.44387262, grad/param norm = 6.6615e-02, time/batch = 0.1097s	
1528/2050 (epoch 37.268), train_loss = 1.47759971, grad/param norm = 6.5568e-02, time/batch = 0.1097s	
1529/2050 (epoch 37.293), train_loss = 1.47764700, grad/param norm = 6.6903e-02, time/batch = 0.1097s	
1530/2050 (epoch 37.317), train_loss = 1.49300369, grad/param norm = 6.8055e-02, time/batch = 0.1096s	
1531/2050 (epoch 37.341), train_loss = 1.47774130, grad/param norm = 6.5658e-02, time/batch = 0.1102s	
1532/2050 (epoch 37.366), train_loss = 1.47892642, grad/param norm = 5.7892e-02, time/batch = 0.1097s	
1533/2050 (epoch 37.390), train_loss = 1.47704531, grad/param norm = 5.2804e-02, time/batch = 0.1097s	
1534/2050 (epoch 37.415), train_loss = 1.45804121, grad/param norm = 5.6976e-02, time/batch = 0.1098s	
1535/2050 (epoch 37.439), train_loss = 1.47495219, grad/param norm = 5.6502e-02, time/batch = 0.1098s	
1536/2050 (epoch 37.463), train_loss = 1.46935247, grad/param norm = 6.5800e-02, time/batch = 0.1098s	
1537/2050 (epoch 37.488), train_loss = 1.47198164, grad/param norm = 7.8159e-02, time/batch = 0.1098s	
1538/2050 (epoch 37.512), train_loss = 1.46477207, grad/param norm = 7.9533e-02, time/batch = 0.1098s	
1539/2050 (epoch 37.537), train_loss = 1.47148726, grad/param norm = 7.6051e-02, time/batch = 0.1096s	
1540/2050 (epoch 37.561), train_loss = 1.46472992, grad/param norm = 7.0319e-02, time/batch = 0.1096s	
1541/2050 (epoch 37.585), train_loss = 1.46590888, grad/param norm = 6.7374e-02, time/batch = 0.1102s	
1542/2050 (epoch 37.610), train_loss = 1.47814084, grad/param norm = 6.6236e-02, time/batch = 0.1097s	
1543/2050 (epoch 37.634), train_loss = 1.46964749, grad/param norm = 6.5263e-02, time/batch = 0.1097s	
1544/2050 (epoch 37.659), train_loss = 1.46854929, grad/param norm = 7.8220e-02, time/batch = 0.1110s	
1545/2050 (epoch 37.683), train_loss = 1.47445313, grad/param norm = 9.3048e-02, time/batch = 0.1098s	
1546/2050 (epoch 37.707), train_loss = 1.45622604, grad/param norm = 9.0635e-02, time/batch = 0.1098s	
1547/2050 (epoch 37.732), train_loss = 1.47309551, grad/param norm = 7.1934e-02, time/batch = 0.1098s	
1548/2050 (epoch 37.756), train_loss = 1.47900818, grad/param norm = 7.0154e-02, time/batch = 0.1097s	
1549/2050 (epoch 37.780), train_loss = 1.48323645, grad/param norm = 6.7985e-02, time/batch = 0.1098s	
1550/2050 (epoch 37.805), train_loss = 1.45203594, grad/param norm = 6.3889e-02, time/batch = 0.1097s	
1551/2050 (epoch 37.829), train_loss = 1.45572119, grad/param norm = 7.0210e-02, time/batch = 0.1102s	
1552/2050 (epoch 37.854), train_loss = 1.44862650, grad/param norm = 7.7473e-02, time/batch = 0.1098s	
1553/2050 (epoch 37.878), train_loss = 1.46640868, grad/param norm = 7.3596e-02, time/batch = 0.1097s	
1554/2050 (epoch 37.902), train_loss = 1.47986472, grad/param norm = 7.1381e-02, time/batch = 0.1095s	
1555/2050 (epoch 37.927), train_loss = 1.45238308, grad/param norm = 7.6343e-02, time/batch = 0.1097s	
1556/2050 (epoch 37.951), train_loss = 1.49108065, grad/param norm = 9.3986e-02, time/batch = 0.1097s	
1557/2050 (epoch 37.976), train_loss = 1.46211412, grad/param norm = 1.1579e-01, time/batch = 0.1097s	
decayed learning rate by a factor 0.97 to 0.00082681869802713	
1558/2050 (epoch 38.000), train_loss = 1.47719678, grad/param norm = 1.2203e-01, time/batch = 0.1098s	
1559/2050 (epoch 38.024), train_loss = 1.63274562, grad/param norm = 1.0367e-01, time/batch = 0.1096s	
1560/2050 (epoch 38.049), train_loss = 1.47305802, grad/param norm = 8.2881e-02, time/batch = 0.1100s	
1561/2050 (epoch 38.073), train_loss = 1.43243105, grad/param norm = 7.8209e-02, time/batch = 0.1102s	
1562/2050 (epoch 38.098), train_loss = 1.46310085, grad/param norm = 8.2187e-02, time/batch = 0.1096s	
1563/2050 (epoch 38.122), train_loss = 1.45485338, grad/param norm = 7.9132e-02, time/batch = 0.1097s	
1564/2050 (epoch 38.146), train_loss = 1.45566066, grad/param norm = 8.0734e-02, time/batch = 0.1095s	
1565/2050 (epoch 38.171), train_loss = 1.46397489, grad/param norm = 8.1176e-02, time/batch = 0.1109s	
1566/2050 (epoch 38.195), train_loss = 1.46220031, grad/param norm = 6.8272e-02, time/batch = 0.1101s	
1567/2050 (epoch 38.220), train_loss = 1.45581134, grad/param norm = 5.2110e-02, time/batch = 0.1099s	
1568/2050 (epoch 38.244), train_loss = 1.43636545, grad/param norm = 5.2321e-02, time/batch = 0.1100s	
1569/2050 (epoch 38.268), train_loss = 1.47056217, grad/param norm = 5.4101e-02, time/batch = 0.1096s	
1570/2050 (epoch 38.293), train_loss = 1.47014336, grad/param norm = 5.3134e-02, time/batch = 0.1097s	
1571/2050 (epoch 38.317), train_loss = 1.48501658, grad/param norm = 4.8801e-02, time/batch = 0.1106s	
1572/2050 (epoch 38.341), train_loss = 1.47054225, grad/param norm = 5.5188e-02, time/batch = 0.1097s	
1573/2050 (epoch 38.366), train_loss = 1.47406553, grad/param norm = 6.7076e-02, time/batch = 0.1097s	
1574/2050 (epoch 38.390), train_loss = 1.47343653, grad/param norm = 8.0964e-02, time/batch = 0.1096s	
1575/2050 (epoch 38.415), train_loss = 1.45589533, grad/param norm = 8.4489e-02, time/batch = 0.1098s	
1576/2050 (epoch 38.439), train_loss = 1.47331386, grad/param norm = 8.7614e-02, time/batch = 0.1098s	
1577/2050 (epoch 38.463), train_loss = 1.46645373, grad/param norm = 8.3751e-02, time/batch = 0.1097s	
1578/2050 (epoch 38.488), train_loss = 1.46452426, grad/param norm = 7.1630e-02, time/batch = 0.1098s	
1579/2050 (epoch 38.512), train_loss = 1.45723772, grad/param norm = 6.2644e-02, time/batch = 0.1096s	
1580/2050 (epoch 38.537), train_loss = 1.46377638, grad/param norm = 5.9990e-02, time/batch = 0.1097s	
1581/2050 (epoch 38.561), train_loss = 1.45875149, grad/param norm = 7.1355e-02, time/batch = 0.1103s	
1582/2050 (epoch 38.585), train_loss = 1.46169730, grad/param norm = 7.6986e-02, time/batch = 0.1098s	
1583/2050 (epoch 38.610), train_loss = 1.47217219, grad/param norm = 7.1912e-02, time/batch = 0.1097s	
1584/2050 (epoch 38.634), train_loss = 1.46415339, grad/param norm = 6.5623e-02, time/batch = 0.1096s	
1585/2050 (epoch 38.659), train_loss = 1.46093631, grad/param norm = 6.1894e-02, time/batch = 0.1098s	
1586/2050 (epoch 38.683), train_loss = 1.46461354, grad/param norm = 5.9723e-02, time/batch = 0.1102s	
1587/2050 (epoch 38.707), train_loss = 1.44650691, grad/param norm = 6.8783e-02, time/batch = 0.1097s	
1588/2050 (epoch 38.732), train_loss = 1.46573915, grad/param norm = 7.9840e-02, time/batch = 0.1098s	
1589/2050 (epoch 38.756), train_loss = 1.47436510, grad/param norm = 9.2943e-02, time/batch = 0.1097s	
1590/2050 (epoch 38.780), train_loss = 1.48046392, grad/param norm = 9.2415e-02, time/batch = 0.1100s	
1591/2050 (epoch 38.805), train_loss = 1.44759742, grad/param norm = 7.9630e-02, time/batch = 0.1102s	
1592/2050 (epoch 38.829), train_loss = 1.44918382, grad/param norm = 7.0758e-02, time/batch = 0.1096s	
1593/2050 (epoch 38.854), train_loss = 1.44121363, grad/param norm = 7.6164e-02, time/batch = 0.1097s	
1594/2050 (epoch 38.878), train_loss = 1.45998256, grad/param norm = 7.3696e-02, time/batch = 0.1095s	
1595/2050 (epoch 38.902), train_loss = 1.47238977, grad/param norm = 6.3384e-02, time/batch = 0.1096s	
1596/2050 (epoch 38.927), train_loss = 1.44547418, grad/param norm = 6.2535e-02, time/batch = 0.1098s	
1597/2050 (epoch 38.951), train_loss = 1.48202357, grad/param norm = 7.1993e-02, time/batch = 0.1097s	
1598/2050 (epoch 38.976), train_loss = 1.45097859, grad/param norm = 7.5364e-02, time/batch = 0.1098s	
decayed learning rate by a factor 0.97 to 0.00080201413708631	
1599/2050 (epoch 39.000), train_loss = 1.46527706, grad/param norm = 7.8590e-02, time/batch = 0.1096s	
1600/2050 (epoch 39.024), train_loss = 1.62421146, grad/param norm = 8.0129e-02, time/batch = 0.1097s	
1601/2050 (epoch 39.049), train_loss = 1.46441264, grad/param norm = 7.5271e-02, time/batch = 0.1107s	
1602/2050 (epoch 39.073), train_loss = 1.42499641, grad/param norm = 6.3626e-02, time/batch = 0.1096s	
1603/2050 (epoch 39.098), train_loss = 1.45437870, grad/param norm = 5.8506e-02, time/batch = 0.1097s	
1604/2050 (epoch 39.122), train_loss = 1.44644206, grad/param norm = 5.0215e-02, time/batch = 0.1096s	
1605/2050 (epoch 39.146), train_loss = 1.44716898, grad/param norm = 5.2098e-02, time/batch = 0.1097s	
1606/2050 (epoch 39.171), train_loss = 1.45565622, grad/param norm = 5.5176e-02, time/batch = 0.1097s	
1607/2050 (epoch 39.195), train_loss = 1.45476791, grad/param norm = 4.9198e-02, time/batch = 0.1097s	
1608/2050 (epoch 39.220), train_loss = 1.44991348, grad/param norm = 4.7580e-02, time/batch = 0.1098s	
1609/2050 (epoch 39.244), train_loss = 1.43078872, grad/param norm = 5.3692e-02, time/batch = 0.1096s	
1610/2050 (epoch 39.268), train_loss = 1.46508458, grad/param norm = 5.5539e-02, time/batch = 0.1097s	
1611/2050 (epoch 39.293), train_loss = 1.46460523, grad/param norm = 5.4435e-02, time/batch = 0.1103s	
1612/2050 (epoch 39.317), train_loss = 1.48008988, grad/param norm = 5.6317e-02, time/batch = 0.1100s	
1613/2050 (epoch 39.341), train_loss = 1.46696445, grad/param norm = 7.2278e-02, time/batch = 0.1097s	
1614/2050 (epoch 39.366), train_loss = 1.47180449, grad/param norm = 8.5839e-02, time/batch = 0.1095s	
1615/2050 (epoch 39.390), train_loss = 1.47045458, grad/param norm = 9.6582e-02, time/batch = 0.1097s	
1616/2050 (epoch 39.415), train_loss = 1.45189851, grad/param norm = 9.7878e-02, time/batch = 0.1100s	
1617/2050 (epoch 39.439), train_loss = 1.46839743, grad/param norm = 1.0345e-01, time/batch = 0.1097s	
1618/2050 (epoch 39.463), train_loss = 1.46364664, grad/param norm = 1.0467e-01, time/batch = 0.1098s	
1619/2050 (epoch 39.488), train_loss = 1.45973071, grad/param norm = 8.7251e-02, time/batch = 0.1096s	
1620/2050 (epoch 39.512), train_loss = 1.45247494, grad/param norm = 6.6806e-02, time/batch = 0.1097s	
1621/2050 (epoch 39.537), train_loss = 1.45731959, grad/param norm = 5.4211e-02, time/batch = 0.1102s	
1622/2050 (epoch 39.561), train_loss = 1.45112633, grad/param norm = 5.6243e-02, time/batch = 0.1097s	
1623/2050 (epoch 39.585), train_loss = 1.45370551, grad/param norm = 7.0404e-02, time/batch = 0.1096s	
1624/2050 (epoch 39.610), train_loss = 1.46717630, grad/param norm = 8.2330e-02, time/batch = 0.1096s	
1625/2050 (epoch 39.634), train_loss = 1.45989803, grad/param norm = 7.9322e-02, time/batch = 0.1097s	
1626/2050 (epoch 39.659), train_loss = 1.45703496, grad/param norm = 7.8031e-02, time/batch = 0.1098s	
1627/2050 (epoch 39.683), train_loss = 1.46064140, grad/param norm = 7.5143e-02, time/batch = 0.1103s	
1628/2050 (epoch 39.707), train_loss = 1.44097319, grad/param norm = 6.1451e-02, time/batch = 0.1097s	
1629/2050 (epoch 39.732), train_loss = 1.45739782, grad/param norm = 5.5204e-02, time/batch = 0.1097s	
1630/2050 (epoch 39.756), train_loss = 1.46525233, grad/param norm = 6.0746e-02, time/batch = 0.1098s	
1631/2050 (epoch 39.780), train_loss = 1.47180968, grad/param norm = 6.9910e-02, time/batch = 0.1102s	
1632/2050 (epoch 39.805), train_loss = 1.44313780, grad/param norm = 8.2473e-02, time/batch = 0.1097s	
1633/2050 (epoch 39.829), train_loss = 1.44867787, grad/param norm = 9.7949e-02, time/batch = 0.1097s	
1634/2050 (epoch 39.854), train_loss = 1.44128490, grad/param norm = 1.0013e-01, time/batch = 0.1095s	
1635/2050 (epoch 39.878), train_loss = 1.45788648, grad/param norm = 9.1540e-02, time/batch = 0.1097s	
1636/2050 (epoch 39.902), train_loss = 1.47131293, grad/param norm = 9.1256e-02, time/batch = 0.1098s	
1637/2050 (epoch 39.927), train_loss = 1.44357579, grad/param norm = 8.4384e-02, time/batch = 0.1097s	
1638/2050 (epoch 39.951), train_loss = 1.47653448, grad/param norm = 7.4679e-02, time/batch = 0.1098s	
1639/2050 (epoch 39.976), train_loss = 1.44425189, grad/param norm = 6.7067e-02, time/batch = 0.1099s	
decayed learning rate by a factor 0.97 to 0.00077795371297373	
1640/2050 (epoch 40.000), train_loss = 1.45708175, grad/param norm = 5.3959e-02, time/batch = 0.1098s	
1641/2050 (epoch 40.024), train_loss = 1.61737669, grad/param norm = 6.2093e-02, time/batch = 0.1102s	
1642/2050 (epoch 40.049), train_loss = 1.45801290, grad/param norm = 6.6050e-02, time/batch = 0.1103s	
1643/2050 (epoch 40.073), train_loss = 1.41999911, grad/param norm = 7.0770e-02, time/batch = 0.1097s	
1644/2050 (epoch 40.098), train_loss = 1.45110989, grad/param norm = 7.6291e-02, time/batch = 0.1096s	
1645/2050 (epoch 40.122), train_loss = 1.44260952, grad/param norm = 7.0334e-02, time/batch = 0.1097s	
1646/2050 (epoch 40.146), train_loss = 1.44342503, grad/param norm = 6.9897e-02, time/batch = 0.1098s	
1647/2050 (epoch 40.171), train_loss = 1.45178814, grad/param norm = 7.4917e-02, time/batch = 0.1097s	
1648/2050 (epoch 40.195), train_loss = 1.45079972, grad/param norm = 6.8593e-02, time/batch = 0.1098s	
1649/2050 (epoch 40.220), train_loss = 1.44512224, grad/param norm = 6.0901e-02, time/batch = 0.1098s	
1650/2050 (epoch 40.244), train_loss = 1.42631699, grad/param norm = 6.5346e-02, time/batch = 0.1097s	
1651/2050 (epoch 40.268), train_loss = 1.46047455, grad/param norm = 6.6055e-02, time/batch = 0.1102s	
1652/2050 (epoch 40.293), train_loss = 1.46019811, grad/param norm = 6.6277e-02, time/batch = 0.1097s	
1653/2050 (epoch 40.317), train_loss = 1.47504144, grad/param norm = 6.7168e-02, time/batch = 0.1102s	
1654/2050 (epoch 40.341), train_loss = 1.46015546, grad/param norm = 6.5190e-02, time/batch = 0.1096s	
1655/2050 (epoch 40.366), train_loss = 1.46143070, grad/param norm = 5.4621e-02, time/batch = 0.1097s	
1656/2050 (epoch 40.390), train_loss = 1.45969950, grad/param norm = 4.9382e-02, time/batch = 0.1097s	
1657/2050 (epoch 40.415), train_loss = 1.44051886, grad/param norm = 5.4347e-02, time/batch = 0.1099s	
1658/2050 (epoch 40.439), train_loss = 1.45747954, grad/param norm = 5.3162e-02, time/batch = 0.1098s	
1659/2050 (epoch 40.463), train_loss = 1.45163818, grad/param norm = 5.9004e-02, time/batch = 0.1096s	
1660/2050 (epoch 40.488), train_loss = 1.45293386, grad/param norm = 6.8267e-02, time/batch = 0.1097s	
1661/2050 (epoch 40.512), train_loss = 1.44595981, grad/param norm = 6.9861e-02, time/batch = 0.1102s	
1662/2050 (epoch 40.537), train_loss = 1.45318230, grad/param norm = 6.6208e-02, time/batch = 0.1097s	
1663/2050 (epoch 40.561), train_loss = 1.44634914, grad/param norm = 6.4458e-02, time/batch = 0.1097s	
1664/2050 (epoch 40.585), train_loss = 1.44817785, grad/param norm = 6.7830e-02, time/batch = 0.1096s	
1665/2050 (epoch 40.610), train_loss = 1.46071218, grad/param norm = 7.2508e-02, time/batch = 0.1097s	
1666/2050 (epoch 40.634), train_loss = 1.45268637, grad/param norm = 7.2071e-02, time/batch = 0.1098s	
1667/2050 (epoch 40.659), train_loss = 1.45151695, grad/param norm = 8.1628e-02, time/batch = 0.1098s	
1668/2050 (epoch 40.683), train_loss = 1.45590064, grad/param norm = 7.8908e-02, time/batch = 0.1102s	
1669/2050 (epoch 40.707), train_loss = 1.43603738, grad/param norm = 6.9588e-02, time/batch = 0.1096s	
1670/2050 (epoch 40.732), train_loss = 1.45360483, grad/param norm = 6.2353e-02, time/batch = 0.1097s	
1671/2050 (epoch 40.756), train_loss = 1.45989318, grad/param norm = 6.5233e-02, time/batch = 0.1102s	
1672/2050 (epoch 40.780), train_loss = 1.46625149, grad/param norm = 6.6045e-02, time/batch = 0.1097s	
1673/2050 (epoch 40.805), train_loss = 1.43464347, grad/param norm = 6.2784e-02, time/batch = 0.1097s	
1674/2050 (epoch 40.829), train_loss = 1.43777185, grad/param norm = 6.4861e-02, time/batch = 0.1095s	
1675/2050 (epoch 40.854), train_loss = 1.43065027, grad/param norm = 8.3033e-02, time/batch = 0.1097s	
1676/2050 (epoch 40.878), train_loss = 1.45270663, grad/param norm = 9.7529e-02, time/batch = 0.1098s	
1677/2050 (epoch 40.902), train_loss = 1.46661729, grad/param norm = 9.3274e-02, time/batch = 0.1098s	
1678/2050 (epoch 40.927), train_loss = 1.43863477, grad/param norm = 8.5391e-02, time/batch = 0.1097s	
1679/2050 (epoch 40.951), train_loss = 1.47374032, grad/param norm = 8.1656e-02, time/batch = 0.1096s	
1680/2050 (epoch 40.976), train_loss = 1.43990216, grad/param norm = 7.5541e-02, time/batch = 0.1097s	
decayed learning rate by a factor 0.97 to 0.00075461510158451	
1681/2050 (epoch 41.000), train_loss = 1.45636124, grad/param norm = 8.4164e-02, time/batch = 0.1102s	
1682/2050 (epoch 41.024), train_loss = 1.61796598, grad/param norm = 8.3279e-02, time/batch = 0.1097s	
1683/2050 (epoch 41.049), train_loss = 1.45333890, grad/param norm = 6.8478e-02, time/batch = 0.1099s	
1684/2050 (epoch 41.073), train_loss = 1.41407144, grad/param norm = 6.5951e-02, time/batch = 0.1095s	
1685/2050 (epoch 41.098), train_loss = 1.44468940, grad/param norm = 7.0413e-02, time/batch = 0.1097s	
1686/2050 (epoch 41.122), train_loss = 1.43645009, grad/param norm = 5.6848e-02, time/batch = 0.1099s	
1687/2050 (epoch 41.146), train_loss = 1.43676341, grad/param norm = 5.0169e-02, time/batch = 0.1097s	
1688/2050 (epoch 41.171), train_loss = 1.44452244, grad/param norm = 5.3110e-02, time/batch = 0.1098s	
1689/2050 (epoch 41.195), train_loss = 1.44401333, grad/param norm = 5.1454e-02, time/batch = 0.1097s	
1690/2050 (epoch 41.220), train_loss = 1.43904194, grad/param norm = 5.0118e-02, time/batch = 0.1097s	
1691/2050 (epoch 41.244), train_loss = 1.42007784, grad/param norm = 5.4884e-02, time/batch = 0.1102s	
1692/2050 (epoch 41.268), train_loss = 1.45426181, grad/param norm = 5.4978e-02, time/batch = 0.1097s	
1693/2050 (epoch 41.293), train_loss = 1.45397671, grad/param norm = 5.6729e-02, time/batch = 0.1097s	
1694/2050 (epoch 41.317), train_loss = 1.46897167, grad/param norm = 5.9531e-02, time/batch = 0.1100s	
1695/2050 (epoch 41.341), train_loss = 1.45469096, grad/param norm = 6.4464e-02, time/batch = 0.1097s	
1696/2050 (epoch 41.366), train_loss = 1.45682153, grad/param norm = 6.1289e-02, time/batch = 0.1099s	
1697/2050 (epoch 41.390), train_loss = 1.45549783, grad/param norm = 6.1793e-02, time/batch = 0.1097s	
1698/2050 (epoch 41.415), train_loss = 1.43744245, grad/param norm = 7.2750e-02, time/batch = 0.1099s	
1699/2050 (epoch 41.439), train_loss = 1.45484137, grad/param norm = 7.7015e-02, time/batch = 0.1096s	
1700/2050 (epoch 41.463), train_loss = 1.44995512, grad/param norm = 8.4495e-02, time/batch = 0.1098s	
1701/2050 (epoch 41.488), train_loss = 1.45132268, grad/param norm = 9.2191e-02, time/batch = 0.1102s	
1702/2050 (epoch 41.512), train_loss = 1.44367896, grad/param norm = 8.8399e-02, time/batch = 0.1097s	
1703/2050 (epoch 41.537), train_loss = 1.44965591, grad/param norm = 7.9316e-02, time/batch = 0.1097s	
1704/2050 (epoch 41.561), train_loss = 1.44259576, grad/param norm = 7.9372e-02, time/batch = 0.1096s	
1705/2050 (epoch 41.585), train_loss = 1.44527331, grad/param norm = 8.4098e-02, time/batch = 0.1097s	
1706/2050 (epoch 41.610), train_loss = 1.45642533, grad/param norm = 8.0907e-02, time/batch = 0.1097s	
1707/2050 (epoch 41.634), train_loss = 1.44757627, grad/param norm = 6.9731e-02, time/batch = 0.1097s	
1708/2050 (epoch 41.659), train_loss = 1.44427574, grad/param norm = 6.2409e-02, time/batch = 0.1098s	
1709/2050 (epoch 41.683), train_loss = 1.44804404, grad/param norm = 5.8442e-02, time/batch = 0.1097s	
1710/2050 (epoch 41.707), train_loss = 1.42983019, grad/param norm = 6.9221e-02, time/batch = 0.1097s	
1711/2050 (epoch 41.732), train_loss = 1.44832675, grad/param norm = 8.1987e-02, time/batch = 0.1103s	
1712/2050 (epoch 41.756), train_loss = 1.45745911, grad/param norm = 8.6418e-02, time/batch = 0.1096s	
1713/2050 (epoch 41.780), train_loss = 1.46319485, grad/param norm = 7.6825e-02, time/batch = 0.1097s	
1714/2050 (epoch 41.805), train_loss = 1.43075893, grad/param norm = 6.7764e-02, time/batch = 0.1108s	
1715/2050 (epoch 41.829), train_loss = 1.43270585, grad/param norm = 6.1201e-02, time/batch = 0.1098s	
1716/2050 (epoch 41.854), train_loss = 1.42327793, grad/param norm = 6.4411e-02, time/batch = 0.1100s	
1717/2050 (epoch 41.878), train_loss = 1.44303140, grad/param norm = 5.9700e-02, time/batch = 0.1098s	
1718/2050 (epoch 41.902), train_loss = 1.45592152, grad/param norm = 5.0376e-02, time/batch = 0.1098s	
1719/2050 (epoch 41.927), train_loss = 1.42903236, grad/param norm = 5.0550e-02, time/batch = 0.1097s	
1720/2050 (epoch 41.951), train_loss = 1.46407856, grad/param norm = 5.9876e-02, time/batch = 0.1098s	
1721/2050 (epoch 41.976), train_loss = 1.43300454, grad/param norm = 6.2295e-02, time/batch = 0.1103s	
decayed learning rate by a factor 0.97 to 0.00073197664853698	
1722/2050 (epoch 42.000), train_loss = 1.44796726, grad/param norm = 6.2548e-02, time/batch = 0.1097s	
1723/2050 (epoch 42.024), train_loss = 1.61000391, grad/param norm = 6.7359e-02, time/batch = 0.1105s	
1724/2050 (epoch 42.049), train_loss = 1.44832461, grad/param norm = 6.7682e-02, time/batch = 0.1099s	
1725/2050 (epoch 42.073), train_loss = 1.40914608, grad/param norm = 6.2747e-02, time/batch = 0.1097s	
1726/2050 (epoch 42.098), train_loss = 1.43957919, grad/param norm = 6.3084e-02, time/batch = 0.1098s	
1727/2050 (epoch 42.122), train_loss = 1.43106284, grad/param norm = 5.8959e-02, time/batch = 0.1097s	
1728/2050 (epoch 42.146), train_loss = 1.43278486, grad/param norm = 6.4230e-02, time/batch = 0.1097s	
1729/2050 (epoch 42.171), train_loss = 1.44139934, grad/param norm = 7.2877e-02, time/batch = 0.1096s	
1730/2050 (epoch 42.195), train_loss = 1.44102137, grad/param norm = 6.9013e-02, time/batch = 0.1098s	
1731/2050 (epoch 42.220), train_loss = 1.43613107, grad/param norm = 6.7237e-02, time/batch = 0.1102s	
1732/2050 (epoch 42.244), train_loss = 1.41774595, grad/param norm = 7.2258e-02, time/batch = 0.1098s	
1733/2050 (epoch 42.268), train_loss = 1.45100677, grad/param norm = 7.1166e-02, time/batch = 0.1097s	
1734/2050 (epoch 42.293), train_loss = 1.45035613, grad/param norm = 7.0004e-02, time/batch = 0.1415s	
1735/2050 (epoch 42.317), train_loss = 1.46504482, grad/param norm = 7.0769e-02, time/batch = 0.2071s	
1736/2050 (epoch 42.341), train_loss = 1.45024551, grad/param norm = 6.8116e-02, time/batch = 0.2116s	
1737/2050 (epoch 42.366), train_loss = 1.45239306, grad/param norm = 6.2106e-02, time/batch = 0.1748s	
1738/2050 (epoch 42.390), train_loss = 1.45165036, grad/param norm = 6.7076e-02, time/batch = 0.1354s	
1739/2050 (epoch 42.415), train_loss = 1.43308818, grad/param norm = 7.3541e-02, time/batch = 0.1354s	
1740/2050 (epoch 42.439), train_loss = 1.45106178, grad/param norm = 8.1007e-02, time/batch = 0.1351s	
1741/2050 (epoch 42.463), train_loss = 1.44569807, grad/param norm = 8.4362e-02, time/batch = 0.1262s	
1742/2050 (epoch 42.488), train_loss = 1.44345088, grad/param norm = 7.1251e-02, time/batch = 0.1237s	
1743/2050 (epoch 42.512), train_loss = 1.43566090, grad/param norm = 5.7318e-02, time/batch = 0.1282s	
1744/2050 (epoch 42.537), train_loss = 1.44237711, grad/param norm = 5.6045e-02, time/batch = 0.1244s	
1745/2050 (epoch 42.561), train_loss = 1.43691787, grad/param norm = 6.9314e-02, time/batch = 0.1237s	
1746/2050 (epoch 42.585), train_loss = 1.43921321, grad/param norm = 7.2050e-02, time/batch = 0.1240s	
1747/2050 (epoch 42.610), train_loss = 1.44966058, grad/param norm = 6.2958e-02, time/batch = 0.1221s	
1748/2050 (epoch 42.634), train_loss = 1.44111307, grad/param norm = 5.7216e-02, time/batch = 0.1137s	
1749/2050 (epoch 42.659), train_loss = 1.43856250, grad/param norm = 5.3256e-02, time/batch = 0.1135s	
1750/2050 (epoch 42.683), train_loss = 1.44330369, grad/param norm = 5.8048e-02, time/batch = 0.1133s	
1751/2050 (epoch 42.707), train_loss = 1.42580480, grad/param norm = 7.5573e-02, time/batch = 0.1150s	
1752/2050 (epoch 42.732), train_loss = 1.44410933, grad/param norm = 8.7738e-02, time/batch = 0.1134s	
1753/2050 (epoch 42.756), train_loss = 1.45265547, grad/param norm = 8.2052e-02, time/batch = 0.1132s	
1754/2050 (epoch 42.780), train_loss = 1.45825131, grad/param norm = 7.3472e-02, time/batch = 0.1131s	
1755/2050 (epoch 42.805), train_loss = 1.42595780, grad/param norm = 6.7391e-02, time/batch = 0.1133s	
1756/2050 (epoch 42.829), train_loss = 1.42843391, grad/param norm = 6.5568e-02, time/batch = 0.1134s	
1757/2050 (epoch 42.854), train_loss = 1.41918742, grad/param norm = 6.8502e-02, time/batch = 0.1133s	
1758/2050 (epoch 42.878), train_loss = 1.43841753, grad/param norm = 5.9534e-02, time/batch = 0.1134s	
1759/2050 (epoch 42.902), train_loss = 1.45124632, grad/param norm = 4.8414e-02, time/batch = 0.1135s	
1760/2050 (epoch 42.927), train_loss = 1.42429637, grad/param norm = 4.8278e-02, time/batch = 0.1133s	
1761/2050 (epoch 42.951), train_loss = 1.45900186, grad/param norm = 5.7961e-02, time/batch = 0.1146s	
1762/2050 (epoch 42.976), train_loss = 1.42793654, grad/param norm = 6.1553e-02, time/batch = 0.1135s	
decayed learning rate by a factor 0.97 to 0.00071001734908087	
1763/2050 (epoch 43.000), train_loss = 1.44325845, grad/param norm = 6.3617e-02, time/batch = 0.1133s	
1764/2050 (epoch 43.024), train_loss = 1.60651836, grad/param norm = 6.8884e-02, time/batch = 0.1131s	
1765/2050 (epoch 43.049), train_loss = 1.44431810, grad/param norm = 7.0503e-02, time/batch = 0.1134s	
1766/2050 (epoch 43.073), train_loss = 1.40529703, grad/param norm = 6.7993e-02, time/batch = 0.1134s	
1767/2050 (epoch 43.098), train_loss = 1.43564394, grad/param norm = 6.9571e-02, time/batch = 0.1133s	
1768/2050 (epoch 43.122), train_loss = 1.42717443, grad/param norm = 6.7389e-02, time/batch = 0.1134s	
1769/2050 (epoch 43.146), train_loss = 1.42949611, grad/param norm = 7.3891e-02, time/batch = 0.1133s	
1770/2050 (epoch 43.171), train_loss = 1.43782268, grad/param norm = 8.2220e-02, time/batch = 0.1134s	
1771/2050 (epoch 43.195), train_loss = 1.43732018, grad/param norm = 7.6594e-02, time/batch = 0.1146s	
1772/2050 (epoch 43.220), train_loss = 1.43171252, grad/param norm = 7.0963e-02, time/batch = 0.1133s	
1773/2050 (epoch 43.244), train_loss = 1.41262716, grad/param norm = 6.9962e-02, time/batch = 0.1133s	
1774/2050 (epoch 43.268), train_loss = 1.44549177, grad/param norm = 6.4931e-02, time/batch = 0.1130s	
1775/2050 (epoch 43.293), train_loss = 1.44468771, grad/param norm = 6.3240e-02, time/batch = 0.1134s	
1776/2050 (epoch 43.317), train_loss = 1.45916065, grad/param norm = 6.3840e-02, time/batch = 0.1135s	
1777/2050 (epoch 43.341), train_loss = 1.44450071, grad/param norm = 6.2378e-02, time/batch = 0.1132s	
1778/2050 (epoch 43.366), train_loss = 1.44658126, grad/param norm = 5.5925e-02, time/batch = 0.1134s	
1779/2050 (epoch 43.390), train_loss = 1.44527983, grad/param norm = 5.7651e-02, time/batch = 0.1137s	
1780/2050 (epoch 43.415), train_loss = 1.42651361, grad/param norm = 6.1690e-02, time/batch = 0.1137s	
1781/2050 (epoch 43.439), train_loss = 1.44319251, grad/param norm = 5.9913e-02, time/batch = 0.1145s	
1782/2050 (epoch 43.463), train_loss = 1.43753355, grad/param norm = 6.1681e-02, time/batch = 0.1133s	
1783/2050 (epoch 43.488), train_loss = 1.43694167, grad/param norm = 6.3259e-02, time/batch = 0.1132s	
1784/2050 (epoch 43.512), train_loss = 1.43030141, grad/param norm = 5.9803e-02, time/batch = 0.1132s	
1785/2050 (epoch 43.537), train_loss = 1.43773861, grad/param norm = 5.8990e-02, time/batch = 0.1135s	
1786/2050 (epoch 43.561), train_loss = 1.43169261, grad/param norm = 6.7328e-02, time/batch = 0.1136s	
1787/2050 (epoch 43.585), train_loss = 1.43423227, grad/param norm = 7.3672e-02, time/batch = 0.1135s	
1788/2050 (epoch 43.610), train_loss = 1.44563182, grad/param norm = 7.1181e-02, time/batch = 0.1133s	
1789/2050 (epoch 43.634), train_loss = 1.43669470, grad/param norm = 6.3367e-02, time/batch = 0.1135s	
1790/2050 (epoch 43.659), train_loss = 1.43443233, grad/param norm = 6.1500e-02, time/batch = 0.1632s	
1791/2050 (epoch 43.683), train_loss = 1.43907003, grad/param norm = 6.0628e-02, time/batch = 0.1813s	
1792/2050 (epoch 43.707), train_loss = 1.42023217, grad/param norm = 6.5506e-02, time/batch = 0.1780s	
1793/2050 (epoch 43.732), train_loss = 1.43765903, grad/param norm = 7.1618e-02, time/batch = 0.1510s	
1794/2050 (epoch 43.756), train_loss = 1.44630515, grad/param norm = 7.5996e-02, time/batch = 0.1200s	
1795/2050 (epoch 43.780), train_loss = 1.45242814, grad/param norm = 6.7828e-02, time/batch = 0.1201s	
1796/2050 (epoch 43.805), train_loss = 1.42036379, grad/param norm = 5.9471e-02, time/batch = 0.1209s	
1797/2050 (epoch 43.829), train_loss = 1.42335155, grad/param norm = 5.9549e-02, time/batch = 0.1157s	
1798/2050 (epoch 43.854), train_loss = 1.41438342, grad/param norm = 6.6791e-02, time/batch = 0.1156s	
1799/2050 (epoch 43.878), train_loss = 1.43427473, grad/param norm = 6.1713e-02, time/batch = 0.1213s	
1800/2050 (epoch 43.902), train_loss = 1.44761123, grad/param norm = 5.3641e-02, time/batch = 0.1701s	
1801/2050 (epoch 43.927), train_loss = 1.42114086, grad/param norm = 5.7082e-02, time/batch = 0.1736s	
1802/2050 (epoch 43.951), train_loss = 1.45595810, grad/param norm = 6.8782e-02, time/batch = 0.1649s	
1803/2050 (epoch 43.976), train_loss = 1.42546339, grad/param norm = 7.5705e-02, time/batch = 0.1214s	
decayed learning rate by a factor 0.97 to 0.00068871682860844	
1804/2050 (epoch 44.000), train_loss = 1.44111381, grad/param norm = 7.9343e-02, time/batch = 0.1150s	
1805/2050 (epoch 44.024), train_loss = 1.60486628, grad/param norm = 7.9674e-02, time/batch = 0.1147s	
1806/2050 (epoch 44.049), train_loss = 1.44138299, grad/param norm = 7.5888e-02, time/batch = 0.1143s	
1807/2050 (epoch 44.073), train_loss = 1.40201181, grad/param norm = 7.5251e-02, time/batch = 0.1126s	
1808/2050 (epoch 44.098), train_loss = 1.43259294, grad/param norm = 7.8223e-02, time/batch = 0.1125s	
1809/2050 (epoch 44.122), train_loss = 1.42325511, grad/param norm = 7.3953e-02, time/batch = 0.1126s	
1810/2050 (epoch 44.146), train_loss = 1.42516720, grad/param norm = 7.4301e-02, time/batch = 0.1128s	
1811/2050 (epoch 44.171), train_loss = 1.43232401, grad/param norm = 7.3546e-02, time/batch = 0.1131s	
1812/2050 (epoch 44.195), train_loss = 1.43107321, grad/param norm = 6.0338e-02, time/batch = 0.1125s	
1813/2050 (epoch 44.220), train_loss = 1.42545126, grad/param norm = 5.3210e-02, time/batch = 0.1124s	
1814/2050 (epoch 44.244), train_loss = 1.40626323, grad/param norm = 5.3329e-02, time/batch = 0.1100s	
1815/2050 (epoch 44.268), train_loss = 1.43949602, grad/param norm = 4.8558e-02, time/batch = 0.1097s	
1816/2050 (epoch 44.293), train_loss = 1.43842915, grad/param norm = 4.4816e-02, time/batch = 0.1098s	
1817/2050 (epoch 44.317), train_loss = 1.45293133, grad/param norm = 4.3240e-02, time/batch = 0.1097s	
1818/2050 (epoch 44.341), train_loss = 1.43885225, grad/param norm = 4.9398e-02, time/batch = 0.1098s	
1819/2050 (epoch 44.366), train_loss = 1.44229021, grad/param norm = 5.4619e-02, time/batch = 0.1096s	
1820/2050 (epoch 44.390), train_loss = 1.44154031, grad/param norm = 6.5086e-02, time/batch = 0.1097s	
1821/2050 (epoch 44.415), train_loss = 1.42304074, grad/param norm = 6.9008e-02, time/batch = 0.1104s	
1822/2050 (epoch 44.439), train_loss = 1.44030977, grad/param norm = 7.2988e-02, time/batch = 0.1097s	
1823/2050 (epoch 44.463), train_loss = 1.43497066, grad/param norm = 7.4234e-02, time/batch = 0.1097s	
1824/2050 (epoch 44.488), train_loss = 1.43240278, grad/param norm = 6.4300e-02, time/batch = 0.1096s	
1825/2050 (epoch 44.512), train_loss = 1.42571673, grad/param norm = 5.6143e-02, time/batch = 0.1098s	
1826/2050 (epoch 44.537), train_loss = 1.43360399, grad/param norm = 5.6631e-02, time/batch = 0.1099s	
1827/2050 (epoch 44.561), train_loss = 1.42779941, grad/param norm = 6.8994e-02, time/batch = 0.1097s	
1828/2050 (epoch 44.585), train_loss = 1.42996475, grad/param norm = 7.1587e-02, time/batch = 0.1099s	
1829/2050 (epoch 44.610), train_loss = 1.44056155, grad/param norm = 6.2916e-02, time/batch = 0.1098s	
1830/2050 (epoch 44.634), train_loss = 1.43177179, grad/param norm = 5.8058e-02, time/batch = 0.1098s	
1831/2050 (epoch 44.659), train_loss = 1.42918152, grad/param norm = 4.9524e-02, time/batch = 0.1103s	
1832/2050 (epoch 44.683), train_loss = 1.43364180, grad/param norm = 5.1136e-02, time/batch = 0.1100s	
1833/2050 (epoch 44.707), train_loss = 1.41568197, grad/param norm = 6.5568e-02, time/batch = 0.1097s	
1834/2050 (epoch 44.732), train_loss = 1.43333743, grad/param norm = 7.7201e-02, time/batch = 0.1096s	
1835/2050 (epoch 44.756), train_loss = 1.44209597, grad/param norm = 7.2398e-02, time/batch = 0.1097s	
1836/2050 (epoch 44.780), train_loss = 1.44856880, grad/param norm = 6.6025e-02, time/batch = 0.1103s	
1837/2050 (epoch 44.805), train_loss = 1.41634366, grad/param norm = 6.0464e-02, time/batch = 0.1097s	
1838/2050 (epoch 44.829), train_loss = 1.41869435, grad/param norm = 5.7679e-02, time/batch = 0.1098s	
1839/2050 (epoch 44.854), train_loss = 1.40915624, grad/param norm = 6.0473e-02, time/batch = 0.1097s	
1840/2050 (epoch 44.878), train_loss = 1.42909781, grad/param norm = 5.4095e-02, time/batch = 0.1098s	
1841/2050 (epoch 44.902), train_loss = 1.44289650, grad/param norm = 5.1719e-02, time/batch = 0.1103s	
1842/2050 (epoch 44.927), train_loss = 1.41659553, grad/param norm = 5.8555e-02, time/batch = 0.1097s	
1843/2050 (epoch 44.951), train_loss = 1.45113998, grad/param norm = 7.3040e-02, time/batch = 0.1098s	
1844/2050 (epoch 44.976), train_loss = 1.42125442, grad/param norm = 8.3668e-02, time/batch = 0.1095s	
decayed learning rate by a factor 0.97 to 0.00066805532375019	
1845/2050 (epoch 45.000), train_loss = 1.43682133, grad/param norm = 8.2365e-02, time/batch = 0.1098s	
1846/2050 (epoch 45.024), train_loss = 1.60143562, grad/param norm = 8.7615e-02, time/batch = 0.1098s	
1847/2050 (epoch 45.049), train_loss = 1.43877389, grad/param norm = 8.9427e-02, time/batch = 0.1100s	
1848/2050 (epoch 45.073), train_loss = 1.39930854, grad/param norm = 8.8900e-02, time/batch = 0.1098s	
1849/2050 (epoch 45.098), train_loss = 1.42935723, grad/param norm = 8.4748e-02, time/batch = 0.1097s	
1850/2050 (epoch 45.122), train_loss = 1.41863804, grad/param norm = 7.2679e-02, time/batch = 0.1098s	
1851/2050 (epoch 45.146), train_loss = 1.42030608, grad/param norm = 6.8573e-02, time/batch = 0.1102s	
1852/2050 (epoch 45.171), train_loss = 1.42708563, grad/param norm = 6.5518e-02, time/batch = 0.1097s	
1853/2050 (epoch 45.195), train_loss = 1.42610771, grad/param norm = 5.2218e-02, time/batch = 0.1097s	
1854/2050 (epoch 45.220), train_loss = 1.42063159, grad/param norm = 4.6019e-02, time/batch = 0.1096s	
1855/2050 (epoch 45.244), train_loss = 1.40155867, grad/param norm = 4.7550e-02, time/batch = 0.1098s	
1856/2050 (epoch 45.268), train_loss = 1.43485689, grad/param norm = 4.3320e-02, time/batch = 0.1105s	
1857/2050 (epoch 45.293), train_loss = 1.43367696, grad/param norm = 4.0642e-02, time/batch = 0.1098s	
1858/2050 (epoch 45.317), train_loss = 1.44821558, grad/param norm = 3.9795e-02, time/batch = 0.1099s	
1859/2050 (epoch 45.341), train_loss = 1.43445469, grad/param norm = 5.0764e-02, time/batch = 0.1097s	
1860/2050 (epoch 45.366), train_loss = 1.43855490, grad/param norm = 6.0296e-02, time/batch = 0.1097s	
1861/2050 (epoch 45.390), train_loss = 1.43772117, grad/param norm = 7.0609e-02, time/batch = 0.1103s	
1862/2050 (epoch 45.415), train_loss = 1.41895649, grad/param norm = 6.7682e-02, time/batch = 0.1099s	
1863/2050 (epoch 45.439), train_loss = 1.43487714, grad/param norm = 6.3072e-02, time/batch = 0.1098s	
1864/2050 (epoch 45.463), train_loss = 1.42905755, grad/param norm = 6.0748e-02, time/batch = 0.1096s	
1865/2050 (epoch 45.488), train_loss = 1.42657385, grad/param norm = 5.3360e-02, time/batch = 0.1099s	
1866/2050 (epoch 45.512), train_loss = 1.42020272, grad/param norm = 4.8056e-02, time/batch = 0.1099s	
1867/2050 (epoch 45.537), train_loss = 1.42833877, grad/param norm = 5.0219e-02, time/batch = 0.1098s	
1868/2050 (epoch 45.561), train_loss = 1.42260242, grad/param norm = 6.2466e-02, time/batch = 0.1098s	
1869/2050 (epoch 45.585), train_loss = 1.42502340, grad/param norm = 6.9386e-02, time/batch = 0.1096s	
1870/2050 (epoch 45.610), train_loss = 1.43661685, grad/param norm = 6.4353e-02, time/batch = 0.1098s	
1871/2050 (epoch 45.634), train_loss = 1.42752390, grad/param norm = 5.8139e-02, time/batch = 0.1102s	
1872/2050 (epoch 45.659), train_loss = 1.42515090, grad/param norm = 5.1178e-02, time/batch = 0.1110s	
1873/2050 (epoch 45.683), train_loss = 1.42952247, grad/param norm = 5.0213e-02, time/batch = 0.1801s	
1874/2050 (epoch 45.707), train_loss = 1.41090696, grad/param norm = 5.7174e-02, time/batch = 0.1774s	
1875/2050 (epoch 45.732), train_loss = 1.42826046, grad/param norm = 6.6283e-02, time/batch = 0.1714s	
1876/2050 (epoch 45.756), train_loss = 1.43698797, grad/param norm = 6.6262e-02, time/batch = 0.1204s	
1877/2050 (epoch 45.780), train_loss = 1.44417316, grad/param norm = 6.3496e-02, time/batch = 0.1149s	
1878/2050 (epoch 45.805), train_loss = 1.41192066, grad/param norm = 5.7429e-02, time/batch = 0.1150s	
1879/2050 (epoch 45.829), train_loss = 1.41394431, grad/param norm = 5.0634e-02, time/batch = 0.1144s	
1880/2050 (epoch 45.854), train_loss = 1.40387962, grad/param norm = 5.1717e-02, time/batch = 0.1125s	
1881/2050 (epoch 45.878), train_loss = 1.42463512, grad/param norm = 5.0698e-02, time/batch = 0.1132s	
1882/2050 (epoch 45.902), train_loss = 1.43892434, grad/param norm = 5.1653e-02, time/batch = 0.1126s	
1883/2050 (epoch 45.927), train_loss = 1.41251785, grad/param norm = 5.7947e-02, time/batch = 0.1129s	
1884/2050 (epoch 45.951), train_loss = 1.44669942, grad/param norm = 7.1964e-02, time/batch = 0.1125s	
1885/2050 (epoch 45.976), train_loss = 1.41663325, grad/param norm = 8.0213e-02, time/batch = 0.1124s	
decayed learning rate by a factor 0.97 to 0.00064801366403768	
1886/2050 (epoch 46.000), train_loss = 1.43201282, grad/param norm = 7.6921e-02, time/batch = 0.1125s	
1887/2050 (epoch 46.024), train_loss = 1.59713207, grad/param norm = 8.1053e-02, time/batch = 0.1103s	
1888/2050 (epoch 46.049), train_loss = 1.43376268, grad/param norm = 8.2113e-02, time/batch = 0.1098s	
1889/2050 (epoch 46.073), train_loss = 1.39448181, grad/param norm = 8.2128e-02, time/batch = 0.1096s	
1890/2050 (epoch 46.098), train_loss = 1.42535403, grad/param norm = 8.2743e-02, time/batch = 0.1097s	
1891/2050 (epoch 46.122), train_loss = 1.41484859, grad/param norm = 7.5110e-02, time/batch = 0.1102s	
1892/2050 (epoch 46.146), train_loss = 1.41705461, grad/param norm = 7.3753e-02, time/batch = 0.1097s	
1893/2050 (epoch 46.171), train_loss = 1.42377954, grad/param norm = 7.1987e-02, time/batch = 0.1097s	
1894/2050 (epoch 46.195), train_loss = 1.42282583, grad/param norm = 5.7663e-02, time/batch = 0.1098s	
1895/2050 (epoch 46.220), train_loss = 1.41716784, grad/param norm = 5.0895e-02, time/batch = 0.1097s	
1896/2050 (epoch 46.244), train_loss = 1.39800490, grad/param norm = 5.0910e-02, time/batch = 0.1098s	
1897/2050 (epoch 46.268), train_loss = 1.43101002, grad/param norm = 4.5805e-02, time/batch = 0.1097s	
1898/2050 (epoch 46.293), train_loss = 1.42964631, grad/param norm = 4.2781e-02, time/batch = 0.1103s	
1899/2050 (epoch 46.317), train_loss = 1.44426033, grad/param norm = 4.3888e-02, time/batch = 0.1096s	
1900/2050 (epoch 46.341), train_loss = 1.43089242, grad/param norm = 5.8095e-02, time/batch = 0.1097s	
1901/2050 (epoch 46.366), train_loss = 1.43559040, grad/param norm = 6.8509e-02, time/batch = 0.1102s	
1902/2050 (epoch 46.390), train_loss = 1.43457753, grad/param norm = 7.6910e-02, time/batch = 0.1098s	
1903/2050 (epoch 46.415), train_loss = 1.41559793, grad/param norm = 6.8791e-02, time/batch = 0.1097s	
1904/2050 (epoch 46.439), train_loss = 1.43088058, grad/param norm = 6.1311e-02, time/batch = 0.1096s	
1905/2050 (epoch 46.463), train_loss = 1.42494059, grad/param norm = 5.8225e-02, time/batch = 0.1098s	
1906/2050 (epoch 46.488), train_loss = 1.42241453, grad/param norm = 4.8770e-02, time/batch = 0.1098s	
1907/2050 (epoch 46.512), train_loss = 1.41574715, grad/param norm = 4.5268e-02, time/batch = 0.1097s	
1908/2050 (epoch 46.537), train_loss = 1.42436829, grad/param norm = 5.1444e-02, time/batch = 0.1100s	
1909/2050 (epoch 46.561), train_loss = 1.41900676, grad/param norm = 6.6119e-02, time/batch = 0.1100s	
1910/2050 (epoch 46.585), train_loss = 1.42115429, grad/param norm = 7.0456e-02, time/batch = 0.1097s	
1911/2050 (epoch 46.610), train_loss = 1.43200402, grad/param norm = 6.0196e-02, time/batch = 0.1103s	
1912/2050 (epoch 46.634), train_loss = 1.42266259, grad/param norm = 5.4610e-02, time/batch = 0.1097s	
1913/2050 (epoch 46.659), train_loss = 1.42078859, grad/param norm = 5.1536e-02, time/batch = 0.1097s	
1914/2050 (epoch 46.683), train_loss = 1.42598056, grad/param norm = 5.6320e-02, time/batch = 0.1095s	
1915/2050 (epoch 46.707), train_loss = 1.40809772, grad/param norm = 7.1375e-02, time/batch = 0.1097s	
1916/2050 (epoch 46.732), train_loss = 1.42541425, grad/param norm = 7.8369e-02, time/batch = 0.1098s	
1917/2050 (epoch 46.756), train_loss = 1.43364893, grad/param norm = 7.2894e-02, time/batch = 0.1098s	
1918/2050 (epoch 46.780), train_loss = 1.44030453, grad/param norm = 6.4616e-02, time/batch = 0.1097s	
1919/2050 (epoch 46.805), train_loss = 1.40757113, grad/param norm = 5.5517e-02, time/batch = 0.1096s	
1920/2050 (epoch 46.829), train_loss = 1.41015730, grad/param norm = 5.2971e-02, time/batch = 0.1097s	
1921/2050 (epoch 46.854), train_loss = 1.40083536, grad/param norm = 6.2681e-02, time/batch = 0.1102s	
1922/2050 (epoch 46.878), train_loss = 1.42178594, grad/param norm = 5.9287e-02, time/batch = 0.1098s	
1923/2050 (epoch 46.902), train_loss = 1.43477676, grad/param norm = 4.8806e-02, time/batch = 0.1098s	
1924/2050 (epoch 46.927), train_loss = 1.40808468, grad/param norm = 4.8849e-02, time/batch = 0.1099s	
1925/2050 (epoch 46.951), train_loss = 1.44120100, grad/param norm = 5.7393e-02, time/batch = 0.1099s	
1926/2050 (epoch 46.976), train_loss = 1.41016235, grad/param norm = 5.6409e-02, time/batch = 0.1097s	
decayed learning rate by a factor 0.97 to 0.00062857325411655	
1927/2050 (epoch 47.000), train_loss = 1.42607394, grad/param norm = 5.8793e-02, time/batch = 0.1098s	
1928/2050 (epoch 47.024), train_loss = 1.59263950, grad/param norm = 7.0303e-02, time/batch = 0.1097s	
1929/2050 (epoch 47.049), train_loss = 1.42716069, grad/param norm = 7.0074e-02, time/batch = 0.1107s	
1930/2050 (epoch 47.073), train_loss = 1.38764963, grad/param norm = 5.9819e-02, time/batch = 0.1098s	
1931/2050 (epoch 47.098), train_loss = 1.41779353, grad/param norm = 5.4975e-02, time/batch = 0.1103s	
1932/2050 (epoch 47.122), train_loss = 1.40837679, grad/param norm = 4.8522e-02, time/batch = 0.1098s	
1933/2050 (epoch 47.146), train_loss = 1.41076450, grad/param norm = 5.0232e-02, time/batch = 0.1098s	
1934/2050 (epoch 47.171), train_loss = 1.41810776, grad/param norm = 5.3238e-02, time/batch = 0.1096s	
1935/2050 (epoch 47.195), train_loss = 1.41808659, grad/param norm = 4.5055e-02, time/batch = 0.1102s	
1936/2050 (epoch 47.220), train_loss = 1.41262108, grad/param norm = 4.2212e-02, time/batch = 0.1097s	
1937/2050 (epoch 47.244), train_loss = 1.39379455, grad/param norm = 4.6892e-02, time/batch = 0.1097s	
1938/2050 (epoch 47.268), train_loss = 1.42738349, grad/param norm = 4.6998e-02, time/batch = 0.1099s	
1939/2050 (epoch 47.293), train_loss = 1.42619555, grad/param norm = 4.5416e-02, time/batch = 0.1098s	
1940/2050 (epoch 47.317), train_loss = 1.44068143, grad/param norm = 4.6880e-02, time/batch = 0.1097s	
1941/2050 (epoch 47.341), train_loss = 1.42818379, grad/param norm = 6.5891e-02, time/batch = 0.1103s	
1942/2050 (epoch 47.366), train_loss = 1.43361395, grad/param norm = 7.9599e-02, time/batch = 0.1096s	
1943/2050 (epoch 47.390), train_loss = 1.43248351, grad/param norm = 8.9597e-02, time/batch = 0.1098s	
1944/2050 (epoch 47.415), train_loss = 1.41361052, grad/param norm = 8.3960e-02, time/batch = 0.1096s	
1945/2050 (epoch 47.439), train_loss = 1.42784471, grad/param norm = 7.3309e-02, time/batch = 0.1098s	
1946/2050 (epoch 47.463), train_loss = 1.42186659, grad/param norm = 6.5474e-02, time/batch = 0.1098s	
1947/2050 (epoch 47.488), train_loss = 1.41895433, grad/param norm = 5.9426e-02, time/batch = 0.1097s	
1948/2050 (epoch 47.512), train_loss = 1.41327678, grad/param norm = 6.2302e-02, time/batch = 0.1098s	
1949/2050 (epoch 47.537), train_loss = 1.42218170, grad/param norm = 6.9799e-02, time/batch = 0.1096s	
1950/2050 (epoch 47.561), train_loss = 1.41618299, grad/param norm = 7.9431e-02, time/batch = 0.1100s	
1951/2050 (epoch 47.585), train_loss = 1.41837537, grad/param norm = 8.0458e-02, time/batch = 0.1102s	
1952/2050 (epoch 47.610), train_loss = 1.42934393, grad/param norm = 6.6126e-02, time/batch = 0.1098s	
1953/2050 (epoch 47.634), train_loss = 1.41871647, grad/param norm = 5.6165e-02, time/batch = 0.1098s	
1954/2050 (epoch 47.659), train_loss = 1.41646717, grad/param norm = 4.9257e-02, time/batch = 0.1096s	
1955/2050 (epoch 47.683), train_loss = 1.42185344, grad/param norm = 5.4456e-02, time/batch = 0.1097s	
1956/2050 (epoch 47.707), train_loss = 1.40305520, grad/param norm = 5.6053e-02, time/batch = 0.1098s	
1957/2050 (epoch 47.732), train_loss = 1.41930053, grad/param norm = 5.5178e-02, time/batch = 0.1098s	
1958/2050 (epoch 47.756), train_loss = 1.42824093, grad/param norm = 5.4475e-02, time/batch = 0.1098s	
1959/2050 (epoch 47.780), train_loss = 1.43589625, grad/param norm = 6.1262e-02, time/batch = 0.1096s	
1960/2050 (epoch 47.805), train_loss = 1.40517812, grad/param norm = 6.6668e-02, time/batch = 0.1096s	
1961/2050 (epoch 47.829), train_loss = 1.40938648, grad/param norm = 7.4154e-02, time/batch = 0.1106s	
1962/2050 (epoch 47.854), train_loss = 1.39950315, grad/param norm = 7.5185e-02, time/batch = 0.1097s	
1963/2050 (epoch 47.878), train_loss = 1.41942046, grad/param norm = 6.8249e-02, time/batch = 0.1097s	
1964/2050 (epoch 47.902), train_loss = 1.43365949, grad/param norm = 7.1226e-02, time/batch = 0.1096s	
1965/2050 (epoch 47.927), train_loss = 1.40733794, grad/param norm = 7.5066e-02, time/batch = 0.1101s	
1966/2050 (epoch 47.951), train_loss = 1.43910198, grad/param norm = 7.5465e-02, time/batch = 0.1098s	
1967/2050 (epoch 47.976), train_loss = 1.40764079, grad/param norm = 6.6189e-02, time/batch = 0.1097s	
decayed learning rate by a factor 0.97 to 0.00060971605649306	
1968/2050 (epoch 48.000), train_loss = 1.42162444, grad/param norm = 4.9809e-02, time/batch = 0.1098s	
1969/2050 (epoch 48.024), train_loss = 1.58803736, grad/param norm = 5.6469e-02, time/batch = 0.1593s	
1970/2050 (epoch 48.049), train_loss = 1.42270416, grad/param norm = 6.0588e-02, time/batch = 0.2074s	
1971/2050 (epoch 48.073), train_loss = 1.38415418, grad/param norm = 6.3540e-02, time/batch = 0.2051s	
1972/2050 (epoch 48.098), train_loss = 1.41517247, grad/param norm = 6.5185e-02, time/batch = 0.1998s	
1973/2050 (epoch 48.122), train_loss = 1.40541695, grad/param norm = 5.9811e-02, time/batch = 0.1885s	
1974/2050 (epoch 48.146), train_loss = 1.40787290, grad/param norm = 6.0461e-02, time/batch = 0.1315s	
1975/2050 (epoch 48.171), train_loss = 1.41491050, grad/param norm = 6.2793e-02, time/batch = 0.1309s	
1976/2050 (epoch 48.195), train_loss = 1.41461294, grad/param norm = 5.2587e-02, time/batch = 0.1311s	
1977/2050 (epoch 48.220), train_loss = 1.40887471, grad/param norm = 4.5021e-02, time/batch = 0.1248s	
1978/2050 (epoch 48.244), train_loss = 1.38994445, grad/param norm = 4.6403e-02, time/batch = 0.1236s	
1979/2050 (epoch 48.268), train_loss = 1.42341527, grad/param norm = 4.4872e-02, time/batch = 0.1236s	
1980/2050 (epoch 48.293), train_loss = 1.42214236, grad/param norm = 4.4433e-02, time/batch = 0.1239s	
1981/2050 (epoch 48.317), train_loss = 1.43642768, grad/param norm = 4.4857e-02, time/batch = 0.1261s	
1982/2050 (epoch 48.341), train_loss = 1.42280971, grad/param norm = 5.3233e-02, time/batch = 0.1235s	
1983/2050 (epoch 48.366), train_loss = 1.42664009, grad/param norm = 5.5080e-02, time/batch = 0.1281s	
1984/2050 (epoch 48.390), train_loss = 1.42520201, grad/param norm = 5.7705e-02, time/batch = 0.1169s	
1985/2050 (epoch 48.415), train_loss = 1.40621142, grad/param norm = 5.4682e-02, time/batch = 0.1138s	
1986/2050 (epoch 48.439), train_loss = 1.42139392, grad/param norm = 4.7852e-02, time/batch = 0.1135s	
1987/2050 (epoch 48.463), train_loss = 1.41632288, grad/param norm = 4.7326e-02, time/batch = 0.1133s	
1988/2050 (epoch 48.488), train_loss = 1.41455082, grad/param norm = 4.8147e-02, time/batch = 0.1134s	
1989/2050 (epoch 48.512), train_loss = 1.40834122, grad/param norm = 4.7584e-02, time/batch = 0.1133s	
1990/2050 (epoch 48.537), train_loss = 1.41668136, grad/param norm = 4.9015e-02, time/batch = 0.1132s	
1991/2050 (epoch 48.561), train_loss = 1.41037228, grad/param norm = 5.6980e-02, time/batch = 0.1149s	
1992/2050 (epoch 48.585), train_loss = 1.41215380, grad/param norm = 6.3399e-02, time/batch = 0.1133s	
1993/2050 (epoch 48.610), train_loss = 1.42420451, grad/param norm = 6.1511e-02, time/batch = 0.1133s	
1994/2050 (epoch 48.634), train_loss = 1.41465466, grad/param norm = 5.6657e-02, time/batch = 0.1130s	
1995/2050 (epoch 48.659), train_loss = 1.41331317, grad/param norm = 5.6186e-02, time/batch = 0.1133s	
1996/2050 (epoch 48.683), train_loss = 1.41893633, grad/param norm = 5.8459e-02, time/batch = 0.1132s	
1997/2050 (epoch 48.707), train_loss = 1.40039326, grad/param norm = 6.8519e-02, time/batch = 0.1132s	
1998/2050 (epoch 48.732), train_loss = 1.41771148, grad/param norm = 7.5787e-02, time/batch = 0.1133s	
1999/2050 (epoch 48.756), train_loss = 1.42643425, grad/param norm = 7.8856e-02, time/batch = 0.1134s	
evaluating loss over split index 2	
1/3...	
2/3...	
3/3...	
saving checkpoint to cv/lm_lstm_epoch48.78_1.5120.t7	
2000/2050 (epoch 48.780), train_loss = 1.43347450, grad/param norm = 7.1408e-02, time/batch = 0.1134s	
2001/2050 (epoch 48.805), train_loss = 1.60279211, grad/param norm = 8.5116e-02, time/batch = 0.1736s	
2002/2050 (epoch 48.829), train_loss = 1.40876209, grad/param norm = 9.1009e-02, time/batch = 0.1720s	
2003/2050 (epoch 48.854), train_loss = 1.39700056, grad/param norm = 8.5587e-02, time/batch = 0.1714s	
2004/2050 (epoch 48.878), train_loss = 1.41528334, grad/param norm = 6.4714e-02, time/batch = 0.1714s	
2005/2050 (epoch 48.902), train_loss = 1.42834929, grad/param norm = 4.8454e-02, time/batch = 0.1664s	
2006/2050 (epoch 48.927), train_loss = 1.40096154, grad/param norm = 4.7854e-02, time/batch = 0.1205s	
2007/2050 (epoch 48.951), train_loss = 1.43349095, grad/param norm = 5.4077e-02, time/batch = 0.1155s	
2008/2050 (epoch 48.976), train_loss = 1.40220958, grad/param norm = 4.9517e-02, time/batch = 0.1156s	
decayed learning rate by a factor 0.97 to 0.00059142457479826	
2009/2050 (epoch 49.000), train_loss = 1.41816569, grad/param norm = 4.7545e-02, time/batch = 0.1149s	
2010/2050 (epoch 49.024), train_loss = 1.58443237, grad/param norm = 6.1162e-02, time/batch = 0.1126s	
2011/2050 (epoch 49.049), train_loss = 1.41937831, grad/param norm = 5.9237e-02, time/batch = 0.1131s	
2012/2050 (epoch 49.073), train_loss = 1.38011290, grad/param norm = 5.6714e-02, time/batch = 0.1128s	
2013/2050 (epoch 49.098), train_loss = 1.41120885, grad/param norm = 5.9067e-02, time/batch = 0.1126s	
2014/2050 (epoch 49.122), train_loss = 1.40136216, grad/param norm = 5.4887e-02, time/batch = 0.1124s	
2015/2050 (epoch 49.146), train_loss = 1.40400249, grad/param norm = 5.5740e-02, time/batch = 0.1124s	
2016/2050 (epoch 49.171), train_loss = 1.41118802, grad/param norm = 5.9177e-02, time/batch = 0.1123s	
2017/2050 (epoch 49.195), train_loss = 1.41102153, grad/param norm = 4.9844e-02, time/batch = 0.1097s	
2018/2050 (epoch 49.220), train_loss = 1.40564716, grad/param norm = 4.7268e-02, time/batch = 0.1098s	
2019/2050 (epoch 49.244), train_loss = 1.38686333, grad/param norm = 4.9762e-02, time/batch = 0.1095s	
2020/2050 (epoch 49.268), train_loss = 1.41995799, grad/param norm = 4.5785e-02, time/batch = 0.1101s	
2021/2050 (epoch 49.293), train_loss = 1.41862684, grad/param norm = 4.3600e-02, time/batch = 0.1102s	
2022/2050 (epoch 49.317), train_loss = 1.43273103, grad/param norm = 4.3652e-02, time/batch = 0.1098s	
2023/2050 (epoch 49.341), train_loss = 1.41881095, grad/param norm = 4.7303e-02, time/batch = 0.1097s	
2024/2050 (epoch 49.366), train_loss = 1.42230203, grad/param norm = 4.6755e-02, time/batch = 0.1096s	
2025/2050 (epoch 49.390), train_loss = 1.42141184, grad/param norm = 5.2585e-02, time/batch = 0.1098s	
2026/2050 (epoch 49.415), train_loss = 1.40271733, grad/param norm = 5.4456e-02, time/batch = 0.1097s	
2027/2050 (epoch 49.439), train_loss = 1.41833285, grad/param norm = 5.3741e-02, time/batch = 0.1098s	
2028/2050 (epoch 49.463), train_loss = 1.41378639, grad/param norm = 5.5799e-02, time/batch = 0.1098s	
2029/2050 (epoch 49.488), train_loss = 1.41193104, grad/param norm = 5.3888e-02, time/batch = 0.1096s	
2030/2050 (epoch 49.512), train_loss = 1.40588443, grad/param norm = 5.2633e-02, time/batch = 0.1097s	
2031/2050 (epoch 49.537), train_loss = 1.41439444, grad/param norm = 5.6510e-02, time/batch = 0.1102s	
2032/2050 (epoch 49.561), train_loss = 1.40871545, grad/param norm = 7.0525e-02, time/batch = 0.1097s	
2033/2050 (epoch 49.585), train_loss = 1.41033184, grad/param norm = 7.3741e-02, time/batch = 0.1097s	
2034/2050 (epoch 49.610), train_loss = 1.42133771, grad/param norm = 6.5941e-02, time/batch = 0.1095s	
2035/2050 (epoch 49.634), train_loss = 1.41194904, grad/param norm = 6.3867e-02, time/batch = 0.1101s	
2036/2050 (epoch 49.659), train_loss = 1.41037873, grad/param norm = 5.9923e-02, time/batch = 0.1098s	
2037/2050 (epoch 49.683), train_loss = 1.41607128, grad/param norm = 6.5738e-02, time/batch = 0.1097s	
2038/2050 (epoch 49.707), train_loss = 1.39861178, grad/param norm = 8.4440e-02, time/batch = 0.1098s	
2039/2050 (epoch 49.732), train_loss = 1.41526305, grad/param norm = 8.6349e-02, time/batch = 0.1097s	
2040/2050 (epoch 49.756), train_loss = 1.42268933, grad/param norm = 6.9822e-02, time/batch = 0.1098s	
2041/2050 (epoch 49.780), train_loss = 1.42927896, grad/param norm = 5.9965e-02, time/batch = 0.1102s	
2042/2050 (epoch 49.805), train_loss = 1.39695614, grad/param norm = 5.0336e-02, time/batch = 0.1102s	
2043/2050 (epoch 49.829), train_loss = 1.39931036, grad/param norm = 4.6486e-02, time/batch = 0.1096s	
2044/2050 (epoch 49.854), train_loss = 1.38915535, grad/param norm = 5.2125e-02, time/batch = 0.1096s	
2045/2050 (epoch 49.878), train_loss = 1.41031908, grad/param norm = 4.6568e-02, time/batch = 0.1097s	
2046/2050 (epoch 49.902), train_loss = 1.42409642, grad/param norm = 4.1121e-02, time/batch = 0.1100s	
2047/2050 (epoch 49.927), train_loss = 1.39742012, grad/param norm = 4.4054e-02, time/batch = 0.1097s	
2048/2050 (epoch 49.951), train_loss = 1.42974525, grad/param norm = 5.3864e-02, time/batch = 0.1098s	
2049/2050 (epoch 49.976), train_loss = 1.39905028, grad/param norm = 5.3163e-02, time/batch = 0.1097s	
decayed learning rate by a factor 0.97 to 0.00057368183755432	
evaluating loss over split index 2	
1/3...	
2/3...	
3/3...	
saving checkpoint to cv/lm_lstm_epoch50.00_1.5067.t7	
2050/2050 (epoch 50.000), train_loss = 1.41500118, grad/param norm = 5.0546e-02, time/batch = 0.1101s	
